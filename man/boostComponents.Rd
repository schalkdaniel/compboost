% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/boost_components.R
\name{boostComponents}
\alias{boostComponents}
\title{Wrapper to boost general additive models using components}
\usage{
boostComponents(
  data,
  target,
  optimizer = NULL,
  loss = NULL,
  learning_rate = 0.05,
  iterations = 100,
  trace = -1,
  degree = 3,
  n_knots = 20,
  penalty = 2,
  df = 0,
  differences = 2,
  data_source = InMemoryData,
  oob_fraction = NULL,
  bin_root = 0,
  cache_type = "inverse",
  stop_args = list(),
  df_cat = 1,
  stop_time = "microseconds",
  additional_risk_logs = list()
)
}
\arguments{
\item{data}{(\code{data.frame()})\cr
A data frame containing the data.}

\item{target}{(\code{character(1)} | \link{ResponseRegr} | \link{ResponseBinaryClassif})\cr
Character value containing the target variable or response object. Note that the loss must match the
data type of the target.}

\item{optimizer}{(\link{OptimizerCoordinateDescent} | \link{OptimizerCoordinateDescentLineSearch} | \link{OptimizerAGBM} | \link{OptimizerCosineAnnealing})\cr
An initialized \code{S4} optimizer object (requires to call \code{Optimizer*.new(..)}.
See the respective help page for further information.}

\item{loss}{(\link{LossQuadratic} | \link{LossBinomial} | \link{LossHuber} | \link{LossAbsolute} | \link{LossQuantile})\cr
An initialized \code{S4} loss object (requires to call \verb{Loss*$new(...)}).
See the respective help page for further information.}

\item{learning_rate}{(\code{numeric(1)})\cr
Learning rate to shrink the parameter in each step.}

\item{iterations}{(\code{integer(1)})\cr
Number of iterations that are trained. If \code{iterations == 0}, the untrained object is returned. This
can be useful if other base learners (e.g. an interaction via a tensor base learner) are added.}

\item{trace}{(\code{integer(1)})\cr
Integer indicating how often a trace should be printed. Specifying \code{trace = 10}, then every
10th iteration is printed. If no trace should be printed set \code{trace = 0}. Default is
-1 which means that in total 40 iterations are printed.}

\item{degree}{(\code{integer(1)})cr
Polynomial degree of the splines.}

\item{n_knots}{(\code{integer(1)})\cr
Number of equidistant "inner knots". The actual number of used knots does also depend on
the polynomial degree.}

\item{penalty}{(\code{numeric(1)})\cr
Penalty term for p-splines. If the penalty equals 0, then ordinary b-splines are fitted.
The higher the penalty, the higher the smoothness.}

\item{df}{(\code{numeric(1)})\cr
Degrees of freedom of the base learner(s).}

\item{differences}{(\code{integer(1)})\cr
Number of differences that are used for penalization. The higher the difference, the higher the smoothness.}

\item{data_source}{(\verb{Data*})\cr
Uninitialized \verb{Data*} object which is used to store the data. At the moment
just in memory training is supported.}

\item{oob_fraction}{(\code{numeric(1)})\cr
Fraction of how much data are used to track the out of bag risk.}

\item{bin_root}{(\code{integer(1)})\cr
The binning root to reduce the data to \eqn{n^{1/\text{binroot}}} data points
(default \code{bin_root = 1}, which means no binning is applied).
A value of \code{bin_root = 2} is suggested for the best approximation
error (cf. \emph{Wood et al. (2017) Generalized additive models for gigadata:
modeling the UK black smoke network daily data}).}

\item{cache_type}{(\code{character(1)})\cr
String to indicate what method should be used to estimate the parameter in each iteration.
Default is \code{cache_type = "cholesky"} which computes the Cholesky decomposition,
caches it, and reuses the matrix over and over again. The other option is to use
\code{cache_type = "inverse"} which does the same but caches the inverse.}

\item{stop_args}{(\code{list(2)})\cr
List containing two elements \code{patience} and \code{eps_for_break} which can be set
to use early stopping on the left out data from setting \code{oob_fraction}.
If \code{! is.null(stop_args)}, early stopping is triggered.}

\item{df_cat}{(\code{numeric(1)})\cr
Degrees of freedom of the categorical base-learner.}

\item{stop_time}{(\code{character(1)})\cr
Unit of measured time.}

\item{additional_risk_logs}{(\code{list(Logger)})\cr
Additional logger passed to the \code{Compboost} object.}
}
\value{
A model of the \link{Compboost} class. This model is an \link{R6} object
which can be used for retraining, predicting, plotting, and anything described in
\code{?Compboost}.
}
\description{
This wrapper function automatically initializes the model by adding all numerical
features as components. This means, that for each numerical feature a linear effect
and non-linear spline base-learner is added. The non-linear part is constructed in way
that it cannot model the linear part. Hence, it is just selected if a non-linear
base learner is really necessary. Categorical features are dummy encoded and inserted
using another linear base-learners without intercept.

The returned object is an object of the \link{Compboost} class. This object can be
used for further analyses (see \code{?Compboost} for details).
}
\examples{
mod = boostComponents(data = iris, target = "Sepal.Length", df = 4)
mod$getBaselearnerNames()
table(mod$getSelectedBaselearner())
plotPEUni(mod, "Petal.Length")
mod$predict()
}
