[{"path":[]},{"path":"https://schalkdaniel.github.io/compboost/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"interest fostering open welcoming environment, contributors maintainers pledge making participation project community harassment-free experience everyone, regardless age, body size, disability, ethnicity, gender identity expression, level experience, nationality, personal appearance, race, religion, sexual identity orientation.","code":""},{"path":"https://schalkdaniel.github.io/compboost/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes creating positive environment include: Using welcoming inclusive language respectful differing viewpoints experiences Gracefully accepting constructive criticism Focusing best community Showing empathy towards community members Examples unacceptable behavior participants include: use sexualized language imagery unwelcome sexual attention advances Trolling, insulting/derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical electronic address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://schalkdaniel.github.io/compboost/CODE_OF_CONDUCT.html","id":"our-responsibilities","dir":"","previous_headings":"","what":"Our Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Project maintainers responsible clarifying standards acceptable behavior expected take appropriate fair corrective action response instances unacceptable behavior. Project maintainers right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, ban temporarily permanently contributor behaviors deem inappropriate, threatening, offensive, harmful.","code":""},{"path":"https://schalkdaniel.github.io/compboost/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within project spaces public spaces individual representing project community. Examples representing project community include using official project e-mail address, posting via official social media account, acting appointed representative online offline event. Representation project may defined clarified project maintainers.","code":""},{"path":"https://schalkdaniel.github.io/compboost/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported contacting project team contact@danielschalk.com. project team review investigate complaints, respond way deems appropriate circumstances. project team obligated maintain confidentiality regard reporter incident. details specific enforcement policies may posted separately. Project maintainers follow enforce Code Conduct good faith may face temporary permanent repercussions determined members project’s leadership.","code":""},{"path":"https://schalkdaniel.github.io/compboost/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 1.4, available http://contributor-covenant.org/version/1/4","code":""},{"path":"https://schalkdaniel.github.io/compboost/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to compboost development","title":"Contributing to compboost development","text":", authors compboost R package, use guide used contributing development popular ggplot2 R package. document simply formal re-statement fact. goal guide help get contributing compboost quickly possible. guide divided two main pieces: Filing bug report feature request issue. Suggesting change via pull request.","code":""},{"path":"https://schalkdaniel.github.io/compboost/CONTRIBUTING.html","id":"issues","dir":"","previous_headings":"","what":"Issues","title":"Contributing to compboost development","text":"filing issue, important thing include minimal reproducible example can quickly verify problem, figure fix . three things need include make example reproducible: required packages, data, code. Packages loaded top script, ’s easy see ones example needs. easiest way include data use dput() generate R code recreate . example, recreate mtcars dataset R, ’d perform following steps: Run dput(mtcars) R Copy output reproducible script, type mtcars <- paste. even better can create data.frame() just handful rows columns still illustrates problem. Spend little bit time ensuring code easy others read: make sure ’ve used spaces variable names concise, informative use comments indicate problem lies best remove everything related problem. shorter code , easier understand. can check actually made reproducible example starting fresh R session pasting script . (Unless ’ve specifically asked , please don’t include output sessionInfo().)","code":""},{"path":"https://schalkdaniel.github.io/compboost/CONTRIBUTING.html","id":"pull-requests","dir":"","previous_headings":"","what":"Pull requests","title":"Contributing to compboost development","text":"contribute change compboost, follow steps: Create branch git make changes. Push branch github issue pull request (PR). Discuss pull request. Iterate either accept PR decide ’s good fit compboost. steps described detail . might feel overwhelming first time get set , gets easier practice. ’re familiar git GitHub, please start reading http://r-pkgs..co.nz/git.html Pull requests evaluated checklist: Motivation. pull request clearly concisely motivates need change. Plesae describe problem PR addresses show pull request solves concisely possible. Also include motivation NEWS new release compboost comes ’s easy users see ’s changed. Add item top file use markdown formatting. news item end (@yourGithubUsername, #the_issue_number). related changes. submit pull request, please check make sure haven’t accidentally included unrelated changes. make harder see exactly ’s changed, evaluate unexpected side effects. PR corresponds git branch, expect submit multiple changes make sure create multiple branches. multiple changes depend , start first one don’t submit others first one processed. ’re adding new parameters new function, ’ll also need document roxygen. Make sure re-run devtools::document() code submitting. seems like lot work don’t worry pull request isn’t perfect. ’s learning process. pull request process, unless ’ve submitted past ’s unlikely pull request accepted . Please don’t submit pull requests change existing behaviour. Instead, think can add new feature minimally invasive way.","code":""},{"path":"https://schalkdaniel.github.io/compboost/CONTRIBUTORS.html","id":null,"dir":"","previous_headings":"","what":"Contributors of compboost","title":"Contributors of compboost","text":"Committers people made substantial contribution project granted write access project. Daniel creator maintainer project. Janek gives feedback/guidance package design structure implements initial R API. Bernd gives feedback/guidance package design structure initiated project.","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/advanced/ext_losses.html","id":"before-starting","dir":"Articles > Advanced","previous_headings":"","what":"Before Starting","title":"Extending compboost with losses","text":"Read use-case site get know define Compboost object using R6 interface","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/advanced/ext_losses.html","id":"what-is-needed","dir":"Articles > Advanced","previous_headings":"","what":"What is Needed","title":"Extending compboost with losses","text":"compboost designed provide component-wise boosting framework maximal flexibility. vignette gives overview define custom losses R well C++ without recompiling whole package. custom losses can used training model /logging mechanisms. loss function boosting must differentiable. Hence, need define loss function gradient. , boosting initialized loss optimal constant. capture , define loss optimal constant function response vector. three components, quite easy define custom losses. showcase, rebuilding two different loss functions: quadratic loss easy example C++ Poisson loss counting data sophisticated loss example R","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/advanced/ext_losses.html","id":"define-a-new-loss-with-r","dir":"Articles > Advanced","previous_headings":"","what":"Define a New Loss With R","title":"Extending compboost with losses","text":"example using VonBort dataset provided package vcd: “Data von Bortkiewicz (1898), given Andrews & Herzberg (1985), number deaths horse mule kicks 14 corps Prussian army.” like model deaths using Poisson regression boosting. means define proper loss function, gradient, constant initialization. scheme loss, gradient, constant initialization specify function following form: loss: function (truth, response) gradient: function (truth, response) constant initializer: function (truth)","code":"data(VonBort, package = \"vcd\")"},{"path":"https://schalkdaniel.github.io/compboost/articles/advanced/ext_losses.html","id":"the-loss-function","dir":"Articles > Advanced","previous_headings":"Define a New Loss With R","what":"The loss function","title":"Extending compboost with losses","text":"\\[L(y,f) = -\\log\\left( \\exp(f)^y \\exp(\\exp(f)) \\right) - \\log(y!)\\]","code":"lossPoisson = function (truth, response) {   return(-log(exp(response)^truth * exp(-exp(response))) - gamma(truth + 1)) }"},{"path":"https://schalkdaniel.github.io/compboost/articles/advanced/ext_losses.html","id":"the-gradient-of-the-loss-function","dir":"Articles > Advanced","previous_headings":"Define a New Loss With R","what":"The gradient of the loss function","title":"Extending compboost with losses","text":"\\[\\frac{\\partial}{\\partial f} L(y,f) = \\exp(f) - y\\]","code":"gradPoisson = function (truth, response) {   return(exp(response) - truth) }"},{"path":"https://schalkdaniel.github.io/compboost/articles/advanced/ext_losses.html","id":"the-constant-initialization","dir":"Articles > Advanced","previous_headings":"Define a New Loss With R","what":"The constant initialization","title":"Extending compboost with losses","text":"\\[\\mathsf{arg min}_{c\\\\mathbb{R}} \\sum_{= 1}^n L\\left(y^{()}, c\\right) = \\log(\\bar{y})\\]","code":"constInitPoisson = function (truth) {   return(log(mean(truth))) }"},{"path":"https://schalkdaniel.github.io/compboost/articles/advanced/ext_losses.html","id":"define-the-loss","dir":"Articles > Advanced","previous_headings":"Define a New Loss With R","what":"Define the loss","title":"Extending compboost with losses","text":"Finally, three components allows define LossCustom object:","code":"# Define custom loss: my_poisson_loss = LossCustom$new(lossPoisson, gradPoisson, constInitPoisson)"},{"path":"https://schalkdaniel.github.io/compboost/articles/advanced/ext_losses.html","id":"train-a-model","dir":"Articles > Advanced","previous_headings":"Define a New Loss With R","what":"Train a model","title":"Extending compboost with losses","text":"loss object can used task requires loss object:","code":"cboost = Compboost$new(VonBort, \"deaths\", loss = my_poisson_loss) cboost$addBaselearner(\"year\", \"spline\", BaselearnerPSpline) cboost$train(100, trace = 0)"},{"path":"https://schalkdaniel.github.io/compboost/articles/advanced/ext_losses.html","id":"define-a-new-loss-with-c","dir":"Articles > Advanced","previous_headings":"","what":"Define a New Loss With C++","title":"Extending compboost with losses","text":"example, keep simple, using iris dataset Sepal.Length target. aim replicate quadratic loss. achieved exposing external pointers R hold function definition passed constructor LossCustomCpp class. general advise write .cpp file contains whole definition. file needs sourced Rcpp::sourceCpp(). declare head first able expose functions: type definition already indicates, C++ functions require following signature: loss: arma::mat lossFun (const arma::mat& truth, const arma::mat& response) gradient: arma::mat gradFun (const arma::mat& truth, const arma::mat& response) constant initializer: constInitFun (const arma::mat& true_value)","code":"// [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  typedef arma::mat (*lossFunPtr) (const arma::mat& true_value, const arma::mat& prediction); typedef arma::mat (*gradFunPtr) (const arma::mat& true_value, const arma::mat& prediction); typedef double (*constInitFunPtr) (const arma::mat& true_value);"},{"path":"https://schalkdaniel.github.io/compboost/articles/advanced/ext_losses.html","id":"the-loss-function-1","dir":"Articles > Advanced","previous_headings":"Define a New Loss With C++","what":"The loss function","title":"Extending compboost with losses","text":"\\[L(y,f) = -0.5 \\left(y - f\\right)^2\\]","code":"arma::mat lossFun (const arma::mat& true_value, const arma::mat& prediction) {   return arma::pow(true_value - prediction, 2) / 2; }"},{"path":"https://schalkdaniel.github.io/compboost/articles/advanced/ext_losses.html","id":"the-gradient-of-the-loss-function-1","dir":"Articles > Advanced","previous_headings":"Define a New Loss With C++","what":"The gradient of the loss function","title":"Extending compboost with losses","text":"\\[\\frac{\\partial}{\\partial f} L(y,f) = f - y\\]","code":"arma::mat gradFun (const arma::mat& true_value, const arma::mat& prediction) {   return prediction - true_value; }"},{"path":"https://schalkdaniel.github.io/compboost/articles/advanced/ext_losses.html","id":"the-constant-initialization-1","dir":"Articles > Advanced","previous_headings":"Define a New Loss With C++","what":"The constant initialization","title":"Extending compboost with losses","text":"\\[\\mathsf{arg min}_{c\\\\mathbb{R}} \\sum_{= 1}^n L\\left(y^{()}, c\\right) = \\bar{y}\\]","code":"double constInitFun (const arma::mat& true_value) {   return arma::accu(true_value) / true_value.size(); }"},{"path":"https://schalkdaniel.github.io/compboost/articles/advanced/ext_losses.html","id":"exposing-external-pointer","dir":"Articles > Advanced","previous_headings":"Define a New Loss With C++","what":"Exposing external pointer","title":"Extending compboost with losses","text":"functions exposed XPtr. stores pointer function can used parameter LossCustomCpp. Note necessary export upper functions, exporting pointer goal.","code":"// [[Rcpp::export]] Rcpp::XPtr<lossFunPtr> lossFunSetter () {   return Rcpp::XPtr<lossFunPtr> (new lossFunPtr (&lossFun)); }  // [[Rcpp::export]] Rcpp::XPtr<gradFunPtr> gradFunSetter () {   return Rcpp::XPtr<gradFunPtr> (new gradFunPtr (&gradFun)); }  // [[Rcpp::export]] Rcpp::XPtr<constInitFunPtr> constInitFunSetter () {   return Rcpp::XPtr<constInitFunPtr> (new constInitFunPtr (&constInitFun)); }"},{"path":"https://schalkdaniel.github.io/compboost/articles/advanced/ext_losses.html","id":"define-the-loss-1","dir":"Articles > Advanced","previous_headings":"Define a New Loss With C++","what":"Define the loss","title":"Extending compboost with losses","text":"can get pointer function exposing : Now, can pass pointer LossCustomCpp class:","code":"lossFunSetter() gradFunSetter() constInitFunSetter() my_cpp_loss = LossCustomCpp$new(lossFunSetter(), gradFunSetter(), constInitFunSetter())"},{"path":"https://schalkdaniel.github.io/compboost/articles/advanced/ext_losses.html","id":"train-a-model-1","dir":"Articles > Advanced","previous_headings":"Define a New Loss With C++","what":"Train a model","title":"Extending compboost with losses","text":"Finally, use custom loss train model:","code":"cboost = boostSplines(data = iris, target = \"Sepal.Length\",   loss = my_cpp_loss, trace = 25)"},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/early_stopping.html","id":"before-starting","dir":"Articles > Getting_started","previous_headings":"","what":"Before Starting","title":"Early Stopping","text":"Read use-case get know define Compboost object using R6 interface","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/early_stopping.html","id":"data-titanic-passenger-survival-data-set","dir":"Articles > Getting_started","previous_headings":"","what":"Data: Titanic Passenger Survival Data Set","title":"Early Stopping","text":"use titanic dataset binary classification Survived. First store train test data two data frames remove rows contains missing values (NAs): later stopping split dataset train test:","code":"# Store train and test data: df = na.omit(titanic::titanic_train) df$Survived = factor(df$Survived, labels = c(\"no\", \"yes\")) set.seed(123) idx_train = sample(seq_len(nrow(df)), size = nrow(df) * 0.8) idx_test = setdiff(seq_len(nrow(df)), idx_train)"},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/early_stopping.html","id":"defining-the-model","dir":"Articles > Getting_started","previous_headings":"","what":"Defining the Model","title":"Early Stopping","text":"define model use-case just train index without specifying --bag fraction:","code":"cboost = Compboost$new(data = df[idx_train, ], target = \"Survived\")  cboost$addBaselearner(\"Age\", \"spline\", BaselearnerPSpline) cboost$addBaselearner(\"Fare\", \"spline\", BaselearnerPSpline) cboost$addBaselearner(\"Sex\", \"ridge\", BaselearnerCategoricalRidge)"},{"path":[]},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/early_stopping.html","id":"how-does-it-work","dir":"Articles > Getting_started","previous_headings":"Early Stopping in Compboost","what":"How does it work?","title":"Early Stopping","text":"early stopping compboost done using logger objects. Logger executed iteration stores class dependent data runtime risk. Additionally, logger can declared stopper setting use_as_stopper = TRUE. declaring logger stopper, used stop algorithm logger-specific criteria reached. example, LoggerTime stop algorithm pre-defined runtime reached.","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/early_stopping.html","id":"example-with-runtime-stopping","dir":"Articles > Getting_started","previous_headings":"Early Stopping in Compboost","what":"Example with runtime stopping","title":"Early Stopping","text":"Now time define logger track runtime. mentioned , set use_as_stopper = TRUE. setting max_time define long want train model, 50000 microseconds: can see, fittings stopped early 620 train full 2000 iterations. logger data can accessed calling $getLoggerData():","code":"cboost$addLogger(logger = LoggerTime, use_as_stopper = TRUE, logger_id = \"time\",   max_time = 50000, time_unit = \"microseconds\")  cboost$train(2000, trace = 250) #>    1/2000   risk = 0.67  time = 0    #>  250/2000   risk = 0.5  time = 18199    #>  500/2000   risk = 0.48  time = 39054    #>  #>  #> Train 620 iterations in 0 Seconds. #> Final risk based on the train set: 0.48 cboost #>  #>  #> Component-Wise Gradient Boosting #>  #> Target variable: Survived #> Number of base-learners: 3 #> Learning rate: 0.05 #> Iterations: 620 #>  #> Offset: 0.423 #>  #> LossBinomial: L(y,x) = log(1 + exp(-2yf(x)) tail(cboost$getLoggerData()) #>     _iterations  time baselearner train_risk #> 616         615 49551  Age_spline  0.4784104 #> 617         616 49642  Age_spline  0.4783898 #> 618         617 49733 Fare_spline  0.4783639 #> 619         618 49824  Age_spline  0.4783434 #> 620         619 49910   Sex_ridge  0.4783132 #> 621         620 50001  Age_spline  0.4782928"},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/early_stopping.html","id":"loss-based-early-stopping","dir":"Articles > Getting_started","previous_headings":"","what":"Loss-Based Early Stopping","title":"Early Stopping","text":"machine learning, often like stop best model performance. need either tuning early stopping determine good number iterations \\(m\\). well-known procedure log --bag (oob) behavior model stop model performance starts get worse. required parameters logger loss \\(L\\) used stopping: \\[\\mathcal{R}_{\\text{emp}}^{[m]} = \\frac{1}{n}\\sum_{=1}^n L\\left(y^{()}, f^{[m]}(x^{()})\\right)\\] percentage performance increase lower boundary increase: \\[\\text{err}^{[m]} = \\frac{\\mathcal{R}_{\\text{emp}}^{[m- 1]} - \\mathcal{R}_{\\text{emp}}^{[m]}}{\\mathcal{R}_{\\text{emp}}^{[m - 1]}}\\]","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/early_stopping.html","id":"define-the-risk-logger","dir":"Articles > Getting_started","previous_headings":"Loss-Based Early Stopping","what":"Define the risk logger","title":"Early Stopping","text":"Since interested oob behavior, necessary prepare oob data response compboost. Therefore, possible use $prepareResponse() $prepareData() member functions create suitable objects: objects can add oob risk logger, declare stopper, train model: Note: use eps_for_break = 0 hard constrain stop training oob risk starts increase. Taking look logger data tells us stopped exactly first five differences bigger zero (oob risk iterations bigger previous ones):  Taking look 2000 iterations shows stopped quite good:  Note: can happen model’s oob behavior increases locally iterations starts decrease . capture , need “patience” parameter waits , let’s say, 5 iterations stops algorithm improvement 5 iterations smaller criteria. Setting parameter one can lead unstable results:","code":"oob_response = cboost$prepareResponse(df$Survived[idx_test]) oob_data = cboost$prepareData(df[idx_test,]) cboost$addLogger(logger = LoggerOobRisk, use_as_stopper = TRUE, logger_id = \"oob\",   used_loss = LossBinomial$new(), eps_for_break = 0, patience = 5, oob_data = oob_data,   oob_response = oob_response)  cboost$train(2000, trace = 250) #>    1/2000   risk = 0.67  oob = 0.68    #>  250/2000   risk = 0.5  oob = 0.49    #>  500/2000   risk = 0.48  oob = 0.48    #>  #>  #> Train 543 iterations in 0 Seconds. #> Final risk based on the train set: 0.48 tail(cboost$getLoggerData(), n = 10) #>     _iterations       oob baselearner train_risk #> 535         534 0.4784209   Sex_ridge  0.4806872 #> 536         535 0.4784327  Age_spline  0.4806586 #> 537         536 0.4784332 Fare_spline  0.4806242 #> 538         537 0.4784450  Age_spline  0.4805959 #> 539         538 0.4783287   Sex_ridge  0.4805564 #> 540         539 0.4783405  Age_spline  0.4805284 #> 541         540 0.4783415 Fare_spline  0.4804943 #> 542         541 0.4783534  Age_spline  0.4804666 #> 543         542 0.4783547 Fare_spline  0.4804330 #> 544         543 0.4783665  Age_spline  0.4804054 diff(tail(cboost$getLoggerData()$oob, n = 10)) #> [1]  1.180302e-05  5.400260e-07  1.179882e-05 -1.163699e-04  1.187324e-05 #> [6]  9.532968e-07  1.186647e-05  1.321357e-06  1.185917e-05 library(ggplot2)  ggplot(data = cboost$getLoggerData(), aes(x = `_iterations`, y = oob)) +   geom_line() +   xlab(\"Iteration\") +   ylab(\"Empirical Risk\") #> Warning: Removed 1 row containing missing values (`geom_line()`). cboost$train(2000, trace = 0) #>  #> You have already trained 543 iterations. #> Train 1457 additional iterations.  ggplot(data = cboost$getLoggerData(), aes(x = `_iterations`, y = oob)) +   geom_line() +   xlab(\"Iteration\") +   ylab(\"Empirical Risk\") #> Warning: Removed 1 row containing missing values (`geom_line()`). df = na.omit(titanic::titanic_train) df$Survived = factor(df$Survived, labels = c(\"no\", \"yes\"))  set.seed(123) idx_train = sample(seq_len(nrow(df)), size = nrow(df) * 0.8) idx_test = setdiff(seq_len(nrow(df)), idx_train)  cboost = Compboost$new(data = df[idx_train, ], target = \"Survived\", loss = LossBinomial$new())  cboost$addBaselearner(\"Age\", \"spline\", BaselearnerPSpline) cboost$addBaselearner(\"Fare\", \"spline\", BaselearnerPSpline) cboost$addBaselearner(\"Sex\", \"ridge\", BaselearnerCategoricalRidge)  oob_response = cboost$prepareResponse(df$Survived[idx_test]) oob_data = cboost$prepareData(df[idx_test,])  cboost$addLogger(logger = LoggerOobRisk, use_as_stopper = TRUE, logger_id = \"oob\",   used_loss = LossBinomial$new(), eps_for_break = 0, patience = 1, oob_data = oob_data,   oob_response = oob_response)  cboost$train(2000, trace = 0) #> Train 320 iterations in 0 Seconds. #> Final risk based on the train set: 0.49   library(ggplot2) ggplot(data = cboost$getLoggerData(), aes(x = `_iterations`, y = oob)) +   geom_line() +   xlab(\"Iteration\") +   ylab(\"Empirical Risk\") #> Warning: Removed 1 row containing missing values (`geom_line()`)."},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/early_stopping.html","id":"further-comments-on-risk-logging","dir":"Articles > Getting_started","previous_headings":"Loss-Based Early Stopping","what":"Further comments on risk logging","title":"Early Stopping","text":"Since can define many logger like, possible define multiple risk logger regarding different loss functions. also possible log performance measures risk logging mechanism. covered advanced topic.","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/early_stopping.html","id":"some-remarks","dir":"Articles > Getting_started","previous_headings":"","what":"Some remarks","title":"Early Stopping","text":"locally (): algorithm stops first stopping criteria logger reached globally (): algorithm stops stopping criteria reached arguments ignored logger set stopper, e.g. max_time time logger logger functionality summarized ","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/object_references.html","id":"response-class","dir":"Articles > Getting_started","previous_headings":"","what":"Response Class","title":"What is component-wise boosting?","text":"target variable represented object inherits Response class. Depending target type like different transformations internally predicted scores. instance, binary classification task score \\(\\hat{f}(x) \\\\mathbb{R}\\) transformed \\([0,1]\\) scale using logistic function: \\[ \\hat{\\pi}(x) = \\frac{1}{1 + \\exp(-\\hat{f}(x))} \\] show references work , first define ResponseBinaryClassif object. Therefore, use mtcars dataset create new binary target variable fast \\(\\text{qsec} < 17\\) slow \\(\\text{qsec} \\geq 17\\) cars create response object: access underlying representation response class (binary variable) one can use $getResponse(). initialization new response object, prediction \\(\\hat{f} \\\\mathbb{R}\\) initialized zeros. can also use response object calculate transformed predictions \\(\\hat{\\pi} \\[0,1]\\): case binary classification, can use response object calculate predictions label basis using specified threshold \\(\\): \\[ \\hat{y} = 1 \\ \\ \\text{} \\ \\ \\hat{\\pi}(x) \\geq \\] default threshold 0.5: setting threshold 0.6 observe now class predicted negative: behavior nothing references moment. fitting component-wise boosting model, predictions adjusted Compboost object. reference comes : look predictions shows difference values training. fitting process, predictions response object updated model:","code":"df = mtcars[, c(\"mpg\", \"disp\", \"hp\", \"drat\", \"wt\")] df$qsec_cat = ifelse(mtcars$qsec < 17, \"fast\", \"slow\")  obj_response = ResponseBinaryClassif$new(\"qsec_cat\", \"fast\", df$qsec_cat) obj_response #>  #> Binary classification response of target \"qsec_cat\" and threshold 0.5 #> ResponseBinaryClassifPrinter knitr::kable(head(data.frame(   target = df$qsec_cat,   target_representation = obj_response$getResponse(),   prediction_initialization = obj_response$getPrediction(),   prediction_transformed = obj_response$getPredictionTransform() ))) obj_response$getThreshold() #> [1] 0.5 head(obj_response$getPredictionResponse()) #>      [,1] #> [1,]    1 #> [2,]    1 #> [3,]    1 #> [4,]    1 #> [5,]    1 #> [6,]    1 obj_response$setThreshold(0.6) head(obj_response$getPredictionResponse()) #>      [,1] #> [1,]   -1 #> [2,]   -1 #> [3,]   -1 #> [4,]   -1 #> [5,]   -1 #> [6,]   -1 cboost = boostSplines(data = df, target = obj_response,   iterations = 2000L, trace = 500L) #>    1/2000   risk = 0.59  time = 0    #>  500/2000   risk = 0.22  time = 12328    #> 1000/2000   risk = 0.15  time = 32687    #> 1500/2000   risk = 0.12  time = 61197    #> 2000/2000   risk = 0.1  time = 97854    #>  #>  #> Train 2000 iterations in 0 Seconds. #> Final risk based on the train set: 0.1 knitr::kable(head(data.frame(   target = df$qsec_cat,   prediction = obj_response$getPrediction(),   prediction_transformed = obj_response$getPredictionTransform(),   prediction_response = obj_response$getPredictionResponse() )))"},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/robust_regression.html","id":"how-does-it-work","dir":"Articles > Getting_started","previous_headings":"","what":"How does it work?","title":"Quantile/Robust regression","text":"Predicting quantiles controlled choice loss function. Quantile regression originally motivated general additive/linear models (see ). use define following loss function: \\[ L(y, f(x)) = h|y - f(x)| \\] \\[ h = \\left\\{ \\begin{array}{ccc} 2q       & \\ \\ \\text{} \\ \\ & y - f(x) > 0 \\\\ 2(1 - q) & \\ \\ \\text{} \\ \\ & \\text{otherwise} \\end{array} \\right. \\] \\(q\\) q-quantile. Visualizing loss \\(y - f(x)\\) shows , e.g., boosting 90 % quantile punishes residuals harder residuals smaller zeros leads optimization 90 % quantile:","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/robust_regression.html","id":"simulate-data","dir":"Articles > Getting_started","previous_headings":"","what":"Simulate data","title":"Quantile/Robust regression","text":"show effect quantile/robust regression simulate data follows sinus curve 20 outliers:","code":"nsim = 1000 noutlier = 20 outlier_mean = 1e6  x = runif(nsim, 0, 10) y = 3 + 2 * sin(x) + rnorm(nsim, 0, 1)  outlier_idx = sample(nsim, noutlier) y[outlier_idx] = sample(x = c(-1, 1), size = noutlier, replace = TRUE) * rnorm(noutlier, outlier_mean, 1)  df = data.frame(x = x, y = y) #> Warning: Removed 20 rows containing missing values (`geom_point()`)."},{"path":[]},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/robust_regression.html","id":"boosting-the-median","dir":"Articles > Getting_started","previous_headings":"How to use","what":"Boosting the median","title":"Quantile/Robust regression","text":"need use LossQuantile class generator. example, boost median (50 % quantile) pass 0.5 constructor: little side note: Boosting median 50 % quantile equivalent conduct boosting absolute loss. loss_quantile90 loss object can now used define train new Compboost object: Using quantiles, absolute loss median, also known robust regression. visualize effect outliers regression, also train model QuadraticLoss:  Boosting quadratic loss quite sensitive outliers. course, example exaggerated show effect boosting median.","code":"loss_quantile50 = LossQuantile$new(0.5) loss_quantile50 #> LossQuantile: L(y,x) = h|y - f(x)| #>  #> h = 2q        if  y - f(x) > 0 #>   h = 2(1 - q)  otherwise #>  #>   with quantile q = 0.5 cboost_quantile50 = boostSplines(data = df, target = \"y\", loss = loss_quantile50, iterations = 1000L, trace = 200L) #>    1/1000   risk = 2e+04  time = 0    #>  200/1000   risk = 2e+04  time = 12229    #>  400/1000   risk = 2e+04  time = 25500    #>  600/1000   risk = 2e+04  time = 40024    #>  800/1000   risk = 2e+04  time = 55958    #> 1000/1000   risk = 2e+04  time = 73359    #>  #>  #> Train 1000 iterations in 0 Seconds. #> Final risk based on the train set: 2e+04 cboost_mean = boostSplines(data = df, target = \"y\", iterations = 1000L, trace = 0) #> Train 1000 iterations in 0 Seconds. #> Final risk based on the train set: 9.9e+09  df_plot = data.frame(   feature = rep(x, times = 2),   preds = c(cboost_mean$predict(), cboost_quantile50$predict()),   estimator = rep(c(\"mean\", \"median\"), each = length(x)) )  library(ggsci)  gg1 = ggplot() +   geom_point(data = df, aes(x = x, y = y), show.legend = FALSE, alpha = 0.5) +   geom_line(data = df_plot, aes(x = feature, y = preds, color = estimator), size = 1.2) +   ggtitle(\"Full target range (with outliers)\") #> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. #> ℹ Please use `linewidth` instead.  gg2 = ggplot() +   geom_point(data = df, aes(x = x, y = y), show.legend = FALSE, alpha = 0.5) +   geom_line(data = df_plot, aes(x = feature, y = preds, color = estimator), size = 1.2) +   ggtitle(\"Real target range (without outliers)\") +   ylim(-2, 8)  (gg1 | gg2) +   plot_layout(guides = \"collect\") &   theme(legend.position = \"bottom\") &   theme_tufte() &   scale_color_jama() #> Warning: Removed 20 rows containing missing values (`geom_point()`). #> Warning: Removed 1000 rows containing missing values (`geom_line()`)."},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/robust_regression.html","id":"boosting-arbitrary-quantiles","dir":"Articles > Getting_started","previous_headings":"How to use","what":"Boosting arbitrary quantiles","title":"Quantile/Robust regression","text":"Instead boosting median can boost quantile like. following example, boost 10 % 90 % quantile get “kind confidence interval”. careful using term confidence interval . predictions estimated independently may tighten boundaries:","code":"cboost = boostSplines(data = df, target = \"y\", iterations = 1000, trace = 0) #> Train 1000 iterations in 0 Seconds. #> Final risk based on the train set: 9.9e+09 cboost_10 = boostSplines(data = df, target = \"y\", loss = LossQuantile$new(0.1), iterations = 1000, trace = 0) #> Train 1000 iterations in 0 Seconds. #> Final risk based on the train set: 2e+04 cboost_50 = boostSplines(data = df, target = \"y\", loss = LossQuantile$new(0.5), iterations = 1000, trace = 0) #> Train 1000 iterations in 0 Seconds. #> Final risk based on the train set: 2e+04 cboost_90 = boostSplines(data = df, target = \"y\", loss = LossQuantile$new(0.9), iterations = 1000, trace = 0) #> Train 1000 iterations in 0 Seconds. #> Final risk based on the train set: 2e+04  df_pred = data.frame(   feat = x,   target = y,   mean = cboost$predict(),   quantile_10 = cboost_10$predict(),   quantile_50 = cboost_50$predict(),   quantile_90 = cboost_90$predict() ) df_pred = tidyr::gather(data = df_pred, key = \"estimator\", value = \"pred\", mean:quantile_90)  gg1 = ggplot() +   geom_point(data = df_pred, aes(x = feat, y = target), alpha = 0.5) +   geom_line(data = df_pred, aes(x = feat, y = pred, color = estimator, linetype = estimator), size = 1.2) +   xlab(\"Feature\") +   ylab(\"Target\") +   ggtitle(\"Full target range (with outliers)\")  gg2 = ggplot() +   geom_point(data = df_pred, aes(x = feat, y = target), alpha = 0.5) +   geom_line(data = df_pred, aes(x = feat, y = pred, color = estimator, linetype = estimator), size = 1.2) +   ylim(-2, 8) +   xlab(\"Feature\") +   ylab(\"\") +   ggtitle(\"Real target range (without outliers)\")  (gg1 | gg2) +   plot_layout(guides = \"collect\") &   theme(legend.position = \"bottom\") &   theme_tufte() &   scale_color_jama() #> Warning: Removed 80 rows containing missing values (`geom_point()`). #> Warning: Removed 1000 rows containing missing values (`geom_line()`)."},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/robust_regression.html","id":"comments","dir":"Articles > Getting_started","previous_headings":"","what":"Comments","title":"Quantile/Robust regression","text":"Boosting median technique get robust model. Nevertheless, boosting mean nice estimation properties. variance estimators introduced use quantile loss. get precise predictions, need data reduce variance. Another loss superior terms variance Huber loss LossHuber$new() uses quadratic approximation around zero linear extrapolation threshold \\(\\delta\\).","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/use_case.html","id":"data-titanic-passenger-survival-data-set","dir":"Articles > Getting_started","previous_headings":"","what":"Data: Titanic Passenger Survival Data Set","title":"","text":"use titanic dataset binary classification survived. First store train test data two data frames remove rows contains NAs: next step transform response factor intuitive levels:","code":"# Store train and test data: df_train = na.omit(titanic::titanic_train)  str(df_train) #> 'data.frame':    714 obs. of  12 variables: #>  $ PassengerId: int  1 2 3 4 5 7 8 9 10 11 ... #>  $ Survived   : int  0 1 1 1 0 0 0 1 1 1 ... #>  $ Pclass     : int  3 1 3 1 3 1 3 3 2 3 ... #>  $ Name       : chr  \"Braund, Mr. Owen Harris\" \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\" \"Heikkinen, Miss. Laina\" \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\" ... #>  $ Sex        : chr  \"male\" \"female\" \"female\" \"female\" ... #>  $ Age        : num  22 38 26 35 35 54 2 27 14 4 ... #>  $ SibSp      : int  1 1 0 1 0 0 3 0 1 1 ... #>  $ Parch      : int  0 0 0 0 0 0 1 2 0 1 ... #>  $ Ticket     : chr  \"A/5 21171\" \"PC 17599\" \"STON/O2. 3101282\" \"113803\" ... #>  $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ... #>  $ Cabin      : chr  \"\" \"C85\" \"\" \"C123\" ... #>  $ Embarked   : chr  \"S\" \"C\" \"S\" \"S\" ... #>  - attr(*, \"na.action\")= 'omit' Named int [1:177] 6 18 20 27 29 30 32 33 37 43 ... #>   ..- attr(*, \"names\")= chr [1:177] \"6\" \"18\" \"20\" \"27\" ... df_train$Survived = factor(df_train$Survived, labels = c(\"no\", \"yes\"))"},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/use_case.html","id":"initializing-model","dir":"Articles > Getting_started","previous_headings":"","what":"Initializing Model","title":"","text":"Due R6 API necessary create new class object gets data, target character, used loss. Note important give initialized loss object: Use initialized object loss gives opportunity use loss initialized custom offset.","code":"cboost = Compboost$new(data = df_train, target = \"Survived\", oob_fraction = 0.3)"},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/use_case.html","id":"adding-base-learner","dir":"Articles > Getting_started","previous_headings":"","what":"Adding Base-Learner","title":"","text":"Adding new base-learners also done giving character indicate feature. second argument important name identifier factory since can define multiple base-learner source.","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/use_case.html","id":"numerical-features","dir":"Articles > Getting_started","previous_headings":"Adding Base-Learner","what":"Numerical Features","title":"","text":"instance, can define spline linear base-learner feature: Additional arguments can specified naming base-learner. complete list see functionality project page:","code":"# Spline base-learner of age: cboost$addBaselearner(\"Age\", \"spline\", BaselearnerPSpline)  # Linear base-learner of age (degree = 1 with intercept is default): cboost$addBaselearner(\"Age\", \"linear\", BaselearnerPolynomial) # Spline base-learner of fare: cboost$addBaselearner(\"Fare\", \"spline\", BaselearnerPSpline, degree = 2,   n_knots = 14, penalty = 10, differences = 2)"},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/use_case.html","id":"categorical-features","dir":"Articles > Getting_started","previous_headings":"Adding Base-Learner","what":"Categorical Features","title":"","text":"adding categorical features use dummy coded representation ridge penalty: Finally, can check factories registered:","code":"cboost$addBaselearner(\"Sex\", \"categorical\", BaselearnerCategoricalRidge) cboost$getBaselearnerNames() #> [1] \"Age_spline\"      \"Age_linear\"      \"Fare_spline\"     \"Sex_categorical\""},{"path":[]},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/use_case.html","id":"time-logger","dir":"Articles > Getting_started","previous_headings":"Define Logger","what":"Time logger","title":"","text":"logger logs elapsed time. time unit can one microseconds, seconds minutes. logger stops max_time reached. use logger stopper :","code":"cboost$addLogger(logger = LoggerTime, use_as_stopper = FALSE, logger_id = \"time\",   max_time = 0, time_unit = \"microseconds\")"},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/use_case.html","id":"train-model-and-access-elements","dir":"Articles > Getting_started","previous_headings":"","what":"Train Model and Access Elements","title":"","text":"Objects Compboost class member functions getCoef(), getInbagRisk() predict() access results: obtain vector selected base learners use getSelectedBaselearner(): can also access predictions directly response object cboost$response cboost$response_oob. Note $response_oob created automatically defining oob_fraction within constructor:","code":"cboost$train(2000, trace = 250) #>    1/2000   risk = 0.66  oob_risk = 0.69   time = 0    #>  250/2000   risk = 0.5  oob_risk = 0.51   time = 18706    #>  500/2000   risk = 0.47  oob_risk = 0.49   time = 39918    #>  750/2000   risk = 0.47  oob_risk = 0.49   time = 63123    #> 1000/2000   risk = 0.46  oob_risk = 0.49   time = 88997    #> 1250/2000   risk = 0.46  oob_risk = 0.49   time = 117142    #> 1500/2000   risk = 0.46  oob_risk = 0.49   time = 147031    #> 1750/2000   risk = 0.46  oob_risk = 0.49   time = 178643    #> 2000/2000   risk = 0.46  oob_risk = 0.49   time = 212721    #>  #>  #> Train 2000 iterations in 0 Seconds. #> Final risk based on the train set: 0.46 cboost #>  #>  #> Component-Wise Gradient Boosting #>  #> Target variable: Survived #> Number of base-learners: 4 #> Learning rate: 0.05 #> Iterations: 2000 #>  #> Offset: 0.4811 #>  #> LossBinomial: L(y,x) = log(1 + exp(-2yf(x)) str(cboost$getCoef()) #> List of 4 #>  $ Age_spline     : num [1:24, 1] -6.775 -1.912 -0.575 1.342 -0.105 ... #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerPSpline\" #>  $ Fare_spline    : num [1:17, 1] 1.166 0.216 -0.492 -1.284 -1.377 ... #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerPSpline\" #>  $ Sex_categorical: num [1:2, 1] -1.42 0.88 #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : chr [1:2] \"female\" \"male\" #>   .. ..$ : NULL #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerCategoricalRidge\" #>  $ offset         : num 0.481 str(cboost$getInbagRisk()) #>  num [1:2001] 0.665 0.662 0.659 0.656 0.653 ... str(cboost$predict()) #>  num [1:500, 1] -1.4157 -0.4954 1.5242 1.1318 0.0482 ... table(cboost$getSelectedBaselearner()) #>  #>      Age_spline     Fare_spline Sex_categorical  #>            1314             347             339 oob_label = cboost$response_oob$getResponse() oob_pred = cboost$response_oob$getPredictionResponse() table(true_label = oob_label, predicted = oob_pred) #>           predicted #> true_label -1  1 #>         -1 69 30 #>         1  16 99"},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/use_case.html","id":"retrain-the-model","dir":"Articles > Getting_started","previous_headings":"","what":"Retrain the Model","title":"","text":"continue training set whole model another iteration simply re-call train():","code":"cboost$train(3000) #>  #> You have already trained 2000 iterations. #> Train 1000 additional iterations. #>  #> 2025/3000   risk = 0.46  oob_risk = 0.49   time = 216370    #> 2100/3000   risk = 0.46  oob_risk = 0.49   time = 226928    #> 2175/3000   risk = 0.46  oob_risk = 0.49   time = 237590    #> 2250/3000   risk = 0.46  oob_risk = 0.49   time = 248416    #> 2325/3000   risk = 0.46  oob_risk = 0.49   time = 259473    #> 2400/3000   risk = 0.46  oob_risk = 0.49   time = 270709    #> 2475/3000   risk = 0.46  oob_risk = 0.49   time = 282142    #> 2550/3000   risk = 0.46  oob_risk = 0.49   time = 293745    #> 2625/3000   risk = 0.46  oob_risk = 0.49   time = 305540    #> 2700/3000   risk = 0.46  oob_risk = 0.49   time = 317506    #> 2775/3000   risk = 0.46  oob_risk = 0.49   time = 329700    #> 2850/3000   risk = 0.46  oob_risk = 0.49   time = 342115    #> 2925/3000   risk = 0.46  oob_risk = 0.49   time = 354726    #> 3000/3000   risk = 0.46  oob_risk = 0.49   time = 367765  str(cboost$getCoef()) #> List of 4 #>  $ Age_spline     : num [1:24, 1] -8.239 -1.623 -0.767 1.494 -0.173 ... #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerPSpline\" #>  $ Fare_spline    : num [1:17, 1] 1.204 0.229 -0.502 -1.375 -1.446 ... #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerPSpline\" #>  $ Sex_categorical: num [1:2, 1] -1.456 0.899 #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : chr [1:2] \"female\" \"male\" #>   .. ..$ : NULL #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerCategoricalRidge\" #>  $ offset         : num 0.481 str(cboost$getInbagRisk()) #>  num [1:3001] 0.665 0.662 0.659 0.656 0.653 ... table(cboost$getSelectedBaselearner()) #>  #>      Age_spline     Fare_spline Sex_categorical  #>            2172             438             390"},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/use_case.html","id":"next-steps","dir":"Articles > Getting_started","previous_headings":"","what":"Next steps","title":"","text":"look visualization capabilities package. See loss functions effect model training.","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/visualizations.html","id":"fit-compboost","dir":"Articles > Getting_started","previous_headings":"","what":"Fit compboost","title":"Visualizing a compboost model","text":"data set use mpg: want model miles per gallon (mpg). features include linear centered spline hp, wt, qsec. Additionally, add categorical base learner number cylinders cyl:","code":"mtcars$cyl = as.factor(mtcars$cyl)  set.seed(31415) cboost = Compboost$new(data = mtcars, target = \"mpg\", learning_rate = 0.02, oob_fraction = 0.2)  cboost$addComponents(\"hp\", df = 3) cboost$addComponents(\"wt\", df = 3) cboost$addComponents(\"qsec\", df = 3) cboost$addBaselearner(\"cyl\", \"ridge\", BaselearnerCategoricalRidge, df = 3)  cboost$train(500L, trace = 100L) #>   1/500   risk = 17  oob_risk = 17    #> 100/500   risk = 3  oob_risk = 2.8    #> 200/500   risk = 2.4  oob_risk = 3.2    #> 300/500   risk = 2.3  oob_risk = 3.2    #> 400/500   risk = 2.3  oob_risk = 3.1    #> 500/500   risk = 2.2  oob_risk = 3.1    #>  #>  #> Train 500 iterations in 0 Seconds. #> Final risk based on the train set: 2.2"},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/visualizations.html","id":"visualize-risk-feature-importance-and-selection-traces","dir":"Articles > Getting_started","previous_headings":"","what":"Visualize risk, feature importance, and selection traces","title":"Visualizing a compboost model","text":"starting point analyzing component-wise boosting model take look train validation risk:  can see, best validation risk iteration 98. Hence, set model iteration: Next, interested important base learners/features:  last thing can get better overview model look features/base learners included model:","code":"plotRisk(cboost) m_optimal = which.min(cboost$getLoggerData()[[\"oob_risk\"]]) cboost$train(m_optimal) plotFeatureImportance(cboost) plotBaselearnerTraces(cboost)"},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/visualizations.html","id":"visualize-base-learner-and-partial-effects","dir":"Articles > Getting_started","previous_headings":"","what":"Visualize base learner and partial effects","title":"Visualizing a compboost model","text":"Next, want deep dive effects individual features, .e, effect base learners. end, plot partial effects important feature wt:  observe clear negative trend, meaning increasing weight indicates lower mpg. Additionally, can visualize individual base learners. example categorical feature cyl:  , observe 4 cylinder indicates positive contribution mpg 6 8 cylinder reducing .","code":"plotPEUni(cboost, \"wt\") plotBaselearner(cboost, \"cyl_ridge\")"},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/visualizations.html","id":"visualizing-individual-predictions","dir":"Articles > Getting_started","previous_headings":"","what":"Visualizing individual predictions","title":"Visualizing a compboost model","text":"predictions also want get idea specific contribution feature predicted score. Therefore, take look first observation validation data set:  can see, prediction dominated offset. remove figure set offset = FALSE:  wt hp positive contribution predicted score. means car requires less fuel 6 cylinder slightly increases mpg prediction.","code":"plotIndividualContribution(cboost, newdata = cboost$data_oob[1, ]) plotIndividualContribution(cboost, newdata = cboost$data_oob[1, ], offset = FALSE)"},{"path":"https://schalkdaniel.github.io/compboost/articles/getting_started/visualizations.html","id":"visualizing-tensor-products","dir":"Articles > Getting_started","previous_headings":"","what":"Visualizing tensor products","title":"Visualizing a compboost model","text":"last visualization convenience wrapper illustrate interactions included tensors. Therefore, add tensors model: Depending feature combination (numeric - numeric, numeric - categorical, categorical - categorical) different visualization technique used:","code":"mtcars$vs = as.factor(mtcars$vs) mtcars$gear = as.factor(mtcars$gear)  set.seed(31415) cboost = Compboost$new(data = mtcars, target = \"mpg\", oob_fraction = 0.2)  cboost$addTensor(\"wt\", \"qsec\", df = 2) cboost$addTensor(\"hp\", \"cyl\", df = 2) cboost$addTensor(\"gear\", \"vs\", df = 2)  cboost$train(500L, trace = 100L) #>   1/500   risk = 16  oob_risk = 16    #> 100/500   risk = 2.4  oob_risk = 4    #> 200/500   risk = 2.2  oob_risk = 4.4    #> 300/500   risk = 2.1  oob_risk = 4.4    #> 400/500   risk = 2.1  oob_risk = 4.5    #> 500/500   risk = 2  oob_risk = 4.5    #>  #>  #> Train 500 iterations in 0 Seconds. #> Final risk based on the train set: 2 table(cboost$getSelectedBaselearner()) #>  #> gear_vs_tensor  hp_cyl_tensor wt_qsec_tensor  #>            258            181             61 library(ggplot2)  gg1 = plotTensor(cboost, \"wt_qsec_tensor\") + ggtitle(\"Num - Num\") gg2 = plotTensor(cboost, \"hp_cyl_tensor\") + ggtitle(\"Num - Cat\") gg3 = plotTensor(cboost, \"gear_vs_tensor\") + ggtitle(\"Cat - Cat\")  library(patchwork)  gg1 | gg2 | gg3"},{"path":"https://schalkdaniel.github.io/compboost/articles/use_case.html","id":"data-titanic-passenger-survival-data-set","dir":"Articles","previous_headings":"","what":"Data: Titanic Passenger Survival Data Set","title":"Use-case","text":"use titanic dataset binary classification survived. First store train test data two data frames remove rows contains NAs: next step transform response factor intuitive levels:","code":"# Store train and test data: df_train = na.omit(titanic::titanic_train)  str(df_train) #> 'data.frame':    714 obs. of  12 variables: #>  $ PassengerId: int  1 2 3 4 5 7 8 9 10 11 ... #>  $ Survived   : int  0 1 1 1 0 0 0 1 1 1 ... #>  $ Pclass     : int  3 1 3 1 3 1 3 3 2 3 ... #>  $ Name       : chr  \"Braund, Mr. Owen Harris\" \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\" \"Heikkinen, Miss. Laina\" \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\" ... #>  $ Sex        : chr  \"male\" \"female\" \"female\" \"female\" ... #>  $ Age        : num  22 38 26 35 35 54 2 27 14 4 ... #>  $ SibSp      : int  1 1 0 1 0 0 3 0 1 1 ... #>  $ Parch      : int  0 0 0 0 0 0 1 2 0 1 ... #>  $ Ticket     : chr  \"A/5 21171\" \"PC 17599\" \"STON/O2. 3101282\" \"113803\" ... #>  $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ... #>  $ Cabin      : chr  \"\" \"C85\" \"\" \"C123\" ... #>  $ Embarked   : chr  \"S\" \"C\" \"S\" \"S\" ... #>  - attr(*, \"na.action\")= 'omit' Named int [1:177] 6 18 20 27 29 30 32 33 37 43 ... #>   ..- attr(*, \"names\")= chr [1:177] \"6\" \"18\" \"20\" \"27\" ... df_train$Survived = factor(df_train$Survived, labels = c(\"no\", \"yes\"))"},{"path":"https://schalkdaniel.github.io/compboost/articles/use_case.html","id":"initializing-model","dir":"Articles","previous_headings":"","what":"Initializing Model","title":"Use-case","text":"Due R6 API necessary create new class object gets data, target character, used loss. Note important give initialized loss object: Use initialized object loss gives opportunity use loss initialized custom offset.","code":"cboost = Compboost$new(data = df_train, target = \"Survived\", oob_fraction = 0.3)"},{"path":"https://schalkdaniel.github.io/compboost/articles/use_case.html","id":"adding-base-learner","dir":"Articles","previous_headings":"","what":"Adding Base-Learner","title":"Use-case","text":"Adding new base-learners also done giving character indicate feature. second argument important name identifier factory since can define multiple base-learner source.","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/use_case.html","id":"numerical-features","dir":"Articles","previous_headings":"Adding Base-Learner","what":"Numerical Features","title":"Use-case","text":"instance, can define spline linear base-learner feature: Additional arguments can specified naming base-learner. complete list see functionality project page:","code":"# Spline base-learner of age: cboost$addBaselearner(\"Age\", \"spline\", BaselearnerPSpline)  # Linear base-learner of age (degree = 1 with intercept is default): cboost$addBaselearner(\"Age\", \"linear\", BaselearnerPolynomial) # Spline base-learner of fare: cboost$addBaselearner(\"Fare\", \"spline\", BaselearnerPSpline, degree = 2,   n_knots = 14, penalty = 10, differences = 2)"},{"path":"https://schalkdaniel.github.io/compboost/articles/use_case.html","id":"categorical-features","dir":"Articles","previous_headings":"Adding Base-Learner","what":"Categorical Features","title":"Use-case","text":"adding categorical features use dummy coded representation ridge penalty: Finally, can check factories registered:","code":"cboost$addBaselearner(\"Sex\", \"categorical\", BaselearnerCategoricalRidge) cboost$getBaselearnerNames() #> [1] \"Age_spline\"      \"Age_linear\"      \"Fare_spline\"     \"Sex_categorical\""},{"path":[]},{"path":"https://schalkdaniel.github.io/compboost/articles/use_case.html","id":"time-logger","dir":"Articles","previous_headings":"Define Logger","what":"Time logger","title":"Use-case","text":"logger logs elapsed time. time unit can one microseconds, seconds minutes. logger stops max_time reached. use logger stopper :","code":"cboost$addLogger(logger = LoggerTime, use_as_stopper = FALSE, logger_id = \"time\",   max_time = 0, time_unit = \"microseconds\")"},{"path":"https://schalkdaniel.github.io/compboost/articles/use_case.html","id":"train-model-and-access-elements","dir":"Articles","previous_headings":"","what":"Train Model and Access Elements","title":"Use-case","text":"Objects Compboost class member functions getCoef(), getInbagRisk() predict() access results: obtain vector selected base learners use getSelectedBaselearner(): can also access predictions directly response object cboost$response cboost$response_oob. Note $response_oob created automatically defining oob_fraction within constructor:","code":"cboost$train(2000, trace = 250) #>    1/2000   risk = 0.67  oob_risk = 0.68   time = 0    #>  250/2000   risk = 0.52  oob_risk = 0.47   time = 18615    #>  500/2000   risk = 0.5  oob_risk = 0.45   time = 39378    #>  750/2000   risk = 0.5  oob_risk = 0.44   time = 62481    #> 1000/2000   risk = 0.49  oob_risk = 0.44   time = 88532    #> 1250/2000   risk = 0.49  oob_risk = 0.44   time = 115818    #> 1500/2000   risk = 0.49  oob_risk = 0.45   time = 145205    #> 1750/2000   risk = 0.49  oob_risk = 0.45   time = 176641    #> 2000/2000   risk = 0.49  oob_risk = 0.45   time = 210199    #>  #>  #> Train 2000 iterations in 0 Seconds. #> Final risk based on the train set: 0.49 cboost #>  #>  #> Component-Wise Gradient Boosting #>  #> Target variable: Survived #> Number of base-learners: 4 #> Learning rate: 0.05 #> Iterations: 2000 #>  #> Offset: 0.4305 #>  #> LossBinomial: L(y,x) = log(1 + exp(-2yf(x)) str(cboost$getCoef()) #> List of 4 #>  $ Age_spline     : num [1:24, 1] -9.087 -1.435 0.531 0.859 1.095 ... #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerPSpline\" #>  $ Fare_spline    : num [1:17, 1] 1.088 0.257 -0.608 -1.309 -1.441 ... #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerPSpline\" #>  $ Sex_categorical: num [1:2, 1] -1.279 0.796 #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : chr [1:2] \"female\" \"male\" #>   .. ..$ : NULL #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerCategoricalRidge\" #>  $ offset         : num 0.431 str(cboost$getInbagRisk()) #>  num [1:2001] 0.671 0.668 0.665 0.662 0.66 ... str(cboost$predict()) #>  num [1:500, 1] -1.633 -0.474 -2.146 1.038 0.558 ... table(cboost$getSelectedBaselearner()) #>  #>      Age_spline     Fare_spline Sex_categorical  #>            1328             353             319 oob_label = cboost$response_oob$getResponse() oob_pred = cboost$response_oob$getPredictionResponse() table(true_label = oob_label, predicted = oob_pred) #>           predicted #> true_label  -1   1 #>         -1  69  24 #>         1   14 107"},{"path":"https://schalkdaniel.github.io/compboost/articles/use_case.html","id":"retrain-the-model","dir":"Articles","previous_headings":"","what":"Retrain the Model","title":"Use-case","text":"continue training set whole model another iteration simply re-call train():","code":"cboost$train(3000) #>  #> You have already trained 2000 iterations. #> Train 1000 additional iterations. #>  #> 2025/3000   risk = 0.49  oob_risk = 0.45   time = 213903    #> 2100/3000   risk = 0.49  oob_risk = 0.45   time = 224443    #> 2175/3000   risk = 0.49  oob_risk = 0.45   time = 235200    #> 2250/3000   risk = 0.49  oob_risk = 0.45   time = 246056    #> 2325/3000   risk = 0.48  oob_risk = 0.45   time = 257499    #> 2400/3000   risk = 0.48  oob_risk = 0.45   time = 268752    #> 2475/3000   risk = 0.48  oob_risk = 0.45   time = 280200    #> 2550/3000   risk = 0.48  oob_risk = 0.45   time = 291826    #> 2625/3000   risk = 0.48  oob_risk = 0.45   time = 303996    #> 2700/3000   risk = 0.48  oob_risk = 0.45   time = 315987    #> 2775/3000   risk = 0.48  oob_risk = 0.45   time = 328168    #> 2850/3000   risk = 0.48  oob_risk = 0.45   time = 340519    #> 2925/3000   risk = 0.48  oob_risk = 0.45   time = 353074    #> 3000/3000   risk = 0.48  oob_risk = 0.45   time = 366315  str(cboost$getCoef()) #> List of 4 #>  $ Age_spline     : num [1:24, 1] -11.578 -1.112 0.542 0.684 1.397 ... #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerPSpline\" #>  $ Fare_spline    : num [1:17, 1] 1.078 0.286 -0.624 -1.403 -1.555 ... #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerPSpline\" #>  $ Sex_categorical: num [1:2, 1] -1.313 0.817 #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : chr [1:2] \"female\" \"male\" #>   .. ..$ : NULL #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerCategoricalRidge\" #>  $ offset         : num 0.431 str(cboost$getInbagRisk()) #>  num [1:3001] 0.671 0.668 0.665 0.662 0.66 ... table(cboost$getSelectedBaselearner()) #>  #>      Age_spline     Fare_spline Sex_categorical  #>            2158             473             369"},{"path":"https://schalkdaniel.github.io/compboost/articles/use_case.html","id":"visualizing-the-model","dir":"Articles","previous_headings":"","what":"Visualizing the model","title":"Use-case","text":"High-level plotting functions available visualize risk (plotRisk()), partial feature effects (plotBaselearner() univariate plotTensor() bivariate base learner), base learner selection traces (plotBaselearnerTraces()), feature importance (plotFeatureImportance())","code":"library(ggplot2) library(patchwork)  (plotRisk(cboost) |  plotBaselearnerTraces(cboost) |  plotFeatureImportance(cboost)) /   (plotBaselearner(cboost, \"Fare_spline\") |    plotBaselearner(cboost, \"Age_spline\") |    plotBaselearner(cboost, \"Sex_categorical\")) &   theme_minimal()"},{"path":"https://schalkdaniel.github.io/compboost/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Daniel Schalk. Author, maintainer. Janek Thomas. Author. Bernd Bischl. Author.","code":""},{"path":"https://schalkdaniel.github.io/compboost/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Schalk D, Thomas J, Bischl B (2018). “compboost: Modular Framework Component-Wise Boosting.” JOSS, 3(30), 967. doi:10.21105/joss.00967, https://doi.org/10.21105/joss.00967.","code":"@Article{,   author = {Daniel Schalk and Janek Thomas and Bernd Bischl},   title = {compboost: Modular Framework for Component-Wise Boosting},   doi = {10.21105/joss.00967},   url = {https://doi.org/10.21105/joss.00967},   year = {2018},   publisher = {Journal of Open Source Software},   volume = {3},   number = {30},   pages = {967},   journal = {JOSS}, }"},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"compboost-fast-and-flexible-component-wise-boosting-framework-","dir":"","previous_headings":"","what":"C++ Implementation of Component-Wise Boosting","title":"C++ Implementation of Component-Wise Boosting","text":"Documentation | Contributors | Release Notes","code":""},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"C++ Implementation of Component-Wise Boosting","text":"Component-wise boosting applies boosting framework statistical models, e.g., general additive models using component-wise smoothing splines. Boosting kinds models maintains interpretability enables unbiased model selection high dimensional feature spaces. R package compboost alternative implementation component-wise boosting written C++ obtain high runtime performance full memory control. main idea provide modular class system can extended without editing source code. Therefore, possible use R functions well C++ functions custom base-learners, losses, logging mechanisms stopping criteria. introduction overview functionality visit project page.","code":""},{"path":[]},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"cran-version","dir":"","previous_headings":"Installation","what":"CRAN version:","title":"C++ Implementation of Component-Wise Boosting","text":"","code":"install.packages(\"compboost\")"},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"developer-version","dir":"","previous_headings":"Installation","what":"Developer version:","title":"C++ Implementation of Component-Wise Boosting","text":"","code":"devtools::install_github(\"schalkdaniel/compboost\")"},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"examples","dir":"","previous_headings":"","what":"Examples","title":"C++ Implementation of Component-Wise Boosting","text":"examples rendered using compboost 0.1.1. fastest way train Compboost model use wrapper functions boostLinear() boostSplines():  extensive examples use R6 interface visit project page.","code":"cboost = boostSplines(data = iris, target = \"Sepal.Length\",   oob_fraction = 0.3, iterations = 500L, trace = 100L)  ggrisk = plotRisk(cboost) ggpe = plotPEUni(cboost, \"Petal.Length\") ggicont =  plotIndividualContribution(cboost, iris[70, ], offset = FALSE)  library(patchwork)  ggrisk + ggpe + ggicont"},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"save-and-load-models","dir":"","previous_headings":"","what":"Save and load models","title":"C++ Implementation of Component-Wise Boosting","text":"usage C++ objects backend, possible use Rs save() method save models. Instead, use $saveToJson(\"mymodel.json\") save model mymodel.json Compboost$new(file = \"mymodel.json\") load model:","code":"cboost = boostSplines(iris, \"Sepal.Width\") cboost$saveToJson(\"mymodel.json\")  cboost_new = Compboost$new(file = \"mymodel.json\")"},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"benchmark","dir":"","previous_headings":"","what":"Benchmark","title":"C++ Implementation of Component-Wise Boosting","text":"small benchmark conducted compare compboost mboost. purpose, runtime behavior memory consumption two packages compared. results benchmark can read . bigger benchmark adaptions increase runtime memory efficiency can found .","code":""},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"citing","dir":"","previous_headings":"","what":"Citing","title":"C++ Implementation of Component-Wise Boosting","text":"cite compboost publications, please use: Schalk et al., (2018). compboost: Modular Framework Component-Wise Boosting. Journal Open Source Software, 3(30), 967, https://doi.org/10.21105/joss.00967","code":"@article{schalk2018compboost,   author = {Daniel Schalk, Janek Thomas, Bernd Bischl},   title = {compboost: Modular Framework for Component-Wise Boosting},   URL = {https://doi.org/10.21105/joss.00967},   year = {2018},   publisher = {Journal of Open Source Software},   volume = {3},   number = {30},   pages = {967},   journal = {JOSS} }"},{"path":[]},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"on-your-local-machine","dir":"","previous_headings":"Testing","what":"On your local machine","title":"C++ Implementation of Component-Wise Boosting","text":"order test package functionality can use devtools test package local machine:","code":"devtools::test()"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalBinary.html","id":null,"dir":"Reference","previous_headings":"","what":"Base-learner factory for categorical feature on a binary base-learner basis — BaselearnerCategoricalBinary","title":"Base-learner factory for categorical feature on a binary base-learner basis — BaselearnerCategoricalBinary","text":"BaselearnerCategoricalBinary can used estimate effects one category categorical feature. base-learner gets data index vector observations assigned group. example, vector (, , b, b, , b), index vector (1, 2, 5) group . reduces memory load fasten fitting process.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalBinary.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Base-learner factory for categorical feature on a binary base-learner basis — BaselearnerCategoricalBinary","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalBinary.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Base-learner factory for categorical feature on a binary base-learner basis — BaselearnerCategoricalBinary","text":"","code":"BaselearnerCategoricalBinary$new(data_source, list(n_obs))"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalBinary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"arguments","title":"Base-learner factory for categorical feature on a binary base-learner basis — BaselearnerCategoricalBinary","text":"data_source [data object] data object class CategoricalData contains source data.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalBinary.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Base-learner factory for categorical feature on a binary base-learner basis — BaselearnerCategoricalBinary","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalBinary.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Base-learner factory for categorical feature on a binary base-learner basis — BaselearnerCategoricalBinary","text":"getData() Get data matrix target data used   modeling. summarizeFactory() Summarize base-learner factory object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalBinary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Base-learner factory for categorical feature on a binary base-learner basis — BaselearnerCategoricalBinary","text":"","code":"x = sample(c(\"one\",\"two\"), 20, TRUE) ds = CategoricalDataRaw$new(x, \"cat\") bl = BaselearnerCategoricalRidge$new(ds, \"one\")  bl$getData() #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #> [1,]    1    1    0    0    1    0    0    0    1     1     0     1     1     1 #> [2,]    0    0    1    1    0    1    1    1    0     0     1     0     0     0 #>      [,15] [,16] [,17] [,18] [,19] [,20] #> [1,]     0     1     0     0     1     0 #> [2,]     1     0     1     1     0     1 bl$summarizeFactory() #> Categorical base-learner of category cat"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalRidge.html","id":null,"dir":"Reference","previous_headings":"","what":"Base-learner factory for categorical feature using Ridge penalty — BaselearnerCategoricalRidge","title":"Base-learner factory for categorical feature using Ridge penalty — BaselearnerCategoricalRidge","text":"BaselearnerCategoricalRidge can used estimate effects  categorical features. categories included linear model using binary matrix. Ridge penalty enables unbiased feature selection setting penalty corresponding degree freedoms.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalRidge.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Base-learner factory for categorical feature using Ridge penalty — BaselearnerCategoricalRidge","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalRidge.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Base-learner factory for categorical feature using Ridge penalty — BaselearnerCategoricalRidge","text":"","code":"BaselearnerCategoricalRidge$new(data_source, list(df))"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalRidge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"arguments","title":"Base-learner factory for categorical feature using Ridge penalty — BaselearnerCategoricalRidge","text":"data_source [data object] data object contains source data.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalRidge.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Base-learner factory for categorical feature using Ridge penalty — BaselearnerCategoricalRidge","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalRidge.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Base-learner factory for categorical feature using Ridge penalty — BaselearnerCategoricalRidge","text":"getData() Get data matrix target data used   modeling. summarizeFactory() Summarize base-learner factory object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalRidge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Base-learner factory for categorical feature using Ridge penalty — BaselearnerCategoricalRidge","text":"","code":"x = sample(c(\"one\",\"two\"), 20, TRUE) ds = CategoricalDataRaw$new(x, \"cat\") bl = BaselearnerCategoricalRidge$new(ds, list(df = 1))  bl$getData() #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #> [1,]    1    1    0    1    1    1    0    0    1     1     0     1     1     1 #> [2,]    0    0    1    0    0    0    1    1    0     0     1     0     0     0 #>      [,15] [,16] [,17] [,18] [,19] [,20] #> [1,]     1     0     1     0     1     1 #> [2,]     0     1     0     1     0     0 bl$summarizeFactory() #> Categorical base-learner of category cat"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCentered.html","id":null,"dir":"Reference","previous_headings":"","what":"Base-learner factory to make regression using centered base learner — BaselearnerCentered","title":"Base-learner factory to make regression using centered base learner — BaselearnerCentered","text":"BaselearnerCentered creates base learner factory centered  using another base learner.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCentered.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Base-learner factory to make regression using centered base learner — BaselearnerCentered","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustom.html","id":null,"dir":"Reference","previous_headings":"","what":"Create custom base-learner factory by using R functions. — BaselearnerCustom","title":"Create custom base-learner factory by using R functions. — BaselearnerCustom","text":"BaselearnerCustom creates custom base-learner factory   setting custom R functions. factory object can registered   within base-learner list used training.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustom.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Create custom base-learner factory by using R functions. — BaselearnerCustom","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustom.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create custom base-learner factory by using R functions. — BaselearnerCustom","text":"","code":"BaselearnerCustom$new(data_source, list(instantiate_fun,   train_fun, predict_fun, param_fun))"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustom.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create custom base-learner factory by using R functions. — BaselearnerCustom","text":"data_source [Data Object] Data object contains source data. instantiate_fun [function] R function transform source data. details see   Details. train_fun [function] R function train base-learner target data.   details see Details. predict_fun [function] R function predict object returned train.   details see Details. param_fun [function] R function extract parameter object returned   train. details see Details.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustom.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create custom base-learner factory by using R functions. — BaselearnerCustom","text":"function must following structure: instantiateData(X) { ... return (X_trafo) } matrix argument   X matrix return object. train(y, X) { ... return (SEXP) } vector argument y   matrix argument X. target data used X   y contains response. function can return R   object stored within SEXP. predict(model, newdata) { ... return (prediction) } returned   object train function passed model   argument newdata contains new matrix used predicting. extractParameter() { ... return (parameters) } , model   contains object returned train. returned object must   matrix containing estimated parameter. parameter   estimated one can return NA. example see Examples.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustom.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Create custom base-learner factory by using R functions. — BaselearnerCustom","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustom.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Create custom base-learner factory by using R functions. — BaselearnerCustom","text":"getData() Get data matrix target data used   modeling. summarizeFactory() Summarize base-learner factory object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustom.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create custom base-learner factory by using R functions. — BaselearnerCustom","text":"","code":"# Sample data: data_mat = cbind(1, 1:10) y = 2 + 3 * 1:10  # Create new data object: data_source = InMemoryData$new(data_mat, \"my_data_name\")  instantiateDataFun = function (X) {   return(X) } # Ordinary least squares estimator: trainFun = function (y, X) {   return(solve(t(X) %*% X) %*% t(X) %*% y) } predictFun = function (model, newdata) {   return(as.matrix(newdata %*% model)) } extractParameter = function (model) {   return(as.matrix(model)) }  # Create new custom linear base-learner factory: custom_lin_factory = BaselearnerCustom$new(data_source,   list(instantiate_fun = instantiateDataFun, train_fun = trainFun,     predict_fun = predictFun, param_fun = extractParameter))  # Get the transformed data: custom_lin_factory$getData() #>       [,1] [,2] #>  [1,]    1    1 #>  [2,]    1    2 #>  [3,]    1    3 #>  [4,]    1    4 #>  [5,]    1    5 #>  [6,]    1    6 #>  [7,]    1    7 #>  [8,]    1    8 #>  [9,]    1    9 #> [10,]    1   10  # Summarize factory: custom_lin_factory$summarizeFactory() #> Custom base-learner Factory: #> \t- Name of the used data: my_data_name #> \t- Factory creates the following base-learner: custom"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustomCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"Create custom cpp base-learner factory by using cpp functions and external\npointer. — BaselearnerCustomCpp","title":"Create custom cpp base-learner factory by using cpp functions and external\npointer. — BaselearnerCustomCpp","text":"BaselearnerCustomCpp creates custom base-learner factory   setting custom C++ functions. factory object can registered   within base-learner list used training.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustomCpp.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Create custom cpp base-learner factory by using cpp functions and external\npointer. — BaselearnerCustomCpp","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustomCpp.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create custom cpp base-learner factory by using cpp functions and external\npointer. — BaselearnerCustomCpp","text":"","code":"BaselearnerCustomCpp$new(data_source, list(instantiate_ptr,   train_ptr, predict_ptr))"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustomCpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create custom cpp base-learner factory by using cpp functions and external\npointer. — BaselearnerCustomCpp","text":"data_source [Data Object] Data object contains source data. instantiate_ptr [externalptr] External pointer C++ instantiate data function. train_ptr [externalptr] External pointer C++ train function. predict_ptr [externalptr] External pointer C++ predict function.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustomCpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create custom cpp base-learner factory by using cpp functions and external\npointer. — BaselearnerCustomCpp","text":"example see extending compboost vignette function   getCustomCppExample.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustomCpp.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Create custom cpp base-learner factory by using cpp functions and external\npointer. — BaselearnerCustomCpp","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustomCpp.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Create custom cpp base-learner factory by using cpp functions and external\npointer. — BaselearnerCustomCpp","text":"getData() Get data matrix target data used   modeling. summarizeFactory() Summarize base-learner factory object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustomCpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create custom cpp base-learner factory by using cpp functions and external\npointer. — BaselearnerCustomCpp","text":"","code":"if (FALSE) { # Sample data: data_mat = cbind(1, 1:10) y = 2 + 3 * 1:10  # Create new data object: data_source = InMemoryData$new(data_mat, \"my_data_name\")  # Source the external pointer exposed by using XPtr: Rcpp::sourceCpp(code = getCustomCppExample(silent = TRUE))  # Create new linear base-learner: custom_cpp_factory = BaselearnerCustomCpp$new(data_source,   list(instantiate_ptr = dataFunSetter(), train_ptr = trainFunSetter(),     predict_ptr = predictFunSetter()))  # Get the transformed data: custom_cpp_factory$getData()  # Summarize factory: custom_cpp_factory$summarizeFactory() }"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPSpline.html","id":null,"dir":"Reference","previous_headings":"","what":"Base-learner factory to do non-parametric B or P-spline regression — BaselearnerPSpline","title":"Base-learner factory to do non-parametric B or P-spline regression — BaselearnerPSpline","text":"BaselearnerPSpline creates spline base-learner factory  object can registered within base-learner list used  training.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPSpline.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Base-learner factory to do non-parametric B or P-spline regression — BaselearnerPSpline","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPSpline.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Base-learner factory to do non-parametric B or P-spline regression — BaselearnerPSpline","text":"","code":"BaselearnerPSpline$new(data_source, list(degree, n_knots, penalty,   differences, df, n_bins, bin_method))"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPSpline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"arguments","title":"Base-learner factory to do non-parametric B or P-spline regression — BaselearnerPSpline","text":"data_source [data object] data object contains source data. degree [integer(1)] degree spline functions interpolate knots. n_knots [integer(1)] number inner knots. prevent weird behavior edges   inner knots expanded \\(\\mathrm{degree} - 1\\) additional knots. penalty [numeric(1)] positive numeric value specify penalty parameter. setting   penalty 0 ordinary b-splines used fitting. differences [integer(1)] number differences penalized. higher value leads   smoother curves. bin_root [integer(1)] set value greater zero, binning applied reduces number used   x values n^(1/bin_root) equidistant points. want use binning suggest   set bin_root = 2. bin_method [character(1)] Method used spacing knot points. Options linear (equally spaced grid)   quantile (knot points based quantiles).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPSpline.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Base-learner factory to do non-parametric B or P-spline regression — BaselearnerPSpline","text":"bin_root > 0 original feature discretized equidistant grid   \\([\\min(x),\\max(x)]\\) \\(\\sqrt{n}\\) points. fitting done   using weights per new data point.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPSpline.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Base-learner factory to do non-parametric B or P-spline regression — BaselearnerPSpline","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPSpline.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Base-learner factory to do non-parametric B or P-spline regression — BaselearnerPSpline","text":"getData() Get data matrix target data used   modeling. summarizeFactory() Summarize base-learner factory object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPSpline.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Base-learner factory to do non-parametric B or P-spline regression — BaselearnerPSpline","text":"","code":"# Sample data: data_mat = cbind(1:10) y = sin(1:10)  # Create new data object: data_source = InMemoryData$new(data_mat, \"my_data_name\")  # Create new linear base-learner: spline_factory = BaselearnerPSpline$new(data_source,   list(degree = 3, n_knots = 4, penalty = 2, differences = 2))  # Get the transformed data: spline_factory$getData() #>           [,1]       [,2]         [,3]       [,4]        [,5]        [,6] #> [1,] 0.1666667 0.01463192 0.0000000000 0.00000000 0.000000000 0.000000000 #> [2,] 0.6666667 0.44375857 0.1170553269 0.00617284 0.000000000 0.000000000 #> [3,] 0.1666667 0.51303155 0.6550068587 0.37037037 0.078417924 0.001828989 #> [4,] 0.0000000 0.02857796 0.2277091907 0.57407407 0.622770919 0.296982167 #> [5,] 0.0000000 0.00000000 0.0002286237 0.04938272 0.296982167 0.622770919 #> [6,] 0.0000000 0.00000000 0.0000000000 0.00000000 0.001828989 0.078417924 #> [7,] 0.0000000 0.00000000 0.0000000000 0.00000000 0.000000000 0.000000000 #> [8,] 0.0000000 0.00000000 0.0000000000 0.00000000 0.000000000 0.000000000 #>            [,7]         [,8]       [,9]     [,10] #> [1,] 0.00000000 0.0000000000 0.00000000 0.0000000 #> [2,] 0.00000000 0.0000000000 0.00000000 0.0000000 #> [3,] 0.00000000 0.0000000000 0.00000000 0.0000000 #> [4,] 0.04938272 0.0002286237 0.00000000 0.0000000 #> [5,] 0.57407407 0.2277091907 0.02857796 0.0000000 #> [6,] 0.37037037 0.6550068587 0.51303155 0.1666667 #> [7,] 0.00617284 0.1170553269 0.44375857 0.6666667 #> [8,] 0.00000000 0.0000000000 0.01463192 0.1666667  # Summarize factory: spline_factory$summarizeFactory() #> Spline factory of degree 3 #> \t- Name of the used data: my_data_name #> \t- Factory creates the following base-learner: spline_degree_3"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPolynomial.html","id":null,"dir":"Reference","previous_headings":"","what":"Base-learner factory for polynomial regression — BaselearnerPolynomial","title":"Base-learner factory for polynomial regression — BaselearnerPolynomial","text":"BaselearnerPolynomial creates polynomial base-learner factory  object can registered within base-learner list used  training.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPolynomial.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Base-learner factory for polynomial regression — BaselearnerPolynomial","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPolynomial.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Base-learner factory for polynomial regression — BaselearnerPolynomial","text":"","code":"BaselearnerPolynomial$new(data_source, list(degree, intercept)) BaselearnerPolynomial$new(data_source, blearner_type, list(degree, intercept))"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPolynomial.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base-learner factory for polynomial regression — BaselearnerPolynomial","text":"data_source [Data Object] Data object contains source data. degree [integer(1)] argument used transforming source data. element   taken power degree argument. intercept [logical(1)] Indicating whether intercept added . Default set TRUE.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPolynomial.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Base-learner factory for polynomial regression — BaselearnerPolynomial","text":"polynomial base-learner factory takes matrix user wants   pass number columns indicates much parameter estimated.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPolynomial.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Base-learner factory for polynomial regression — BaselearnerPolynomial","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPolynomial.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Base-learner factory for polynomial regression — BaselearnerPolynomial","text":"getData() Get data matrix target data used   modeling. summarizeFactory() Summarize base-learner factory object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPolynomial.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Base-learner factory for polynomial regression — BaselearnerPolynomial","text":"","code":"# Sample data: data_mat = cbind(1:10)  # Create new data object: data_source = InMemoryData$new(data_mat, \"my_data_name\")  # Create new linear base-learner factory: lin_factory = BaselearnerPolynomial$new(data_source,   list(degree = 2, intercept = FALSE)) lin_factory_int = BaselearnerPolynomial$new(data_source,   list(degree = 2, intercept = TRUE))  # Get the transformed data: lin_factory$getData() #>       [,1] [,2] #>  [1,]    1    1 #>  [2,]    2    4 #>  [3,]    3    9 #>  [4,]    4   16 #>  [5,]    5   25 #>  [6,]    6   36 #>  [7,]    7   49 #>  [8,]    8   64 #>  [9,]    9   81 #> [10,]   10  100 lin_factory_int$getData() #>       [,1] [,2] [,3] #>  [1,]    1    1    1 #>  [2,]    1    2    4 #>  [3,]    1    3    9 #>  [4,]    1    4   16 #>  [5,]    1    5   25 #>  [6,]    1    6   36 #>  [7,]    1    7   49 #>  [8,]    1    8   64 #>  [9,]    1    9   81 #> [10,]    1   10  100  # Summarize factory: lin_factory$summarizeFactory() #> Quadratic base-learner factory: #> \t- Name of the used data: my_data_name #> \t- Factory creates the following base-learner: polynomial_degree_2"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerTensor.html","id":null,"dir":"Reference","previous_headings":"","what":"Base-learner factory to make regression using tensor products — BaselearnerTensor","title":"Base-learner factory to make regression using tensor products — BaselearnerTensor","text":"BaselearnerTensor creates combined base-learner factory  object can registered within base-learner list used  training.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerTensor.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Base-learner factory to make regression using tensor products — BaselearnerTensor","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BlearnerFactoryList.html","id":null,"dir":"Reference","previous_headings":"","what":"Base-learner factory list to define the set of base-learners — BlearnerFactoryList","title":"Base-learner factory list to define the set of base-learners — BlearnerFactoryList","text":"BlearnerFactoryList creates object base-learner factories can registered. object can passed compboost set base-learner used optimizer get new best base-learner.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BlearnerFactoryList.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Base-learner factory list to define the set of base-learners — BlearnerFactoryList","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BlearnerFactoryList.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Base-learner factory list to define the set of base-learners — BlearnerFactoryList","text":"","code":"BlearnerFactoryList$new()"},{"path":"https://schalkdaniel.github.io/compboost/reference/BlearnerFactoryList.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Base-learner factory list to define the set of base-learners — BlearnerFactoryList","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BlearnerFactoryList.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Base-learner factory list to define the set of base-learners — BlearnerFactoryList","text":"registerFactory(BaselearnerFactory) Takes object   class BaseLearnerFactory adds factory set   base-learner. printRegisteredFactories() Get registered factories. clearRegisteredFactories() Remove registered factories.   Note factories deleted, just removed map. getModelFrame() Get target data matrix parsed one   big matrix. getNumberOfRegisteredFactories() Get number registered   factories.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BlearnerFactoryList.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Base-learner factory list to define the set of base-learners — BlearnerFactoryList","text":"","code":"# Sample data: data_mat = cbind(1:10)  # Create new data object: data_source = InMemoryData$new(data_mat, \"my_data_name\")  lin_factory = BaselearnerPolynomial$new(data_source,   list(degree = 1, intercept = TRUE)) poly_factory = BaselearnerPolynomial$new(data_source,   list(degree = 2, intercept = TRUE))  # Create new base-learner list: my_bl_list = BlearnerFactoryList$new()  # Register factories: my_bl_list$registerFactory(lin_factory) my_bl_list$registerFactory(poly_factory)  # Get registered factories: my_bl_list$printRegisteredFactories() #> Registered base-learner: #> \t- my_data_name_polynomial_degree_1 #> \t- my_data_name_polynomial_degree_2  # Get all target data matrices in one big matrix: my_bl_list$getModelFrame() #> $colnames #> [1] \"my_data_name_polynomial_degree_1x11\" \"my_data_name_polynomial_degree_1x12\" #> [3] \"my_data_name_polynomial_degree_2x11\" \"my_data_name_polynomial_degree_2x12\" #> [5] \"my_data_name_polynomial_degree_2x13\" #>  #> $model_frame #>       [,1] [,2] [,3] [,4] [,5] #>  [1,]    1    1    1    1    1 #>  [2,]    1    2    1    2    4 #>  [3,]    1    3    1    3    9 #>  [4,]    1    4    1    4   16 #>  [5,]    1    5    1    5   25 #>  [6,]    1    6    1    6   36 #>  [7,]    1    7    1    7   49 #>  [8,]    1    8    1    8   64 #>  [9,]    1    9    1    9   81 #> [10,]    1   10    1   10  100 #>   # Clear list: my_bl_list$clearRegisteredFactories()  # Get number of registered factories: my_bl_list$getNumberOfRegisteredFactories() #> [1] 0"},{"path":"https://schalkdaniel.github.io/compboost/reference/CategoricalDataRaw.html","id":null,"dir":"Reference","previous_headings":"","what":"Data class for character variables — CategoricalDataRaw","title":"Data class for character variables — CategoricalDataRaw","text":"CategoricalDataRaw creates data object can used source object instantiate categorical base learner. contrast CategoricalData data stored raw categorical vector.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/CategoricalDataRaw.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Data class for character variables — CategoricalDataRaw","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/CategoricalDataRaw.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data class for character variables — CategoricalDataRaw","text":"","code":"CategoricalDataRaw$new(x, data_identifier)"},{"path":"https://schalkdaniel.github.io/compboost/reference/CategoricalDataRaw.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Data class for character variables — CategoricalDataRaw","text":"x [character] Character vector containing classes. data_identifier [character(1)] name data specified data_mat. Note   important data names train evaluation data.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/CategoricalDataRaw.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Data class for character variables — CategoricalDataRaw","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/CategoricalDataRaw.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Data class for character variables — CategoricalDataRaw","text":"getData() Throws error representation calculated. getRawData() Get raw character data. getIdentifier() Get data identifier.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/CategoricalDataRaw.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Data class for character variables — CategoricalDataRaw","text":"","code":"# Sample data: x = sample(c(\"one\",\"two\", \"three\"), 20, TRUE)  # Create new data object: data_obj = CategoricalDataRaw$new(x, \"cat_raw\")  # Get data and identifier: data_obj$getRawData() #>  [1] \"one\"   \"three\" \"three\" \"three\" \"one\"   \"three\" \"three\" \"one\"   \"two\"   #> [10] \"three\" \"one\"   \"one\"   \"two\"   \"three\" \"two\"   \"three\" \"one\"   \"two\"   #> [19] \"one\"   \"three\" data_obj$getIdentifier() #> [1] \"cat_raw\""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":null,"dir":"Reference","previous_headings":"","what":"Component-wise boosting — Compboost","title":"Component-wise boosting — Compboost","text":"Component-wise boosting Component-wise boosting","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Component-wise boosting — Compboost","text":"class wraps `S4` class system exposed `Rcpp` fit component-wise boosting model. two convenient wrapper [boostLinear()] [boostSplines()] also creating objects class. Visualizing internals see [plotBaselearnerTraces()], [plotBaselearner()], [plotFeatureImportance()], [plotPEUni()], [plotTensor()], [plotRisk()]. Visualizing contribution one new observation see [plotIndividualContribution()].","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Component-wise boosting — Compboost","text":"data (`data.frame`) data used training model. Note: `oob_fraction` set, input data split `data` `data_oob`. Hence, `data` contains subset input data train model. data_oob (`data.frame`) --bag data set used risk logging early stopping. `data_oob` split input data (see `data` field). oob_fraction (`numeric(1)`) fraction `nrow(input data)` defining number observations `data_oob`. response ([ResponseRegr] | [ResponseBinaryClassif]) `S4` response object. See `?ResponseRegr` `?ResponseBinaryClassif` help. object holds current prediction, pseudo residuals functions transform scores. Note: response corresponds `data` field holds predictions `data.frame`. response_oob ([ResponseRegr] | [ResponseBinaryClassif]) `S4` response object. See `?ResponseRegr` `?ResponseBinaryClassif` help. `response` `data_oob`. target (`character(1)`) Name target variable `data`. id (`character(1)`) Name data object defined `$new(data, ...)`. optimizer ([OptimizerCoordinateDescent] | [OptimizerCoordinateDescentLineSearch] | [OptimizerAGBM] | [OptimizerCosineAnnealing]) initialized `S4` optimizer object (requires call `Optimizer*.new(..)`. See respective help page information. loss ([LossQuadratic] | [LossBinomial] | [LossHuber] | [LossAbsolute] | [LossQuantile]) initialized `S4` loss object (requires call `Loss*$new(...)`). See respective help page information. learning_rate (`numeric(1)`) learning rate model. Note: optimizer dynamically vary learning rate. model ([Compboost_internal]) internal Compboost object exported `Rcpp`. See `?Compboost_internal` details. bl_factory_list ([BlearnerFactoryList) container base learners. See `?BlearnerFactoryList` details. positive (`character(1)`) positive class case binary classification. stop_all (`logical(1)`) Indicator whether stopper must return `TRUE` early stop algorithm. Comparable `()` `stop_all = TRUE` `()` `stop_all = FALSE`. early_stop (`logical(1)`) Indicator whether early stopping used .","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"active-bindings","dir":"Reference","previous_headings":"","what":"Active bindings","title":"Component-wise boosting — Compboost","text":"baselearner_list (`list()`) Named `list` names `$getBaselearnerNames()`. elements contains * `\"feature\"` (`character(1)`): name feature `data`. * `\"factory\"` (`Baselearner*`): raw base learner  `factory`object. See `?Baselearner*` details. boost_intercept (`logical(1)`) Logical value indicating whether intercept base learner added `$addIntercept()` . logs (`data.frame`) Basic information risk, selected base learner etc. iteration. `oob_data` set, information validation/oob risk also logged. applies time logging etc. Note: Using field `logs` internally set updated call `$getLoggerData()`. Hence, cashes logged data set instead recalculating data set done `$getLoggerData()`. idx_oob (`integer()`) index vector used split `data` `data = data[idx_train, ]` `data_oob = data[idx_oob, ]`. Note: `oob_fraction` ignored argument set. idx_train (`integer()`) index vector used split `data` `data = data[idx_train, ]` `data_oob = data[idx_oob, ]`. Note: `oob_fraction` ignored argument set.","code":""},{"path":[]},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Component-wise boosting — Compboost","text":"Compboost$new() Compboost$addLogger() Compboost$getCurrentIteration() Compboost$addIntercept() Compboost$addBaselearner() Compboost$rmBaselearner() Compboost$addTensor() Compboost$addComponents() Compboost$train() Compboost$prepareData() Compboost$prepareResponse() Compboost$predict() Compboost$predictIndividual() Compboost$transformData() Compboost$getInbagRisk() Compboost$getSelectedBaselearner() Compboost$print() Compboost$getCoef() Compboost$getEstimatedCoef() Compboost$getBaselearnerNames() Compboost$getLoggerData() Compboost$calculateFeatureImportance() Compboost$saveToJson() Compboost$clone()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Component-wise boosting — Compboost","text":"Creates new instance [R6][R6::R6Class] class.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$new(   data = NULL,   target = NULL,   optimizer = NULL,   loss = NULL,   learning_rate = 0.05,   positive = NULL,   oob_fraction = NULL,   early_stop = FALSE,   idx_oob = NULL,   stop_args = list(eps_for_break = 0, patience = 10L),   file = NULL )"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"data (`data.frame`) data set build object. Note: data set completely used training `.null(idx_oob)`. Otherwise, data set split `data = data[idx_train, ]` `data_oob = data[idx_oob, ]`. target (`character(1)`) Character indicating name target variable. optimizer ([OptimizerCoordinateDescent] | [OptimizerCoordinateDescentLineSearch] | [OptimizerAGBM] | [OptimizerCosineAnnealing]) initialized `S4` optimizer object (requires call `Optimizer*.new(..)`. See respective help page information. loss ([LossQuadratic] | [LossBinomial] | [LossHuber] | [LossAbsolute] | [LossQuantile]) initialized `S4` loss object (requires call `Loss*$new(...)`). See respective help page information. learning_rate (`numeric(1)`) Learning rate model (default `0.05`). positive (`character(1)`) name positive class (case binary classification). oob_fraction (`numeric(1)`) fraction `nrow(input data)` defining number observations `data_oob`. argument ignored `idx_oob` set. early_stop (`logical(1)`) Indicator whether early stopping used . idx_oob (`integer()`) index vector used split `data` `data = data[idx_train, ]` `data_oob = data[idx_oob, ]`. Note: `oob_fraction` ignored argument set. stop_args (`list(integer(1), integer(1))`) `list` containing two elements `patience` `eps_for_break` used early stopping. file (`character(1`) File model loaded. `NULL`, `data` `target` must defined.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-addlogger-","dir":"Reference","previous_headings":"","what":"Method addLogger()","title":"Component-wise boosting — Compboost","text":"Add logger model.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$addLogger(logger, use_as_stopper = FALSE, logger_id, ...)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"logger ([LoggerIteration] | [LoggerTime] | [LoggerInbagRisk] | [LoggerOobRisk]) uninitialized logger. use_as_stopper (`logical(1)`) Indicator defining logger stopper considering early stopping. logger_id (`character(1)`) id logger. allows define two logger type (`e.g. risk logging`) different arguments. ... Additional arguments passed `loger$new(logger_id, use_as_stopper, ...)`.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-getcurrentiteration-","dir":"Reference","previous_headings":"","what":"Method getCurrentIteration()","title":"Component-wise boosting — Compboost","text":"Get number current iteration.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$getCurrentIteration()"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"`integer(1)` value.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-addintercept-","dir":"Reference","previous_headings":"","what":"Method addIntercept()","title":"Component-wise boosting — Compboost","text":"functions adds base learner adjusts intercept (selected). Adding intercept base learner may necessary, e.g., adding linear effects without intercept.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$addIntercept(id = \"intercept\", data_source = InMemoryData)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"id (`character(1)`) id base learner (default `\"intercept\"`). data_source ([InMemoryData]) Uninitialized data object used store meta data. Note: moment, just memory storing supported, see `?InMemorydata` details. data_source ([InMemoryData]) Uninitialized data object used store meta data. Note: moment, just memory storing supported, see `?InMemorydata` details.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-addbaselearner-","dir":"Reference","previous_headings":"","what":"Method addBaselearner()","title":"Component-wise boosting — Compboost","text":"Add base learner one feature model considered iteration. Using `$addBaselearner()` just allows including univariate features. See `$addTensor()` bivariate effect modelling `$addComponents()` effect decomposition.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$addBaselearner(   feature,   id,   bl_factory,   data_source = InMemoryData,   ... )"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"feature (`character(1)`) Name feature, must column `data`. feature (`character(1)`) Name feature, must column `data`. id (`character(1)`) name base learner. bl_factory ([BaselearnerPolynomial] | [BaselearnerPSpline] | [BaselearnerCategoricalBinary] | [BaselearnerCategoricalRidge]) Uninitialized base learner class. See respective help page details. data_source ([InMemoryData]) Uninitialized data object used store meta data. Note: moment, just memory storing supported, see `?InMemorydata` details. data_source ([InMemoryData]) Uninitialized data object used store meta data. Note: moment, just memory storing supported, see `?InMemorydata` details. ... argument spassed `$new(...)` constructor `bl_factory`.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-rmbaselearner-","dir":"Reference","previous_headings":"","what":"Method rmBaselearner()","title":"Component-wise boosting — Compboost","text":"Remove base learner model.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$rmBaselearner(blname)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"blname (`character(1)`) Name base learner removed. Must element `$getBaselearnerNames()`.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-addtensor-","dir":"Reference","previous_headings":"","what":"Method addTensor()","title":"Component-wise boosting — Compboost","text":"Add row-wise tensor product features. Note: base learner pre-defined type feature. Numerical features uses `BaselearnerPSpline` categorical features included using `BaselearnerCategoricalRidge` base learner. include arbitrary tensor product requires use `S4` API using `BaselearnerTensor` two base learners type.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$addTensor(   feature1,   feature2,   df = NULL,   df1 = NULL,   df2 = NULL,   isotrop = FALSE,   ... )"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"feature1 (`character(1)`) Name first feature. Must element `names(data)`. feature2 (`character(1)`) Name second feature. Must element `names(data)`. df1 (`numeric(1)`) degrees freedom used first base learner. df2 (`numeric(1)`) degrees freedom used first base learner. isotrop (`logical(1)`) Indicator two penalties combined, `isotrop == TRUE`, total degrees freedom uniformly distributed dimensions `isotrop == FALSE` allows define strong two dimensions penalized. ... Additional arguments passed `$new()` constructor [BaselearnerPSpline] class.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-addcomponents-","dir":"Reference","previous_headings":"","what":"Method addComponents()","title":"Component-wise boosting — Compboost","text":"Add effect individual components. linear term added well non-linear term without linear effect. ensures linear component selected prior non-linear effect. non-linear effect included deviation linear effect required. Note: Internally, [BaselearnerPolynomial] degree one [BaselearnerCentered] added. Centering base learner makes design matrix dense hence memory filled fast. Considering binning may option reduce memory consumption.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$addComponents(feature, ...)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"feature (`character(1)`) Name feature, must column `data`. feature (`character(1)`) Name feature, must column `data`. ... Additional arguments passed `$new()` constructor [BaselearnerPSpline] class.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-train-","dir":"Reference","previous_headings":"","what":"Method train()","title":"Component-wise boosting — Compboost","text":"Start fitting model.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$train(iteration = 100, trace = -1)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"iteration (`integer(1)`) maximal number iteration. algorithm can stopped earlier early stopping active. trace (`integer(1)`) number integers status fitting printed screen. default `trace = -1` internally uses `trace = round(iteration / 40)`. silently fit model use `trace = 0`.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-preparedata-","dir":"Reference","previous_headings":"","what":"Method prepareData()","title":"Component-wise boosting — Compboost","text":"Internally, base learner build [InMemoryData] object. methods (e.g. adding [LoggerOobRisk]) requires pass data `list(InMemoryData | CategoricalDataRaw)` data objects elements. function converts given `data.frame` format.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$prepareData(newdata)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"newdata (`data.frame`) New data set structure `data`. newdata (`data.frame`) New data set structure `data`. newdata (`data.frame`) New data set structure `data`. newdata (`data.frame`) New data set structure `data`.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"`list(InMemoryData | CategoricalDataRaw)` data container elements. Numeric features wrapped [InMemoryData] categorical features included [CategoricalDataRaw].","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-prepareresponse-","dir":"Reference","previous_headings":"","what":"Method prepareResponse()","title":"Component-wise boosting — Compboost","text":"`$prepareData()` response. Internally, `vectorToResponse()` used generate [ResponseRegr] [ResponseBinaryClassif] object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$prepareResponse(response)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"response (`vector()`) vector type `numberic` `categorical` transformed response object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"[ResponseRegr] | [ResponseBinaryClassif] object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-predict-","dir":"Reference","previous_headings":"","what":"Method predict()","title":"Component-wise boosting — Compboost","text":"Calculate predictions.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$predict(newdata = NULL, as_response = FALSE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"newdata (`data.frame`) New data set structure `data`. newdata (`data.frame`) New data set structure `data`. newdata (`data.frame`) New data set structure `data`. newdata (`data.frame`) New data set structure `data`. as_response (`logical(1)`) case binary classification, `as_response = TRUE` returns predictions response, .e. classes.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"Vector predictions.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-predictindividual-","dir":"Reference","previous_headings":"","what":"Method predictIndividual()","title":"Component-wise boosting — Compboost","text":"`$predict()` returns sum base learner predictions, function returns `list` predictions base learner.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$predictIndividual(newdata)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-11","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"newdata (`data.frame`) New data set structure `data`. newdata (`data.frame`) New data set structure `data`. newdata (`data.frame`) New data set structure `data`. newdata (`data.frame`) New data set structure `data`.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"Named `list()` included base learner names names base learner predictions elements.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-transformdata-","dir":"Reference","previous_headings":"","what":"Method transformData()","title":"Component-wise boosting — Compboost","text":"Get design matrices (subset) base learners new `data.frame`.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$transformData(newdata, blnames = NULL)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-12","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"newdata (`data.frame`) New data set structure `data`. newdata (`data.frame`) New data set structure `data`. newdata (`data.frame`) New data set structure `data`. newdata (`data.frame`) New data set structure `data`. blnames (`character()`) Names base learners design matrices returned. `.null(blnames)`, compboost tries guess base learners constructed based feature names `newdata`.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"`list(matrix | Matrix::Matrix)` matrices elements.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-getinbagrisk-","dir":"Reference","previous_headings":"","what":"Method getInbagRisk()","title":"Component-wise boosting — Compboost","text":"Return training risk iteration.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$getInbagRisk()"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"`numeric()` vector risk values `NULL` `$train()` called previously.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-getselectedbaselearner-","dir":"Reference","previous_headings":"","what":"Method getSelectedBaselearner()","title":"Component-wise boosting — Compboost","text":"Get vector name selected base learner iteration.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$getSelectedBaselearner()"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"`character()` vector base learner names.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-print-","dir":"Reference","previous_headings":"","what":"Method print()","title":"Component-wise boosting — Compboost","text":"Printer object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$print()"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"Invisibly returns object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-getcoef-","dir":"Reference","previous_headings":"","what":"Method getCoef()","title":"Component-wise boosting — Compboost","text":"Get estimated coefficients.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-17","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$getCoef()"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"`list(pars, offset)` estimated coefficients/parameters intercept/offset.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-getestimatedcoef-","dir":"Reference","previous_headings":"","what":"Method getEstimatedCoef()","title":"Component-wise boosting — Compboost","text":"DEPRICATED use `$getCoef()` instead. Get estimated coefficients.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-18","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$getEstimatedCoef()"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-10","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"`list(pars, offset)` estimated coefficients/parameters intercept/offset.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-getbaselearnernames-","dir":"Reference","previous_headings":"","what":"Method getBaselearnerNames()","title":"Component-wise boosting — Compboost","text":"Get names registered base learners.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-19","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$getBaselearnerNames()"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-11","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"`charcter()` base learner names.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-getloggerdata-","dir":"Reference","previous_headings":"","what":"Method getLoggerData()","title":"Component-wise boosting — Compboost","text":"Get logged information.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-20","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$getLoggerData()"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-12","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"`data.frame` logging information.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-calculatefeatureimportance-","dir":"Reference","previous_headings":"","what":"Method calculateFeatureImportance()","title":"Component-wise boosting — Compboost","text":"Calculate feature important based training risk. Note early stopping used get adequate importance measures.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-21","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$calculateFeatureImportance(   num_feats = NULL,   aggregate_bl_by_feat = FALSE )"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-13","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"num_feats (`integer(1)`) number considered features, `num_feats` important feature names respective value returned. aggregate_bl_by_feat (`logical(1)`) Indicator whether importance aggregated based feature level. example, adding components included two different base learners feature. `aggregate_bl_by_feat == TRUE`, importance two base learners aggregated instead considering individually.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-13","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"Named `numeric()` vector length `num_feats` (least `num_feats` selected) importance values elements.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-savetojson-","dir":"Reference","previous_headings":"","what":"Method saveToJson()","title":"Component-wise boosting — Compboost","text":"Save [Compboost] object JSON file. underlying C++ objects, possible use R's native load save methods.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-22","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$saveToJson(file)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-14","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"file (`character(1)`) Name/path file.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Component-wise boosting — Compboost","text":"objects class cloneable method.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-23","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$clone(deep = FALSE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-15","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"deep Whether make deep clone.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Component-wise boosting — Compboost","text":"","code":"cboost = Compboost$new(mtcars, \"mpg\", loss = LossQuadratic$new(), oob_fraction = 0.3) cboost$addBaselearner(\"hp\", \"spline\", BaselearnerPSpline, degree = 3,   n_knots = 10, df = 3, differences = 2) cboost$addBaselearner(\"wt\", \"spline\", BaselearnerPSpline) cboost$train(1000) #>    1/1000   risk = 14  oob_risk = 20    #>   25/1000   risk = 3.1  oob_risk = 6.6    #>   50/1000   risk = 1.7  oob_risk = 4.5    #>   75/1000   risk = 1.5  oob_risk = 4.1    #>  100/1000   risk = 1.5  oob_risk = 4    #>  125/1000   risk = 1.5  oob_risk = 3.9    #>  150/1000   risk = 1.4  oob_risk = 3.8    #>  175/1000   risk = 1.4  oob_risk = 3.8    #>  200/1000   risk = 1.4  oob_risk = 3.7    #>  225/1000   risk = 1.4  oob_risk = 3.7    #>  250/1000   risk = 1.3  oob_risk = 3.6    #>  275/1000   risk = 1.3  oob_risk = 3.6    #>  300/1000   risk = 1.3  oob_risk = 3.6    #>  325/1000   risk = 1.3  oob_risk = 3.6    #>  350/1000   risk = 1.3  oob_risk = 3.6    #>  375/1000   risk = 1.3  oob_risk = 3.6    #>  400/1000   risk = 1.3  oob_risk = 3.5    #>  425/1000   risk = 1.2  oob_risk = 3.5    #>  450/1000   risk = 1.2  oob_risk = 3.5    #>  475/1000   risk = 1.2  oob_risk = 3.5    #>  500/1000   risk = 1.2  oob_risk = 3.6    #>  525/1000   risk = 1.2  oob_risk = 3.5    #>  550/1000   risk = 1.2  oob_risk = 3.6    #>  575/1000   risk = 1.2  oob_risk = 3.6    #>  600/1000   risk = 1.2  oob_risk = 3.6    #>  625/1000   risk = 1.2  oob_risk = 3.6    #>  650/1000   risk = 1.1  oob_risk = 3.6    #>  675/1000   risk = 1.1  oob_risk = 3.6    #>  700/1000   risk = 1.1  oob_risk = 3.6    #>  725/1000   risk = 1.1  oob_risk = 3.6    #>  750/1000   risk = 1.1  oob_risk = 3.6    #>  775/1000   risk = 1.1  oob_risk = 3.6    #>  800/1000   risk = 1.1  oob_risk = 3.6    #>  825/1000   risk = 1.1  oob_risk = 3.6    #>  850/1000   risk = 1.1  oob_risk = 3.6    #>  875/1000   risk = 1.1  oob_risk = 3.6    #>  900/1000   risk = 1.1  oob_risk = 3.6    #>  925/1000   risk = 1  oob_risk = 3.7    #>  950/1000   risk = 1  oob_risk = 3.7    #>  975/1000   risk = 1  oob_risk = 3.7    #> 1000/1000   risk = 1  oob_risk = 3.7    #>  #>  #> Train 1000 iterations in 0 Seconds. #> Final risk based on the train set: 1 #>   table(cboost$getSelectedBaselearner()) #>  #> hp_spline wt_spline  #>       215       785  head(cboost$logs) #>   _iterations oob_risk baselearner train_risk #> 1           0       NA   intercept   15.78227 #> 2           1 19.74600   wt_spline   14.47272 #> 3           2 18.46421   wt_spline   13.28993 #> 4           3 17.30581   wt_spline   12.22157 #> 5           4 16.25873   wt_spline   11.25655 #> 6           5 15.31209   wt_spline   10.38480 names(cboost$baselearner_list) #> [1] \"hp_spline\" \"wt_spline\"  # Access information about the a base learner in the list: cboost$baselearner_list$hp_spline$factory$getDF() #>      [,1] #> [1,]    3 cboost$baselearner_list$hp_spline$factory$getPenalty() #>          [,1] #> [1,] 66.09728"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost_internal.html","id":null,"dir":"Reference","previous_headings":"","what":"Main Compboost Class — Compboost_internal","title":"Main Compboost Class — Compboost_internal","text":"class collects parts factory list used logger passes C++. C++ side main algorithm.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost_internal.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Main Compboost Class — Compboost_internal","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost_internal.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Main Compboost Class — Compboost_internal","text":"","code":"Compboost$new(response, learning_rate, stop_if_all_stopper_fulfilled,   factory_list, loss, logger_list, optimizer)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost_internal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Main Compboost Class — Compboost_internal","text":"response [numeric] Vector true values modeled. learning_rate [numeric(1)] learning rate used shrink parameter iteration. stop_if_all_stopper_fulfilled [logical(1)] Boolean indicate stopping strategy used. TRUE   algorithm stops registered logger stopper fulfilled. factory_list [BlearnerFactoryList object] List base-learner factories one base-learner selected   iteration using loss [Loss object] loss used calculate pseudo residuals   iteration. logger_list [LoggerList object] list registered logger used track algorithm. optimizer [Optimizer object] optimizer used select iteration one good   base-learner.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost_internal.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Main Compboost Class — Compboost_internal","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost_internal.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Main Compboost Class — Compboost_internal","text":"train(trace) Initial training model. integer   argument trace indicates logger progress printed   trace indicates iterations printed. continueTraining(trace, logger_list) Continue training   using additional logger_list. retraining stopped   first logger says algorithm stopped. getPrediction() Get inbag prediction done   fitting process. getSelectedBaselearner() Returns character vector   base-learner selected. getLoggerData() Returns list logged data.   algorithm retrained, list contains training one   element. getEstimatedParameter() Returns list estimated   parameter base-learner selected least . getParameterAtIteration(k) Calculates prediction   iteration k. getParameterMatrix() Calculates matrix row   includes parameter iteration . many rows   done iterations. isTrained() function returns just boolean value   indicates initial training already done. predict(newdata) Prediction new data organized within   list source data objects. important names source   data objects matches one used define factories. predictAtIteration(newdata, k) Prediction new data using   another iteration k. setToIteration(k) Set whole model another iteration   k. calling function elements   parameters prediction calculated corresponding k. summarizeCompboost() Summarize Compboost object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost_internal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Main Compboost Class — Compboost_internal","text":"","code":"# Some data: df = mtcars df$mpg_cat = ifelse(df$mpg > 20, \"high\", \"low\")  # # Create new variable to check the polynomial base-learner with degree 2: # df$hp2 = df[[\"hp\"]]^2  # Data for the baselearner are matrices: X_hp = as.matrix(df[[\"hp\"]]) X_wt = as.matrix(df[[\"wt\"]])  # Target variable: response = ResponseBinaryClassif$new(\"mpg_cat\", \"high\", df[[\"mpg_cat\"]])  data_source_hp = InMemoryData$new(X_hp, \"hp\") data_source_wt = InMemoryData$new(X_wt, \"wt\")  # List for oob logging: oob_data = list(data_source_hp, data_source_wt)  # List to test prediction on newdata: test_data = oob_data  # Factories: linear_factory_hp = BaselearnerPolynomial$new(data_source_hp,   list(degree = 1, intercept = TRUE)) linear_factory_wt = BaselearnerPolynomial$new(data_source_wt,   list(degree = 1, intercept = TRUE)) quadratic_factory_hp = BaselearnerPolynomial$new(data_source_hp,   list(degree = 2, intercept = TRUE)) spline_factory_wt = BaselearnerPSpline$new(data_source_wt,   list(degree = 3, n_knots = 10, penalty = 2, differences = 2))  # Create new factory list: factory_list = BlearnerFactoryList$new()  # Register factories: factory_list$registerFactory(linear_factory_hp) factory_list$registerFactory(linear_factory_wt) factory_list$registerFactory(quadratic_factory_hp) factory_list$registerFactory(spline_factory_wt)  # Define loss: loss_bin = LossBinomial$new()  # Define optimizer: optimizer = OptimizerCoordinateDescent$new()  ## Logger  # Define logger. We want just the iterations as stopper but also track the # time, inbag risk and oob risk: log_iterations  = LoggerIteration$new(\" iteration_logger\", TRUE, 500) log_time        = LoggerTime$new(\"time_logger\", FALSE, 500, \"microseconds\")  # Define new logger list: logger_list = LoggerList$new()  # Register the logger: logger_list$registerLogger(log_iterations) logger_list$registerLogger(log_time)  # Run compboost: # --------------  # Initialize object: cboost = Compboost_internal$new(   response      = response,   learning_rate = 0.05,   stop_if_all_stopper_fulfilled = FALSE,   factory_list = factory_list,   loss         = loss_bin,   logger_list  = logger_list,   optimizer    = optimizer )  # Train the model (we want to print the trace): cboost$train(trace = 50) #>   1/500   risk = 0.68  time_logger = 0    #>  50/500   risk = 0.42  time_logger = 535    #> 100/500   risk = 0.31  time_logger = 1129    #> 150/500   risk = 0.25  time_logger = 1804    #> 200/500   risk = 0.21  time_logger = 2537    #> 250/500   risk = 0.19  time_logger = 3347    #> 300/500   risk = 0.17  time_logger = 4246    #> 350/500   risk = 0.16  time_logger = 5214    #> 400/500   risk = 0.15  time_logger = 6274    #> 450/500   risk = 0.14  time_logger = 7404    #> 500/500   risk = 0.13  time_logger = 8616    #>  #>  #> Train 500 iterations in 0 Seconds. #> Final risk based on the train set: 0.13 #>  cboost #>  #> Compboost object with: #> \t- Learning Rate: 0.05 #> \t- Are all logger used as stopper: 0 #> \t- Model is already trained with 500 iterations/fitted baselearner #> \t- Actual state is at iteration 500 #>  #>  #>   # Get estimated parameter: cboost$getEstimatedParameter() #> $hp_polynomial_degree_2 #>               [,1] #> [1,]  6.0660349854 #> [2,] -0.0645193461 #> [3,]  0.0001267875 #>  #> $wt_spline_degree_3 #>             [,1] #>  [1,]  1.9370515 #>  [2,]  1.8485620 #>  [3,]  1.8081122 #>  [4,]  1.8647942 #>  [5,]  1.7149991 #>  [6,]  1.0827258 #>  [7,] -0.4203328 #>  [8,] -1.8130157 #>  [9,] -2.0641036 #> [10,] -2.0050272 #> [11,] -1.8182042 #> [12,] -1.5569128 #> [13,] -1.2730290 #> [14,] -0.9904981 #>   # Get trace of selected base-learner: cboost$getSelectedBaselearner() #>   [1] \"wt_spline_degree_3\"     \"wt_spline_degree_3\"     \"wt_spline_degree_3\"     #>   [4] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #>   [7] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"wt_spline_degree_3\"     #>  [10] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #>  [13] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #>  [16] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #>  [19] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #>  [22] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #>  [25] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #>  [28] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #>  [31] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #>  [34] \"wt_spline_degree_3\"     \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #>  [37] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #>  [40] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #>  [43] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #>  [46] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #>  [49] \"wt_spline_degree_3\"     \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #>  [52] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #>  [55] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #>  [58] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #>  [61] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #>  [64] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #>  [67] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #>  [70] \"wt_spline_degree_3\"     \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #>  [73] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #>  [76] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #>  [79] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #>  [82] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #>  [85] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #>  [88] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #>  [91] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #>  [94] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #>  [97] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [100] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [103] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [106] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [109] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [112] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [115] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [118] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [121] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [124] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [127] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [130] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [133] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [136] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [139] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [142] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [145] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [148] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [151] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" #> [154] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [157] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [160] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [163] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [166] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [169] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [172] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [175] \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [178] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [181] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [184] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [187] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [190] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [193] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [196] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [199] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [202] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [205] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [208] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [211] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [214] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [217] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [220] \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [223] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [226] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [229] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [232] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" #> [235] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [238] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [241] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [244] \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [247] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [250] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [253] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [256] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [259] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [262] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [265] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" #> [268] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [271] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [274] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" #> [277] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [280] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [283] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [286] \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [289] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [292] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [295] \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [298] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [301] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [304] \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [307] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [310] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [313] \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [316] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [319] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [322] \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [325] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [328] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [331] \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [334] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [337] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" #> [340] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [343] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [346] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" #> [349] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [352] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [355] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" #> [358] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [361] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [364] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [367] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [370] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [373] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [376] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [379] \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [382] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [385] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" #> [388] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [391] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [394] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" #> [397] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [400] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [403] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [406] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [409] \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [412] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [415] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [418] \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [421] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [424] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" #> [427] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [430] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [433] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [436] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [439] \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [442] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [445] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" #> [448] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [451] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [454] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [457] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [460] \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [463] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [466] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" #> [469] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [472] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [475] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [478] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [481] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [484] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [487] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [490] \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     #> [493] \"hp_polynomial_degree_2\" \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" #> [496] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\" \"hp_polynomial_degree_2\" #> [499] \"wt_spline_degree_3\"     \"hp_polynomial_degree_2\"  # Set to iteration 200: cboost$setToIteration(200, 30)  # Get new parameter values: cboost$getEstimatedParameter() #> $hp_polynomial_degree_2 #>               [,1] #> [1,]  3.649127e+00 #> [2,] -3.820112e-02 #> [3,]  7.376905e-05 #>  #> $wt_spline_degree_3 #>             [,1] #>  [1,]  1.3479210 #>  [2,]  1.3418629 #>  [3,]  1.3528398 #>  [4,]  1.3714923 #>  [5,]  1.2096167 #>  [6,]  0.7153452 #>  [7,] -0.2712226 #>  [8,] -1.1840205 #>  [9,] -1.4335470 #> [10,] -1.4475909 #> [11,] -1.3450002 #> [12,] -1.1727850 #> [13,] -0.9769323 #> [14,] -0.7809478 #>"},{"path":"https://schalkdaniel.github.io/compboost/reference/InMemoryData.html","id":null,"dir":"Reference","previous_headings":"","what":"In memory data class to store data in RAM — InMemoryData","title":"In memory data class to store data in RAM — InMemoryData","text":"InMemoryData creates data object can used source target object within base-learner factories compboost. convention initialize target data call constructor without arguments.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/InMemoryData.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"In memory data class to store data in RAM — InMemoryData","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/InMemoryData.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"In memory data class to store data in RAM — InMemoryData","text":"","code":"InMemoryData$new() InMemoryData$new(data_mat, data_identifier)"},{"path":"https://schalkdaniel.github.io/compboost/reference/InMemoryData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"In memory data class to store data in RAM — InMemoryData","text":"data_mat [matrix] Matrix containing source data. source data later transformed   obtain design matrix base-learner uses training. data_identifier [character(1)] name data specified data_mat. Note   important data names train evaluation data.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/InMemoryData.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"In memory data class to store data in RAM — InMemoryData","text":"data_mat needs suits base-learner. instance,   spline base-learner just take one column matrix since   just one dimensional splines moment. data_mat data_identifier target data object   set automatically passing source target object   factory. getData() can used access   transformed data target object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/InMemoryData.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"In memory data class to store data in RAM — InMemoryData","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/InMemoryData.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"In memory data class to store data in RAM — InMemoryData","text":"getData()  getIdentifier()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/InMemoryData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"In memory data class to store data in RAM — InMemoryData","text":"","code":"# Sample data: data_mat = cbind(1:10)  # Create new data object: data_obj = InMemoryData$new(data_mat, \"my_data_name\")  # Get data and identifier: data_obj$getData() #>       [,1] #>  [1,]    1 #>  [2,]    2 #>  [3,]    3 #>  [4,]    4 #>  [5,]    5 #>  [6,]    6 #>  [7,]    7 #>  [8,]    8 #>  [9,]    9 #> [10,]   10 data_obj$getIdentifier() #> [1] \"my_data_name\""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerInbagRisk.html","id":null,"dir":"Reference","previous_headings":"","what":"Logger class to log the inbag risk — LoggerInbagRisk","title":"Logger class to log the inbag risk — LoggerInbagRisk","text":"class logs inbag risk specific loss function. also possible use custom losses log performance measures. details see use case extending compboost vignette.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerInbagRisk.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Logger class to log the inbag risk — LoggerInbagRisk","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerInbagRisk.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Logger class to log the inbag risk — LoggerInbagRisk","text":"","code":"LoggerInbagRisk$new(logger_id, use_as_stopper, used_loss, eps_for_break, patience)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerInbagRisk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logger class to log the inbag risk — LoggerInbagRisk","text":"logger_id [character(1)] Unique identifier logger. use_as_stopper [logical(1)] Boolean indicate logger also used stopper. used_loss [Loss object] loss used calculate empirical risk taking mean   returned defined loss within loss object. eps_for_break [numeric(1)] argument used loss also used stopper. relative   improvement logged inbag risk falls boundary stopper   returns TRUE.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerInbagRisk.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Logger class to log the inbag risk — LoggerInbagRisk","text":"logger computes risk given training data \\(\\mathcal{D} = \\{(x^{()},\\ y^{()})\\ |\\ \\\\{1, \\dots, n\\}\\}\\) stores vector. empirical risk \\(\\mathcal{R}\\) iteration \\(m\\) calculated : $$   \\mathcal{R}_\\mathrm{emp}^{[m]} = \\frac{1}{n}\\sum\\limits_{= 1}^n L(y^{()}, \\hat{f}^{[m]}(x^{()})) $$ Note: \\(m=0\\) \\(\\hat{f}\\) just offset. implementation calculate \\(\\mathcal{R}_\\mathrm{emp}^{[m]}\\)     done two steps: Calculate vector risk_temp losses every observation          given response \\(y^{()}\\) prediction \\(\\hat{f}^{[m]}(x^{()})\\). Average risk_temp. procedure ensures, possible e.g. use AUC    arbitrary performance measure risk logging. gives just one    value risk_temp therefore average equals loss    function. just value (like AUC) value    returned.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerInbagRisk.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Logger class to log the inbag risk — LoggerInbagRisk","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerInbagRisk.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Logger class to log the inbag risk — LoggerInbagRisk","text":"summarizeLogger() Summarize logger object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerInbagRisk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Logger class to log the inbag risk — LoggerInbagRisk","text":"","code":"# Used loss: log_bin = LossBinomial$new()  # Define logger: log_inbag_risk = LoggerInbagRisk$new(\"inbag\", FALSE, log_bin, 0.05, 5)  # Summarize logger: log_inbag_risk$summarizeLogger() #> Inbag risk logger: #> \t- Use logger as stopper: 0"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerIteration.html","id":null,"dir":"Reference","previous_headings":"","what":"Logger class to log the current iteration — LoggerIteration","title":"Logger class to log the current iteration — LoggerIteration","text":"Logger class log current iteration","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerIteration.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Logger class to log the current iteration — LoggerIteration","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerIteration.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Logger class to log the current iteration — LoggerIteration","text":"","code":"LoggerIterationWrapper$new(logger_id, use_as_stopper, max_iterations)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerIteration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logger class to log the current iteration — LoggerIteration","text":"logger_id [character(1)] Unique identifier logger. use_as_stopper [logical(1)] Boolean indicate logger also used stopper. max_iterations [integer(1)] logger used stopper argument defines maximal   iterations.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerIteration.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Logger class to log the current iteration — LoggerIteration","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerIteration.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Logger class to log the current iteration — LoggerIteration","text":"summarizeLogger() Summarize logger object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerIteration.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Logger class to log the current iteration — LoggerIteration","text":"","code":"# Define logger: log_iters = LoggerIteration$new(\"iterations\", FALSE, 100)  # Summarize logger: log_iters$summarizeLogger() #> Iteration logger: #> \t- Maximal iterations: 100 #> \t- Use logger as stopper: 0"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerList.html","id":null,"dir":"Reference","previous_headings":"","what":"Logger list class to collect all loggers — LoggerList","title":"Logger list class to collect all loggers — LoggerList","text":"class meant define logger used track progress algorithm.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerList.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Logger list class to collect all loggers — LoggerList","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerList.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Logger list class to collect all loggers — LoggerList","text":"","code":"LoggerList$new()"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerList.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Logger list class to collect all loggers — LoggerList","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerList.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Logger list class to collect all loggers — LoggerList","text":"clearRegisteredLogger() Removes registered logger   list. used logger deleted, just removed   map. getNamesOfRegisteredLogger() Returns registered logger   names character vector. getNumberOfRegisteredLogger() Returns number registered   logger integer. printRegisteredLogger() Prints registered logger. registerLogger(logger) Includes new logger   logger list logger_id key.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerList.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Logger list class to collect all loggers — LoggerList","text":"","code":"# Define logger: log_iters = LoggerIteration$new(\"iteration\", TRUE, 100) log_time = LoggerTime$new(\"time\", FALSE, 20, \"minutes\")  # Create logger list: logger_list = LoggerList$new()  # Register new loggeR: logger_list$registerLogger(log_iters) logger_list$registerLogger(log_time)  # Print registered logger: logger_list$printRegisteredLogger() #> Registered Logger: #> \t>>iteration<< Logger #> \t>>time<< Logger  # Remove all logger: logger_list$clearRegisteredLogger()  # Get number of registered logger: logger_list$getNumberOfRegisteredLogger() #> [1] 0"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerOobRisk.html","id":null,"dir":"Reference","previous_headings":"","what":"Logger class to log the out of bag risk — LoggerOobRisk","title":"Logger class to log the out of bag risk — LoggerOobRisk","text":"class logs bag risk specific loss function. also possible use custom losses log performance measures. details see use case extending compboost vignette.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerOobRisk.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Logger class to log the out of bag risk — LoggerOobRisk","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerOobRisk.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Logger class to log the out of bag risk — LoggerOobRisk","text":"","code":"LoggerOobRisk$new(logger_id, use_as_stopper, used_loss, eps_for_break,   patience, oob_data, oob_response)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerOobRisk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logger class to log the out of bag risk — LoggerOobRisk","text":"logger_id [character(1)] Unique identifier logger. use_as_stopper [logical(1)] Boolean indicate logger also used stopper. used_loss [Loss object] loss used calculate empirical risk taking mean   returned defined loss within loss object. eps_for_break [numeric(1)] argument used loss also used stopper. relative   improvement logged inbag risk falls boundary stopper   returns TRUE. oob_data [list] list contains data source objects corresponds   source data registered factory. source data objects   contain bag data. data used calculate   prediction step. oob_response [numeric] Vector contains response bag data given within   list.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerOobRisk.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Logger class to log the out of bag risk — LoggerOobRisk","text":"logger computes risk given new dataset \\(\\mathcal{D}_\\mathrm{oob} = \\{(x^{()},\\ y^{()})\\ |\\ \\I_\\mathrm{oob}\\}\\) stores vector. OOB risk \\(\\mathcal{R}_\\mathrm{oob}\\) iteration \\(m\\) calculated : $$   \\mathcal{R}_\\mathrm{oob}^{[m]} = \\frac{1}{|\\mathcal{D}_\\mathrm{oob}|}\\sum\\limits_{(x,y) \\\\mathcal{D}_\\mathrm{oob}}   L(y, \\hat{f}^{[m]}(x)) $$ Note: \\(m=0\\) \\(\\hat{f}\\) just offset. implementation calculate \\(\\mathcal{R}_\\mathrm{emp}^{[m]}\\)     done two steps: Calculate vector risk_temp losses every observation         given response \\(y^{()}\\) prediction \\(\\hat{f}^{[m]}(x^{()})\\). Average risk_temp. procedure ensures, possible e.g. use AUC    arbitrary performance measure risk logging. gives just one    value \\(risk_temp\\) therefore average equals loss    function. just value (like AUC) value    returned.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerOobRisk.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Logger class to log the out of bag risk — LoggerOobRisk","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerOobRisk.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Logger class to log the out of bag risk — LoggerOobRisk","text":"summarizeLogger() Summarize logger object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerOobRisk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Logger class to log the out of bag risk — LoggerOobRisk","text":"","code":"# Define data: X1 = cbind(1:10) X2 = cbind(10:1) data_source1 = InMemoryData$new(X1, \"x1\") data_source2 = InMemoryData$new(X2, \"x2\")  oob_list = list(data_source1, data_source2)  set.seed(123) y_oob = rnorm(10)  # Used loss: log_bin = LossBinomial$new()  # Define response object of oob data: oob_response = ResponseRegr$new(\"oob_response\", as.matrix(y_oob))  # Define logger: log_oob_risk = LoggerOobRisk$new(\"oob\", FALSE, log_bin, 0.05, 5, oob_list, oob_response)  # Summarize logger: log_oob_risk$summarizeLogger() #> Out of bag risk logger: #> \t- Epsilon used to stop algorithm: 2.19882e+199 #> \t- Use logger as stopper: 115"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerTime.html","id":null,"dir":"Reference","previous_headings":"","what":"Logger class to log the elapsed time — LoggerTime","title":"Logger class to log the elapsed time — LoggerTime","text":"class just logs elapsed time. handy one wants run algorithm just 2 hours see far comes within time. three time units available logging: minutes seconds microseconds","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerTime.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Logger class to log the elapsed time — LoggerTime","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerTime.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Logger class to log the elapsed time — LoggerTime","text":"","code":"LoggerTime$new(logger_id, use_as_stopper, max_time, time_unit)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerTime.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logger class to log the elapsed time — LoggerTime","text":"logger_id [character(1)] Unique identifier logger. use_as_stopper [logical(1)] Boolean indicate logger also used stopper. max_time [integer(1)] logger used stopper argument contains maximal time   available train model. time_unit [character(1)] Character specify time unit. Possible choices minutes,   seconds microseconds","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerTime.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Logger class to log the elapsed time — LoggerTime","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerTime.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Logger class to log the elapsed time — LoggerTime","text":"summarizeLogger() Summarize logger object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerTime.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Logger class to log the elapsed time — LoggerTime","text":"","code":"# Define logger: log_time = LoggerTime$new(\"time_minutes\", FALSE, 20, \"minutes\")  # Summarize logger: log_time$summarizeLogger() #> Time logger: #> \t- Tracked time unit: minutes"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossAbsolute.html","id":null,"dir":"Reference","previous_headings":"","what":"Absolute loss for regression tasks. — LossAbsolute","title":"Absolute loss for regression tasks. — LossAbsolute","text":"loss can used regression \\(y \\\\mathrm{R}\\).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossAbsolute.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Absolute loss for regression tasks. — LossAbsolute","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossAbsolute.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Absolute loss for regression tasks. — LossAbsolute","text":"Loss Function: $$   L(y, f(x)) = | y - f(x)| $$ Gradient: $$   \\frac{\\delta}{\\delta f(x)}\\ L(y, f(x)) = -\\mathrm{sign}(y - f(x)) $$ Initialization: $$   \\hat{f}^{[0]}(x) = \\mathrm{arg~min}_{c\\R}\\ \\frac{1}{n}\\sum\\limits_{=1}^n   L(y^{()}, c) = \\mathrm{median}(y) $$","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossAbsolute.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Absolute loss for regression tasks. — LossAbsolute","text":"","code":"LossAbsolute$new() LossAbsolute$new(offset)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossAbsolute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Absolute loss for regression tasks. — LossAbsolute","text":"offset [numeric(1)] Numerical value can used set custom offset. ,   value returned instead loss optimal initialization.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossAbsolute.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Absolute loss for regression tasks. — LossAbsolute","text":"","code":"# Create new loss object: absolute_loss = LossAbsolute$new() absolute_loss #> LossAbsolute: L(y,x) = |y - f(x)| #>"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossBinomial.html","id":null,"dir":"Reference","previous_headings":"","what":"0-1 Loss for binary classification derived of the binomial distribution — LossBinomial","title":"0-1 Loss for binary classification derived of the binomial distribution — LossBinomial","text":"loss can used binary classification. coding chosen acts \\(y \\\\{-1, 1\\}\\).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossBinomial.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"0-1 Loss for binary classification derived of the binomial distribution — LossBinomial","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossBinomial.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"0-1 Loss for binary classification derived of the binomial distribution — LossBinomial","text":"Loss Function: $$   L(y, f(x)) = \\log(1 + \\mathrm{exp}(-2yf(x))) $$ Gradient: $$   \\frac{\\delta}{\\delta f(x)}\\ L(y, f(x)) = - \\frac{y}{1 + \\mathrm{exp}(2yf)} $$ Initialization: $$   \\hat{f}^{[0]}(x) = \\frac{1}{2}\\mathrm{log}(p / (1 - p)) $$ $$   p = \\frac{1}{n}\\sum\\limits_{=1}^n\\mathrm{1}_{\\{y^{()} = 1\\}} $$","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossBinomial.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"0-1 Loss for binary classification derived of the binomial distribution — LossBinomial","text":"","code":"LossBinomial$new() LossBinomial$new(offset)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossBinomial.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"0-1 Loss for binary classification derived of the binomial distribution — LossBinomial","text":"offset [numeric(1)] Numerical value can used set custom offset. ,   value returned instead loss optimal initialization.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossBinomial.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"0-1 Loss for binary classification derived of the binomial distribution — LossBinomial","text":"","code":"# Create new loss object: bin_loss = LossBinomial$new() bin_loss #> LossBinomial: L(y,x) = log(1 + exp(-2yf(x)) #>"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustom.html","id":null,"dir":"Reference","previous_headings":"","what":"Create LossCustom by using R functions. — LossCustom","title":"Create LossCustom by using R functions. — LossCustom","text":"LossCustom creates custom loss using Rcpp::Function set R functions.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustom.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Create LossCustom by using R functions. — LossCustom","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustom.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create LossCustom by using R functions. — LossCustom","text":"","code":"LossCustom$new(lossFun, gradientFun, initFun)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustom.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create LossCustom by using R functions. — LossCustom","text":"lossFun [function] R function calculate loss. details see   Details. gradientFun [function] R function calculate gradient. details see   Details. initFun [function] R function calculate constant initialization.   details see Details.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustom.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create LossCustom by using R functions. — LossCustom","text":"functions must following structure: lossFun(truth, prediction) { ... return (loss) } vector   argument truth containing real values vector   predictions prediction. function must return vector   containing loss component. gradientFun(truth, prediction) { ... return (grad) } vector   argument truth containing real values vector   predictions prediction. function must return vector   containing gradient loss component. initFun(truth) { ... return (init) } vector   argument truth containing real values. function must   return numeric value containing offset constant   initialization.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustom.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create LossCustom by using R functions. — LossCustom","text":"","code":"# Loss function: myLoss = function (true_values, prediction) {   return (0.5 * (true_values - prediction)^2) } # Gradient of loss function: myGradient = function (true_values, prediction) {   return (prediction - true_values) } # Constant initialization: myConstInit = function (true_values) {   return (mean(true_values)) }  # Create new custom quadratic loss: my_loss = LossCustom$new(myLoss, myGradient, myConstInit)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustomCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"Create custom cpp losses by using cpp functions and external pointer. — LossCustomCpp","title":"Create custom cpp losses by using cpp functions and external pointer. — LossCustomCpp","text":"LossCustomCpp creates custom loss using Rcpp::XPtr set C++ functions.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustomCpp.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Create custom cpp losses by using cpp functions and external pointer. — LossCustomCpp","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustomCpp.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create custom cpp losses by using cpp functions and external pointer. — LossCustomCpp","text":"","code":"LossCustomCpp$new(loss_ptr, grad_ptr, const_init_ptr)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustomCpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create custom cpp losses by using cpp functions and external pointer. — LossCustomCpp","text":"loss_ptr [externalptr] External pointer C++ loss function. grad_ptr [externalptr] External pointer C++ gradient function. const_init_ptr [externalptr] External pointer C++ constant initialization function.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustomCpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create custom cpp losses by using cpp functions and external pointer. — LossCustomCpp","text":"","code":"if (FALSE) { # Load loss functions: Rcpp::sourceCpp(code = getCustomCppExample(example = \"loss\", silent = TRUE))  # Create new custom quadratic loss: my_cpp_loss = LossCustomCpp$new(lossFunSetter(), gradFunSetter(), constInitFunSetter()) }"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossHuber.html","id":null,"dir":"Reference","previous_headings":"","what":"Huber loss for regression tasks. — LossHuber","title":"Huber loss for regression tasks. — LossHuber","text":"loss can used regression \\(y \\\\mathrm{R}\\).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossHuber.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Huber loss for regression tasks. — LossHuber","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossHuber.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Huber loss for regression tasks. — LossHuber","text":"Loss Function: $$   L(y, f(x)) = 0.5(y - f(x))^2 \\ \\ \\mathrm{} \\ \\ |y - f(x)| < d $$ $$   L(y, f(x)) = d|y - f(x)| - 0.5d^2 \\ \\ \\mathrm{otherwise} $$ Gradient: $$   \\frac{\\delta}{\\delta f(x)}\\ L(y, f(x)) = f(x) - y \\ \\ \\mathrm{} \\ \\ |y - f(x)| < d $$ $$   \\frac{\\delta}{\\delta f(x)}\\ L(y, f(x)) = -d\\mathrm{sign}(y - f(x)) \\ \\ \\mathrm{otherwise} $$","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossHuber.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Huber loss for regression tasks. — LossHuber","text":"","code":"LossHuber$new() LossHuber$new(delta) LossHuber$new(offset, delta)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossHuber.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Huber loss for regression tasks. — LossHuber","text":"offset [numeric(1)] Numerical value can used set custom offset. ,   value returned instead loss optimal initialization. delta [numeric(1)] Numerical value greater 0 specify interval around 0 quadratic error measuring.   Default 1.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossHuber.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Huber loss for regression tasks. — LossHuber","text":"","code":"# Create new loss object: huber_loss = LossHuber$new() huber_loss #> LossHuber: L(y,x) = if (y - f(x) < d) { 0.5(y - f(x))^2 } else { d|y - f(x)| - 0.5d^2 } #>  #>   with delta d = 1"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuadratic.html","id":null,"dir":"Reference","previous_headings":"","what":"Quadratic loss for regression tasks. — LossQuadratic","title":"Quadratic loss for regression tasks. — LossQuadratic","text":"loss can used regression \\(y \\\\mathrm{R}\\).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuadratic.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Quadratic loss for regression tasks. — LossQuadratic","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuadratic.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Quadratic loss for regression tasks. — LossQuadratic","text":"Loss Function: $$   L(y, f(x)) = \\frac{1}{2}( y - f(x))^2 $$ Gradient: $$   \\frac{\\delta}{\\delta f(x)}\\ L(y, f(x)) = f(x) - y $$ Initialization: $$   \\hat{f}^{[0]}(x) = \\mathrm{arg~min}{c\\\\mathrm{R}}{\\mathrm{arg~min}}\\ \\frac{1}{n}\\sum\\limits_{=1}^n   L\\left(y^{()}, c\\right) = \\bar{y} $$","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuadratic.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quadratic loss for regression tasks. — LossQuadratic","text":"","code":"LossQuadratic$new() LossQuadratic$new(offset)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuadratic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quadratic loss for regression tasks. — LossQuadratic","text":"offset [numeric(1)] Numerical value can used set custom offset. ,   value returned instead loss optimal initialization.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuadratic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Quadratic loss for regression tasks. — LossQuadratic","text":"","code":"# Create new loss object: quadratic_loss = LossQuadratic$new() quadratic_loss #> LossQuadratic: L(y,x) = 0.5 * (y - f(x))^2 #>"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuantile.html","id":null,"dir":"Reference","previous_headings":"","what":"Quantile loss for regression tasks. — LossQuantile","title":"Quantile loss for regression tasks. — LossQuantile","text":"loss can used regression \\(y \\\\mathrm{R}\\).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuantile.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Quantile loss for regression tasks. — LossQuantile","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuantile.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Quantile loss for regression tasks. — LossQuantile","text":"Loss Function: $$   L(y, f(x)) = h| y - f(x)| $$ Gradient: $$   \\frac{\\delta}{\\delta f(x)}\\ L(y, f(x)) = -h\\mathrm{sign}( y - f(x)) $$ Initialization: $$   \\hat{f}^{[0]}(x) = \\mathrm{arg~min}_{c\\R}\\ \\frac{1}{n}\\sum\\limits_{=1}^n   L(y^{()}, c) = \\mathrm{quantile}(y, q) $$","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuantile.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quantile loss for regression tasks. — LossQuantile","text":"","code":"LossAbsolute$new() LossAbsolute$new(quantile) LossAbsolute$new(offset, quantile)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuantile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quantile loss for regression tasks. — LossQuantile","text":"offset [numeric(1)] Numerical value can used set custom offset. ,   value returned instead loss optimal initialization. quantile [numeric(1)] Numerical value 0 1 indicating quantile used boosting.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuantile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Quantile loss for regression tasks. — LossQuantile","text":"","code":"# Create new loss object: quadratic_loss = LossQuadratic$new() quadratic_loss #> LossQuadratic: L(y,x) = 0.5 * (y - f(x))^2 #>"},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerAGBM.html","id":null,"dir":"Reference","previous_headings":"","what":"Nesterov momentum — OptimizerAGBM","title":"Nesterov momentum — OptimizerAGBM","text":"class defines new object used conduct Nesterovs momentum optimization technique.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerAGBM.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Nesterov momentum — OptimizerAGBM","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerAGBM.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Nesterov momentum — OptimizerAGBM","text":"","code":"OptimizerAGBM$new(momentum) OptimizerAGBM$new(momentum, ncores)"},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerAGBM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Nesterov momentum — OptimizerAGBM","text":"momentum [numeric(1)] Momentum term used accelerate fitting process. chosen large, algorithm trains   faster also tends overfit faster. ncores [integer(1)] Number cores used fit algorithm. Note number used cores   smaller equal number base learner.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerAGBM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Nesterov momentum — OptimizerAGBM","text":"","code":"optimizer = OptimizerAGBM$new(0.1)"},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescent.html","id":null,"dir":"Reference","previous_headings":"","what":"Coordinate Descent — OptimizerCoordinateDescent","title":"Coordinate Descent — OptimizerCoordinateDescent","text":"class defines new object greedy optimizer. optimizer just calculates base-learner sum squared errors returns base-learner smallest SSE.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescent.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Coordinate Descent — OptimizerCoordinateDescent","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescent.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coordinate Descent — OptimizerCoordinateDescent","text":"","code":"OptimizerCoordinateDescent$new() OptimizerCoordinateDescent$new(ncores)"},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescent.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coordinate Descent — OptimizerCoordinateDescent","text":"ncores [integer(1)] Number cores used fit algorithm. Note number used cores   smaller equal number base learner.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescent.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Coordinate Descent — OptimizerCoordinateDescent","text":"","code":"# Define optimizer: optimizer = OptimizerCoordinateDescent$new()"},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescentLineSearch.html","id":null,"dir":"Reference","previous_headings":"","what":"Coordinate Descent with line search — OptimizerCoordinateDescentLineSearch","title":"Coordinate Descent with line search — OptimizerCoordinateDescentLineSearch","text":"class defines new object used conduct Coordinate Descent line search. optimizer just calculates base-learner sum squared error returns base-learner smallest SSE. addition, optimizer computes line search find optimal step size iteration.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescentLineSearch.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Coordinate Descent with line search — OptimizerCoordinateDescentLineSearch","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescentLineSearch.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coordinate Descent with line search — OptimizerCoordinateDescentLineSearch","text":"","code":"OptimizerCoordinateDescentLineSearch$new() OptimizerCoordinateDescentLineSearch$new(ncores)"},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescentLineSearch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coordinate Descent with line search — OptimizerCoordinateDescentLineSearch","text":"ncores [integer(1)] Number cores used fit algorithm. Note number used cores   smaller equal number base learner.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescentLineSearch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Coordinate Descent with line search — OptimizerCoordinateDescentLineSearch","text":"","code":"# Define optimizer: optimizer = OptimizerCoordinateDescentLineSearch$new()"},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCosineAnnealing.html","id":null,"dir":"Reference","previous_headings":"","what":"Coordinate Descent with Cosine Annealing — OptimizerCosineAnnealing","title":"Coordinate Descent with Cosine Annealing — OptimizerCosineAnnealing","text":"class defines new object used conduct Coordinate Descent cosine annealing learning rate strategy.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCosineAnnealing.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Coordinate Descent with Cosine Annealing — OptimizerCosineAnnealing","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCosineAnnealing.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coordinate Descent with Cosine Annealing — OptimizerCosineAnnealing","text":"","code":"OptimizerCosineAnnealing$new() OptimizerCosineAnnealing$new(ncores) OptimizerCosineAnnealing$new(nu_min, nu_max, cycles, anneal_iter_max, cycles) OptimizerCosineAnnealing$new(nu_min, nu_max, cycles, anneal_iter_max, cycles, ncores)"},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCosineAnnealing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coordinate Descent with Cosine Annealing — OptimizerCosineAnnealing","text":"nu_min [numeric(1)] Minimal learning rate. nu_max [numeric(1)] Maximal learning rate. cycles [integer(1)] Number annealings form nu_max nu_min 1 anneal_iter_max. anneal_iter_max [integer(1)] Maximal number iters annealing applied. iteration bigger   anneal_iter_max, nu_min used fixed learning rate. ncores [integer(1)] Number cores used fit algorithm. Note number used cores   smaller equal number base learner.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCosineAnnealing.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Coordinate Descent with Cosine Annealing — OptimizerCosineAnnealing","text":"","code":"# Define optimizer: optimizer = OptimizerCosineAnnealing$new()"},{"path":"https://schalkdaniel.github.io/compboost/reference/ResponseBinaryClassif.html","id":null,"dir":"Reference","previous_headings":"","what":"Create response object for binary classification. — ResponseBinaryClassif","title":"Create response object for binary classification. — ResponseBinaryClassif","text":"ResponseBinaryClassif creates response object used target fitting process.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/ResponseBinaryClassif.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Create response object for binary classification. — ResponseBinaryClassif","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/ResponseBinaryClassif.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create response object for binary classification. — ResponseBinaryClassif","text":"","code":"ResponseBinaryClassif$new(target_name, pos_class, response) ResponseBinaryClassif$new(target_name, pos_class, response, weights)"},{"path":"https://schalkdaniel.github.io/compboost/reference/ResponseBinaryClassif.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create response object for binary classification. — ResponseBinaryClassif","text":"","code":"response_binary = ResponseBinaryClassif$new(\"target\", \"A\", sample(c(\"A\", \"B\"), 10, TRUE)) response_binary$getResponse() #>       [,1] #>  [1,]    1 #>  [2,]   -1 #>  [3,]    1 #>  [4,]    1 #>  [5,]    1 #>  [6,]    1 #>  [7,]   -1 #>  [8,]   -1 #>  [9,]    1 #> [10,]   -1 response_binary$getPrediction() #>       [,1] #>  [1,]    0 #>  [2,]    0 #>  [3,]    0 #>  [4,]    0 #>  [5,]    0 #>  [6,]    0 #>  [7,]    0 #>  [8,]    0 #>  [9,]    0 #> [10,]    0 response_binary$getPredictionTransform() # Applies sigmoid to prediction scores #>       [,1] #>  [1,]  0.5 #>  [2,]  0.5 #>  [3,]  0.5 #>  [4,]  0.5 #>  [5,]  0.5 #>  [6,]  0.5 #>  [7,]  0.5 #>  [8,]  0.5 #>  [9,]  0.5 #> [10,]  0.5 response_binary$getPredictionResponse()  # Categorizes depending on the transformed predictions #>       [,1] #>  [1,]    1 #>  [2,]    1 #>  [3,]    1 #>  [4,]    1 #>  [5,]    1 #>  [6,]    1 #>  [7,]    1 #>  [8,]    1 #>  [9,]    1 #> [10,]    1 response_binary$getTargetName() #> [1] \"target\" response_binary$setThreshold(0.7) response_binary$getThreshold() #> [1] 0.7 response_binary$getPositiveClass() #> [1] \"A\""},{"path":"https://schalkdaniel.github.io/compboost/reference/ResponseRegr.html","id":null,"dir":"Reference","previous_headings":"","what":"Create response object for regression. — ResponseRegr","title":"Create response object for regression. — ResponseRegr","text":"ResponseRegr creates response object used target fitting process.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/ResponseRegr.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Create response object for regression. — ResponseRegr","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/ResponseRegr.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create response object for regression. — ResponseRegr","text":"","code":"ResponseRegr$new(target_name, response) ResponseRegr$new(target_name, response, weights)"},{"path":"https://schalkdaniel.github.io/compboost/reference/ResponseRegr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create response object for regression. — ResponseRegr","text":"","code":"response_regr = ResponseRegr$new(\"target\", cbind(rnorm(10))) response_regr$getResponse() #>             [,1] #>  [1,]  1.7869131 #>  [2,]  0.4978505 #>  [3,] -1.9666172 #>  [4,]  0.7013559 #>  [5,] -0.4727914 #>  [6,] -1.0678237 #>  [7,] -0.2179749 #>  [8,] -1.0260044 #>  [9,] -0.7288912 #> [10,] -0.6250393 response_regr$getTargetName() #> [1] \"target\""},{"path":"https://schalkdaniel.github.io/compboost/reference/boostLinear.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper to boost linear models for each feature. — boostLinear","title":"Wrapper to boost linear models for each feature. — boostLinear","text":"wrapper function automatically initializes model adding numerical features linear base-learner. Categorical features dummy encoded inserted using another linear base-learners without intercept. function boostLinear also train model.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/boostLinear.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper to boost linear models for each feature. — boostLinear","text":"","code":"boostLinear(   data,   target,   optimizer = NULL,   loss = NULL,   learning_rate = 0.05,   iterations = 100,   trace = -1,   intercept = TRUE,   data_source = InMemoryData,   oob_fraction = NULL )"},{"path":"https://schalkdaniel.github.io/compboost/reference/boostLinear.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper to boost linear models for each feature. — boostLinear","text":"data [data.frame] data frame containing data. target [character(1) Response class] Character value containing target variable response object. Note loss must match data type target. optimizer [S4 Optimizer] initialized S4 Optimizer object exposed Rcpp (e.g. OptimizerCoordinateDescent$new()) select features iteration. loss [S4 Loss] Initialized S4 Loss object exposed Rcpp used calculate risk pseudo residuals (e.g. LossQuadratic$new()). learning_rate [numeric(1)] Learning rate shrink parameter step. iterations [integer(1)] Number iterations trained. `iterations == 0`, untrained object returned. can useful base learners (e.g. interaction via tensor base learner) added. trace [integer(1)] Integer indicating often trace printed. Specifying trace = 10, every 10th iteration printed. trace printed set trace = 0. Default -1 means total 40 iterations printed. intercept [logical(1)] Internally used BaselearnerPolynomial. logical value indicates feature get intercept (default TRUE). data_source [S4 Data] Uninitialized S4 Data object used store data. moment just memory training supported. oob_fraction [numeric(1)] Fraction much data used track bag risk.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/boostLinear.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper to boost linear models for each feature. — boostLinear","text":"model Compboost class. model R6 object   can used retraining, predicting, plotting, anything described  ?Compboost.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/boostLinear.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Wrapper to boost linear models for each feature. — boostLinear","text":"returned object object Compboost class. object can used analyses (see ?Compboost details).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/boostLinear.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wrapper to boost linear models for each feature. — boostLinear","text":"","code":"mod = boostLinear(data = iris, target = \"Sepal.Length\", loss = LossQuadratic$new(),   oob_fraction = 0.3) #>   1/100   risk = 0.32  oob_risk = 0.29    #>   2/100   risk = 0.3  oob_risk = 0.27    #>   4/100   risk = 0.26  oob_risk = 0.23    #>   6/100   risk = 0.23  oob_risk = 0.2    #>   8/100   risk = 0.2  oob_risk = 0.18    #>  10/100   risk = 0.18  oob_risk = 0.16    #>  12/100   risk = 0.16  oob_risk = 0.14    #>  14/100   risk = 0.15  oob_risk = 0.13    #>  16/100   risk = 0.14  oob_risk = 0.12    #>  18/100   risk = 0.13  oob_risk = 0.11    #>  20/100   risk = 0.12  oob_risk = 0.11    #>  22/100   risk = 0.11  oob_risk = 0.1    #>  24/100   risk = 0.11  oob_risk = 0.096    #>  26/100   risk = 0.1  oob_risk = 0.093    #>  28/100   risk = 0.099  oob_risk = 0.09    #>  30/100   risk = 0.097  oob_risk = 0.088    #>  32/100   risk = 0.094  oob_risk = 0.086    #>  34/100   risk = 0.092  oob_risk = 0.084    #>  36/100   risk = 0.091  oob_risk = 0.082    #>  38/100   risk = 0.089  oob_risk = 0.08    #>  40/100   risk = 0.087  oob_risk = 0.078    #>  42/100   risk = 0.086  oob_risk = 0.076    #>  44/100   risk = 0.084  oob_risk = 0.075    #>  46/100   risk = 0.083  oob_risk = 0.073    #>  48/100   risk = 0.081  oob_risk = 0.071    #>  50/100   risk = 0.08  oob_risk = 0.07    #>  52/100   risk = 0.079  oob_risk = 0.069    #>  54/100   risk = 0.078  oob_risk = 0.067    #>  56/100   risk = 0.077  oob_risk = 0.066    #>  58/100   risk = 0.076  oob_risk = 0.065    #>  60/100   risk = 0.075  oob_risk = 0.064    #>  62/100   risk = 0.074  oob_risk = 0.063    #>  64/100   risk = 0.073  oob_risk = 0.062    #>  66/100   risk = 0.073  oob_risk = 0.061    #>  68/100   risk = 0.072  oob_risk = 0.06    #>  70/100   risk = 0.071  oob_risk = 0.06    #>  72/100   risk = 0.07  oob_risk = 0.059    #>  74/100   risk = 0.07  oob_risk = 0.058    #>  76/100   risk = 0.069  oob_risk = 0.057    #>  78/100   risk = 0.069  oob_risk = 0.057    #>  80/100   risk = 0.068  oob_risk = 0.056    #>  82/100   risk = 0.068  oob_risk = 0.056    #>  84/100   risk = 0.067  oob_risk = 0.055    #>  86/100   risk = 0.067  oob_risk = 0.055    #>  88/100   risk = 0.066  oob_risk = 0.054    #>  90/100   risk = 0.066  oob_risk = 0.054    #>  92/100   risk = 0.066  oob_risk = 0.053    #>  94/100   risk = 0.065  oob_risk = 0.053    #>  96/100   risk = 0.065  oob_risk = 0.053    #>  98/100   risk = 0.065  oob_risk = 0.052    #> 100/100   risk = 0.064  oob_risk = 0.052    #>  #>  #> Train 100 iterations in 0 Seconds. #> Final risk based on the train set: 0.064 #>  mod$getBaselearnerNames() #> [1] \"Sepal.Width_linear\"  \"Petal.Length_linear\" \"Petal.Width_linear\"  #> [4] \"Species_ridge\"       mod$getEstimatedCoef() #> Depricated, use `$getCoef()` instead. #> $Petal.Length_linear #>            [,1] #> [1,] -1.6141315 #> [2,]  0.4206597 #> attr(,\"blclass\") #> [1] \"Rcpp_BaselearnerPolynomial\" #>  #> $Sepal.Width_linear #>            [,1] #> [1,] -1.1423498 #> [2,]  0.3755377 #> attr(,\"blclass\") #> [1] \"Rcpp_BaselearnerPolynomial\" #>  #> $offset #> [1] 5.871429 #>  table(mod$getSelectedBaselearner()) #>  #> Petal.Length_linear  Sepal.Width_linear  #>                  66                  34  mod$predict() #>            [,1] #>   [1,] 5.018253 #>   [2,] 4.830484 #>   [3,] 4.863525 #>   [4,] 5.055806 #>   [5,] 4.980699 #>   [6,] 5.022765 #>   [7,] 4.792930 #>   [8,] 4.910104 #>   [9,] 5.135426 #>  [10,] 5.064831 #>  [11,] 4.704286 #>  [12,] 5.121890 #>  [13,] 5.126402 #>  [14,] 5.018253 #>  [15,] 5.257112 #>  [16,] 5.172980 #>  [17,] 4.887543 #>  [18,] 5.069343 #>  [19,] 4.914616 #>  [20,] 5.064831 #>  [21,] 5.060319 #>  [22,] 4.980699 #>  [23,] 5.285641 #>  [24,] 4.821459 #>  [25,] 4.976187 #>  [26,] 4.525542 #>  [27,] 4.863525 #>  [28,] 5.102385 #>  [29,] 5.341244 #>  [30,] 4.830484 #>  [31,] 5.215046 #>  [32,] 4.905591 #>  [33,] 5.135426 #>  [34,] 6.293769 #>  [35,] 5.661323 #>  [36,] 6.101488 #>  [37,] 6.059422 #>  [38,] 6.331322 #>  [39,] 5.404415 #>  [40,] 6.139041 #>  [41,] 5.338332 #>  [42,] 6.008331 #>  [43,] 6.181107 #>  [44,] 5.718382 #>  [45,] 6.130017 #>  [46,] 6.134529 #>  [47,] 5.853604 #>  [48,] 5.694364 #>  [49,] 6.335835 #>  [50,] 6.115024 #>  [51,] 6.143553 #>  [52,] 6.012843 #>  [53,] 6.185619 #>  [54,] 6.344859 #>  [55,] 6.096975 #>  [56,] 5.563654 #>  [57,] 5.572679 #>  [58,] 5.769472 #>  [59,] 6.274264 #>  [60,] 6.134529 #>  [61,] 5.829587 #>  [62,] 6.176595 #>  [63,] 5.773984 #>  [64,] 5.895670 #>  [65,] 6.008331 #>  [66,] 6.012843 #>  [67,] 5.315771 #>  [68,] 5.891158 #>  [69,] 6.878180 #>  [70,] 6.274264 #>  [71,] 6.723453 #>  [72,] 6.559701 #>  [73,] 6.681387 #>  [74,] 7.017915 #>  [75,] 5.946760 #>  [76,] 6.854163 #>  [77,] 6.493618 #>  [78,] 6.462032 #>  [79,] 6.157090 #>  [80,] 6.555189 #>  [81,] 6.993897 #>  [82,] 6.044429 #>  [83,] 6.714428 #>  [84,] 6.227685 #>  [85,] 6.984873 #>  [86,] 6.190132 #>  [87,] 6.751982 #>  [88,] 6.840626 #>  [89,] 6.185619 #>  [90,] 6.302793 #>  [91,] 6.522147 #>  [92,] 6.732477 #>  [93,] 7.234213 #>  [94,] 6.311817 #>  [95,] 6.807585 #>  [96,] 6.747470 #>  [97,] 6.592743 #>  [98,] 6.260727 #>  [99,] 6.550677 #> [100,] 6.424479 #> [101,] 6.274264 #> [102,] 6.798560 #> [103,] 6.751982 #> [104,] 6.428991 #> [105,] 6.663338"},{"path":"https://schalkdaniel.github.io/compboost/reference/boostSplines.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper to boost general additive models for each feature. — boostSplines","title":"Wrapper to boost general additive models for each feature. — boostSplines","text":"wrapper function automatically initializes model adding numerical features spline base-learner. Categorical features dummy encoded inserted using another linear base-learners without intercept. function boostSplines also train model.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/boostSplines.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper to boost general additive models for each feature. — boostSplines","text":"","code":"boostSplines(   data,   target,   optimizer = NULL,   loss = NULL,   learning_rate = 0.05,   iterations = 100,   trace = -1,   degree = 3,   n_knots = 20,   penalty = 2,   df = 0,   differences = 2,   data_source = InMemoryData,   oob_fraction = NULL,   bin_root = 0,   cache_type = \"inverse\",   stop_args = list(),   df_cat = 1,   stop_time = \"microseconds\",   additional_risk_logs = list() )"},{"path":"https://schalkdaniel.github.io/compboost/reference/boostSplines.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper to boost general additive models for each feature. — boostSplines","text":"data [data.frame] data frame containing data. target [character(1) Response class] Character value containing target variable Response object. Note loss must match data type target. optimizer [S4 Optimizer] initialized S4 Optimizer object exposed Rcpp (e.g. OptimizerCoordinateDescent$new()) select features iteration. loss [S4 Loss] Initialized S4 Loss object exposed Rcpp used calculate risk pseudo residuals (e.g. LossQuadratic$new()). learning_rate [numeric(1)] Learning rate shrink parameter step. iterations [integer(1)] Number iterations trained. `iterations == 0`, untrained object returned. can useful base learners (e.g. interaction via tensor base learner) added. trace [integer(1)] Integer indicating often trace printed. Specifying trace = 10, every 10th iteration printed. trace printed set trace = 0. Default -1 means total 40 iterations printed. degree [integer(1)] Polynomial degree splines. n_knots [integer(1)] Number equidistant \"inner knots\". actual number used knots also depend polynomial degree. penalty [numeric(1)] Penalty term p-splines. penalty equals 0, ordinary b-splines fitted. higher penalty, higher smoothness. df [numeric(1)] Degrees freedom whole spline. important set amount degrees freedom able compare different base-learner. differences [integer(1)] Number differences used penalization. higher difference, higher smoothness. data_source [S4 Data] Uninitialized S4 Data object used store data. moment just memory training supported. oob_fraction [numeric(1)] Fraction much data want use track bag risk. bin_root [integer(1)] set value greater zero, binning applied reduces number used x values n^(1/bin_root) equidistant points. want use binning suggest set bin_root = 2. cache_type [character(1)+] String indicate method used estimate parameter iteration. Default cache_type = \"cholesky\" computes Cholesky decomposition, caches , reuses matrix . option use cache_type = \"inverse\" caches inverse. stop_args [list(2)] List containing two elements `patience` `eps_for_break` can set use early stopping left data setting `oob_fraction`. df_cat [numeric(1)] Degrees freedom categorical base-learner. stop_time [character(1)] Unit measured time. additional_risk_logs [list(Logger)] Additional logger passed `Compboost` object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/boostSplines.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper to boost general additive models for each feature. — boostSplines","text":"model Compboost class. model R6 object   can used retraining, predicting, plotting, anything described  ?Compboost.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/boostSplines.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Wrapper to boost general additive models for each feature. — boostSplines","text":"returned object object Compboost class. object can used analyses (see ?Compboost details).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/boostSplines.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wrapper to boost general additive models for each feature. — boostSplines","text":"","code":"mod = boostSplines(data = iris, target = \"Sepal.Length\", loss = LossQuadratic$new(),   oob_fraction = 0.3) #>   1/100   risk = 0.31  oob_risk = 0.32   time = 0    #>   2/100   risk = 0.28  oob_risk = 0.29   time = 46226    #>   4/100   risk = 0.24  oob_risk = 0.26   time = 53359    #>   6/100   risk = 0.21  oob_risk = 0.23   time = 53482    #>   8/100   risk = 0.18  oob_risk = 0.2   time = 53575    #>  10/100   risk = 0.15  oob_risk = 0.18   time = 53668    #>  12/100   risk = 0.13  oob_risk = 0.17   time = 53758    #>  14/100   risk = 0.12  oob_risk = 0.15   time = 53848    #>  16/100   risk = 0.11  oob_risk = 0.14   time = 53936    #>  18/100   risk = 0.095  oob_risk = 0.13   time = 54029    #>  20/100   risk = 0.087  oob_risk = 0.13   time = 54121    #>  22/100   risk = 0.08  oob_risk = 0.12   time = 54211    #>  24/100   risk = 0.074  oob_risk = 0.11   time = 54302    #>  26/100   risk = 0.069  oob_risk = 0.11   time = 54394    #>  28/100   risk = 0.066  oob_risk = 0.11   time = 54485    #>  30/100   risk = 0.062  oob_risk = 0.11   time = 54609    #>  32/100   risk = 0.06  oob_risk = 0.11   time = 54704    #>  34/100   risk = 0.058  oob_risk = 0.1   time = 54798    #>  36/100   risk = 0.056  oob_risk = 0.1   time = 54890    #>  38/100   risk = 0.055  oob_risk = 0.1   time = 54985    #>  40/100   risk = 0.053  oob_risk = 0.1   time = 55078    #>  42/100   risk = 0.052  oob_risk = 0.1   time = 55196    #>  44/100   risk = 0.051  oob_risk = 0.1   time = 55290    #>  46/100   risk = 0.05  oob_risk = 0.1   time = 55388    #>  48/100   risk = 0.049  oob_risk = 0.1   time = 55482    #>  50/100   risk = 0.048  oob_risk = 0.1   time = 55573    #>  52/100   risk = 0.048  oob_risk = 0.1   time = 55665    #>  54/100   risk = 0.047  oob_risk = 0.1   time = 55757    #>  56/100   risk = 0.046  oob_risk = 0.1   time = 55852    #>  58/100   risk = 0.046  oob_risk = 0.1   time = 55956    #>  60/100   risk = 0.045  oob_risk = 0.099   time = 56051    #>  62/100   risk = 0.044  oob_risk = 0.099   time = 56148    #>  64/100   risk = 0.044  oob_risk = 0.099   time = 56242    #>  66/100   risk = 0.043  oob_risk = 0.099   time = 56339    #>  68/100   risk = 0.043  oob_risk = 0.099   time = 56433    #>  70/100   risk = 0.043  oob_risk = 0.099   time = 56528    #>  72/100   risk = 0.042  oob_risk = 0.099   time = 56621    #>  74/100   risk = 0.042  oob_risk = 0.098   time = 56714    #>  76/100   risk = 0.041  oob_risk = 0.098   time = 56808    #>  78/100   risk = 0.041  oob_risk = 0.098   time = 56903    #>  80/100   risk = 0.041  oob_risk = 0.098   time = 56996    #>  82/100   risk = 0.04  oob_risk = 0.098   time = 57091    #>  84/100   risk = 0.04  oob_risk = 0.098   time = 57186    #>  86/100   risk = 0.04  oob_risk = 0.098   time = 57279    #>  88/100   risk = 0.039  oob_risk = 0.098   time = 57372    #>  90/100   risk = 0.039  oob_risk = 0.097   time = 57466    #>  92/100   risk = 0.039  oob_risk = 0.097   time = 57560    #>  94/100   risk = 0.039  oob_risk = 0.097   time = 57653    #>  96/100   risk = 0.038  oob_risk = 0.097   time = 57747    #>  98/100   risk = 0.038  oob_risk = 0.097   time = 57842    #> 100/100   risk = 0.038  oob_risk = 0.097   time = 57937    #>  #>  #> Train 100 iterations in 0 Seconds. #> Final risk based on the train set: 0.038 #>  mod$getBaselearnerNames() #> [1] \"Sepal.Width_spline\"  \"Petal.Length_spline\" \"Petal.Width_spline\"  #> [4] \"Species_ridge\"       mod$getEstimatedCoef() #> Depricated, use `$getCoef()` instead. #> $Petal.Length_spline #>               [,1] #>  [1,] -1.020038063 #>  [2,] -0.842700185 #>  [3,] -0.708210074 #>  [4,] -0.616528258 #>  [5,] -0.629992257 #>  [6,] -0.674377781 #>  [7,] -0.700732067 #>  [8,] -0.697215039 #>  [9,] -0.651967365 #> [10,] -0.537788304 #> [11,] -0.341686833 #> [12,] -0.177164903 #> [13,]  0.007505898 #> [14,]  0.290089474 #> [15,]  0.451293220 #> [16,]  0.400021292 #> [17,]  0.366344295 #> [18,]  0.483695287 #> [19,]  0.784027942 #> [20,]  1.123146019 #> [21,]  1.463578011 #> [22,]  1.656349917 #> [23,]  1.738770154 #> [24,]  1.806016019 #> attr(,\"blclass\") #> [1] \"Rcpp_BaselearnerPSpline\" #>  #> $Petal.Width_spline #>               [,1] #>  [1,] -0.368363815 #>  [2,] -0.230727829 #>  [3,] -0.097569154 #>  [4,] -0.016578666 #>  [5,] -0.028837656 #>  [6,] -0.057857155 #>  [7,] -0.057668639 #>  [8,] -0.028703497 #>  [9,] -0.002385605 #> [10,] -0.010779152 #> [11,] -0.054068445 #> [12,] -0.040466735 #> [13,]  0.064127383 #> [14,]  0.136240247 #> [15,]  0.133221263 #> [16,]  0.102792184 #> [17,]  0.067773564 #> [18,]  0.110728014 #> [19,]  0.161049540 #> [20,]  0.160193965 #> [21,]  0.130363799 #> [22,]  0.046188478 #> [23,] -0.067301221 #> [24,] -0.181037323 #> attr(,\"blclass\") #> [1] \"Rcpp_BaselearnerPSpline\" #>  #> $Sepal.Width_spline #>               [,1] #>  [1,] -0.391634041 #>  [2,] -0.234533599 #>  [3,] -0.086240363 #>  [4,]  0.009214706 #>  [5,]  0.007799400 #>  [6,] -0.065038926 #>  [7,] -0.104271259 #>  [8,] -0.096040595 #>  [9,] -0.054096040 #> [10,] -0.017548114 #> [11,] -0.012499394 #> [12,]  0.013963397 #> [13,] -0.016707110 #> [14,] -0.064197029 #> [15,] -0.059409731 #> [16,]  0.010539003 #> [17,]  0.075101623 #> [18,]  0.111945966 #> [19,]  0.157947620 #> [20,]  0.253521014 #> [21,]  0.357967518 #> [22,]  0.374894173 #> [23,]  0.371380710 #> [24,]  0.369954604 #> attr(,\"blclass\") #> [1] \"Rcpp_BaselearnerPSpline\" #>  #> $offset #> [1] 5.79619 #>  table(mod$getSelectedBaselearner()) #>  #> Petal.Length_spline  Petal.Width_spline  Sepal.Width_spline  #>                  45                  26                  29  mod$predict() #>            [,1] #>   [1,] 5.001037 #>   [2,] 4.971113 #>   [3,] 4.890050 #>   [4,] 5.003350 #>   [5,] 5.410319 #>   [6,] 4.975044 #>   [7,] 4.956201 #>   [8,] 4.891916 #>   [9,] 5.129464 #>  [10,] 5.001642 #>  [11,] 4.859679 #>  [12,] 4.715600 #>  [13,] 5.228497 #>  [14,] 5.077558 #>  [15,] 5.303721 #>  [16,] 5.262642 #>  [17,] 5.223762 #>  [18,] 5.012599 #>  [19,] 5.036910 #>  [20,] 4.939170 #>  [21,] 4.994930 #>  [22,] 5.029948 #>  [23,] 5.266600 #>  [24,] 5.342117 #>  [25,] 5.003350 #>  [26,] 4.842022 #>  [27,] 4.958629 #>  [28,] 4.942359 #>  [29,] 4.928705 #>  [30,] 4.975044 #>  [31,] 5.035150 #>  [32,] 4.890050 #>  [33,] 5.127204 #>  [34,] 5.317974 #>  [35,] 5.212719 #>  [36,] 4.932458 #>  [37,] 5.129464 #>  [38,] 4.910691 #>  [39,] 6.220816 #>  [40,] 6.336589 #>  [41,] 6.109429 #>  [42,] 6.280500 #>  [43,] 5.204318 #>  [44,] 6.182060 #>  [45,] 5.668333 #>  [46,] 5.142687 #>  [47,] 6.005284 #>  [48,] 5.688583 #>  [49,] 5.457387 #>  [50,] 6.140691 #>  [51,] 5.707504 #>  [52,] 6.247146 #>  [53,] 6.262478 #>  [54,] 5.694635 #>  [55,] 6.247862 #>  [56,] 6.144328 #>  [57,] 6.276506 #>  [58,] 6.296937 #>  [59,] 5.282725 #>  [60,] 5.495523 #>  [61,] 5.459322 #>  [62,] 5.537781 #>  [63,] 6.242001 #>  [64,] 6.223752 #>  [65,] 6.067748 #>  [66,] 5.802462 #>  [67,] 5.635231 #>  [68,] 5.917087 #>  [69,] 6.273111 #>  [70,] 5.819931 #>  [71,] 5.833788 #>  [72,] 5.873289 #>  [73,] 5.015609 #>  [74,] 5.769503 #>  [75,] 6.704795 #>  [76,] 6.870084 #>  [77,] 6.445517 #>  [78,] 6.740277 #>  [79,] 7.253793 #>  [80,] 6.581417 #>  [81,] 6.285423 #>  [82,] 6.232806 #>  [83,] 6.242707 #>  [84,] 6.179686 #>  [85,] 6.267134 #>  [86,] 7.795450 #>  [87,] 7.541891 #>  [88,] 6.312717 #>  [89,] 7.576167 #>  [90,] 6.578978 #>  [91,] 6.874760 #>  [92,] 6.719281 #>  [93,] 7.603128 #>  [94,] 6.493966 #>  [95,] 6.281089 #>  [96,] 6.372074 #>  [97,] 6.376361 #>  [98,] 6.374356 #>  [99,] 6.404666 #> [100,] 6.777900 #> [101,] 6.352684 #> [102,] 6.289097 #> [103,] 6.203257 #> [104,] 6.311141 #> [105,] 6.264772"},{"path":"https://schalkdaniel.github.io/compboost/reference/getCustomCppExample.html","id":null,"dir":"Reference","previous_headings":"","what":"Get C++ example script to define a custom cpp logger — getCustomCppExample","title":"Get C++ example script to define a custom cpp logger — getCustomCppExample","text":"function can used print trace parameters   trained compboost object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/getCustomCppExample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get C++ example script to define a custom cpp logger — getCustomCppExample","text":"","code":"getCustomCppExample(example = \"blearner\", silent = FALSE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/getCustomCppExample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get C++ example script to define a custom cpp logger — getCustomCppExample","text":"example [character(1)]  Character value indicating example base-learner loss returned. values one blearner loss. silent [logical(1)]  Logical value indicating example code printed screen .","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/getCustomCppExample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get C++ example script to define a custom cpp logger — getCustomCppExample","text":"function returns character vector can compiled using  Rcpp::sourceCpp(code = getCustomCppExample()) define new    custom cpp logger.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearner.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize contribution of one base learner — plotBaselearner","title":"Visualize contribution of one base learner — plotBaselearner","text":"function visualizes contribution base learner overall prediction score. visualization partial effects see plotPEUni.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize contribution of one base learner — plotBaselearner","text":"","code":"plotBaselearner(cboost, blname, npoints = 100L)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize contribution of one base learner — plotBaselearner","text":"cboost [Compboost class] trained Compboost object. blname [character(1L)] Name base learner. Must one cboost$getBaselearnerNames(). npoints [integer(1L)] Number points predicted lines (applies numerical features).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize contribution of one base learner — plotBaselearner","text":"ggplot object containing graphic.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearner.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize contribution of one base learner — plotBaselearner","text":"","code":"cboost = Compboost$new(data = iris, target = \"Petal.Length\",   loss = LossQuadratic$new()) cboost$addComponents(\"Sepal.Width\") cboost$train(500L) #>   1/500   risk = 1.5   #>  12/500   risk = 1.3   #>  24/500   risk = 1.2   #>  36/500   risk = 1.2   #>  48/500   risk = 1.1   #>  60/500   risk = 1.1   #>  72/500   risk = 1.1   #>  84/500   risk = 1.1   #>  96/500   risk = 1.1   #> 108/500   risk = 1.1   #> 120/500   risk = 1.1   #> 132/500   risk = 1.1   #> 144/500   risk = 1.1   #> 156/500   risk = 1.1   #> 168/500   risk = 1.1   #> 180/500   risk = 1.1   #> 192/500   risk = 1   #> 204/500   risk = 1   #> 216/500   risk = 1   #> 228/500   risk = 1   #> 240/500   risk = 1   #> 252/500   risk = 1   #> 264/500   risk = 1   #> 276/500   risk = 1   #> 288/500   risk = 1   #> 300/500   risk = 1   #> 312/500   risk = 1   #> 324/500   risk = 1   #> 336/500   risk = 1   #> 348/500   risk = 1   #> 360/500   risk = 1   #> 372/500   risk = 1   #> 384/500   risk = 1   #> 396/500   risk = 1   #> 408/500   risk = 1   #> 420/500   risk = 1   #> 432/500   risk = 1   #> 444/500   risk = 1   #> 456/500   risk = 1   #> 468/500   risk = 1   #> 480/500   risk = 1   #> 492/500   risk = 1   #>  #>  #> Train 500 iterations in 0 Seconds. #> Final risk based on the train set: 1 #>  plotBaselearner(cboost, \"Sepal.Width_linear\")  plotBaselearner(cboost, \"Sepal.Width_Sepal.Width_spline_centered\")"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearnerTraces.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize base learner traces — plotBaselearnerTraces","title":"Visualize base learner traces — plotBaselearnerTraces","text":"function shows base learners evolves fitting process. default show frequency single base learner included model evolves. Additionally, value argument, vectors (e.g. risk) can used show base learner specific risk reduction evolves fitting process.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearnerTraces.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize base learner traces — plotBaselearnerTraces","text":"","code":"plotBaselearnerTraces(cboost, value = 1, n_legend = 5L)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearnerTraces.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize base learner traces — plotBaselearnerTraces","text":"cboost [Compboost class] trained Compboost object. value [numeric(1L) | numeric(length(cboost$getSelectedBaselearner()))] Value used show base learner development w.r.t. value. n_legend [integer(1L)] Number colored base learners added legend.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearnerTraces.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize base learner traces — plotBaselearnerTraces","text":"ggplot object containing graphic.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearnerTraces.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize base learner traces — plotBaselearnerTraces","text":"","code":"cboost = Compboost$new(data = iris, target = \"Petal.Length\",  loss = LossQuadratic$new()) cboost$addComponents(\"Sepal.Width\") cboost$addBaselearner(\"Species\", \"ridge\", BaselearnerCategoricalRidge) cboost$train(500L) #>   1/500   risk = 1.4   #>  12/500   risk = 0.52   #>  24/500   risk = 0.21   #>  36/500   risk = 0.13   #>  48/500   risk = 0.1   #>  60/500   risk = 0.093   #>  72/500   risk = 0.088   #>  84/500   risk = 0.085   #>  96/500   risk = 0.083   #> 108/500   risk = 0.081   #> 120/500   risk = 0.079   #> 132/500   risk = 0.078   #> 144/500   risk = 0.077   #> 156/500   risk = 0.076   #> 168/500   risk = 0.075   #> 180/500   risk = 0.074   #> 192/500   risk = 0.074   #> 204/500   risk = 0.073   #> 216/500   risk = 0.073   #> 228/500   risk = 0.072   #> 240/500   risk = 0.072   #> 252/500   risk = 0.072   #> 264/500   risk = 0.071   #> 276/500   risk = 0.071   #> 288/500   risk = 0.071   #> 300/500   risk = 0.071   #> 312/500   risk = 0.071   #> 324/500   risk = 0.07   #> 336/500   risk = 0.07   #> 348/500   risk = 0.07   #> 360/500   risk = 0.07   #> 372/500   risk = 0.07   #> 384/500   risk = 0.07   #> 396/500   risk = 0.069   #> 408/500   risk = 0.069   #> 420/500   risk = 0.069   #> 432/500   risk = 0.069   #> 444/500   risk = 0.069   #> 456/500   risk = 0.069   #> 468/500   risk = 0.069   #> 480/500   risk = 0.069   #> 492/500   risk = 0.069   #>  #>  #> Train 500 iterations in 0 Seconds. #> Final risk based on the train set: 0.069 #>  plotBaselearnerTraces(cboost)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotFeatureImportance.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize the feature importance — plotFeatureImportance","title":"Visualize the feature importance — plotFeatureImportance","text":"function visualizes feature importance horizontal bar plot.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotFeatureImportance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize the feature importance — plotFeatureImportance","text":"","code":"plotFeatureImportance(cboost, num_feats = NULL, aggregate = TRUE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotFeatureImportance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize the feature importance — plotFeatureImportance","text":"cboost [Compboost class] trained Compboost object. num_feats [integer(1L)] Number features visualized. features added set NULL. aggregate [logical(1L)] Flag whether feature importance aggregated feature. Otherwise visualized per base learner.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotFeatureImportance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize the feature importance — plotFeatureImportance","text":"ggplot object containing graphic.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotFeatureImportance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize the feature importance — plotFeatureImportance","text":"","code":"cboost = boostSplines(data = iris, target = \"Sepal.Length\", loss = LossQuadratic$new()) #>   1/100   risk = 0.31  time = 0    #>   2/100   risk = 0.29  time = 89    #>   4/100   risk = 0.25  time = 214    #>   6/100   risk = 0.21  time = 334    #>   8/100   risk = 0.18  time = 451    #>  10/100   risk = 0.16  time = 569    #>  12/100   risk = 0.14  time = 688    #>  14/100   risk = 0.13  time = 805    #>  16/100   risk = 0.11  time = 920    #>  18/100   risk = 0.1  time = 1036    #>  20/100   risk = 0.095  time = 1153    #>  22/100   risk = 0.088  time = 1269    #>  24/100   risk = 0.083  time = 1384    #>  26/100   risk = 0.078  time = 1499    #>  28/100   risk = 0.074  time = 1615    #>  30/100   risk = 0.071  time = 1733    #>  32/100   risk = 0.069  time = 1849    #>  34/100   risk = 0.067  time = 1966    #>  36/100   risk = 0.065  time = 2096    #>  38/100   risk = 0.063  time = 2214    #>  40/100   risk = 0.062  time = 2332    #>  42/100   risk = 0.061  time = 2451    #>  44/100   risk = 0.06  time = 2569    #>  46/100   risk = 0.059  time = 2687    #>  48/100   risk = 0.058  time = 2804    #>  50/100   risk = 0.057  time = 2920    #>  52/100   risk = 0.056  time = 3037    #>  54/100   risk = 0.056  time = 3153    #>  56/100   risk = 0.055  time = 3272    #>  58/100   risk = 0.054  time = 3389    #>  60/100   risk = 0.054  time = 3506    #>  62/100   risk = 0.053  time = 3624    #>  64/100   risk = 0.052  time = 3742    #>  66/100   risk = 0.052  time = 3862    #>  68/100   risk = 0.051  time = 3982    #>  70/100   risk = 0.051  time = 4101    #>  72/100   risk = 0.051  time = 4219    #>  74/100   risk = 0.05  time = 4337    #>  76/100   risk = 0.05  time = 4456    #>  78/100   risk = 0.049  time = 4574    #>  80/100   risk = 0.049  time = 4692    #>  82/100   risk = 0.049  time = 4809    #>  84/100   risk = 0.048  time = 4929    #>  86/100   risk = 0.048  time = 5048    #>  88/100   risk = 0.048  time = 5168    #>  90/100   risk = 0.048  time = 5291    #>  92/100   risk = 0.047  time = 5411    #>  94/100   risk = 0.047  time = 5531    #>  96/100   risk = 0.047  time = 5651    #>  98/100   risk = 0.047  time = 5771    #> 100/100   risk = 0.046  time = 5889    #>  #>  #> Train 100 iterations in 0 Seconds. #> Final risk based on the train set: 0.046 #>  plotFeatureImportance(cboost)  plotFeatureImportance(cboost, num_feats = 2)  plotFeatureImportance(cboost, num_feats = 2, aggregate = FALSE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotIndividualContribution.html","id":null,"dir":"Reference","previous_headings":"","what":"Decompose the predicted value based on the given features — plotIndividualContribution","title":"Decompose the predicted value based on the given features — plotIndividualContribution","text":"function visualizes contribution feature regarding predicted value. default, multiple base learners defined one feature aggregated. want show contribution single base learner, set aggregate = FALSE.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotIndividualContribution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Decompose the predicted value based on the given features — plotIndividualContribution","text":"","code":"plotIndividualContribution(   cboost,   newdata,   aggregate = TRUE,   colbreaks = c(-Inf, 0, Inf),   collabels = c(\"negative\", \"positive\"),   nround = 2L,   offset = TRUE )"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotIndividualContribution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Decompose the predicted value based on the given features — plotIndividualContribution","text":"cboost [Compboost class] trained Compboost object. newdata [data.frame] Data frame containing exactly one row holding new observations. aggregate [logical(1L)] Number colored base learners added legend. colbreaks [numeric()] Breaks visualize/highlight different predicted values. Default creates different colors positive negative score values. set NULL coloring applied. collabels [character(length(colbreaks) - 1)] Labels color breaks. set NULL intervals used labels. nround [integer(1L)] Digit passed round labels (default nround = 2L). offset [logical(1L)] Flag indicate whether offset added figure .","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotIndividualContribution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Decompose the predicted value based on the given features — plotIndividualContribution","text":"ggplot object containing graphic.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotIndividualContribution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Decompose the predicted value based on the given features — plotIndividualContribution","text":"","code":"dat = mtcars fnum = c(\"cyl\", \"disp\", \"hp\", \"drat\", \"wt\", \"qsec\") fcat = c(\"vs\", \"am\", \"gear\", \"carb\") for (fn in fcat) dat[[fn]] = as.factor(dat[[fn]])  cboost = Compboost$new(data = dat, target = \"mpg\",   loss = LossQuadratic$new())  for (fn in fnum) cboost$addComponents(fn, df = 3) for (fn in fcat) cboost$addBaselearner(fn, \"ridge\", BaselearnerCategoricalRidge) cboost$train(500L) #>   1/500   risk = 16   #>  12/500   risk = 7.9   #>  24/500   risk = 4.6   #>  36/500   risk = 3.4   #>  48/500   risk = 3   #>  60/500   risk = 2.7   #>  72/500   risk = 2.6   #>  84/500   risk = 2.6   #>  96/500   risk = 2.5   #> 108/500   risk = 2.5   #> 120/500   risk = 2.4   #> 132/500   risk = 2.4   #> 144/500   risk = 2.4   #> 156/500   risk = 2.3   #> 168/500   risk = 2.3   #> 180/500   risk = 2.3   #> 192/500   risk = 2.3   #> 204/500   risk = 2.3   #> 216/500   risk = 2.2   #> 228/500   risk = 2.2   #> 240/500   risk = 2.2   #> 252/500   risk = 2.2   #> 264/500   risk = 2.2   #> 276/500   risk = 2.2   #> 288/500   risk = 2.1   #> 300/500   risk = 2.1   #> 312/500   risk = 2.1   #> 324/500   risk = 2.1   #> 336/500   risk = 2.1   #> 348/500   risk = 2.1   #> 360/500   risk = 2.1   #> 372/500   risk = 2.1   #> 384/500   risk = 2.1   #> 396/500   risk = 2   #> 408/500   risk = 2   #> 420/500   risk = 2   #> 432/500   risk = 2   #> 444/500   risk = 2   #> 456/500   risk = 2   #> 468/500   risk = 2   #> 480/500   risk = 2   #> 492/500   risk = 2   #>  #>  #> Train 500 iterations in 0 Seconds. #> Final risk based on the train set: 2 #>  cbreaks = c(-Inf, -0.1, 0.1, Inf) clabs   = c(\"bad\", \"middle\", \"good\") plotIndividualContribution(cboost, dat[10, ], colbreaks = cbreaks,   collabels = clabs)  plotIndividualContribution(cboost, dat[10, ], offset = FALSE,   colbreaks = cbreaks, collabels = clabs)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotPEUni.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize partial effect of a feature — plotPEUni","title":"Visualize partial effect of a feature — plotPEUni","text":"function visualizes contribution specific feature overall prediction score. multiple base learner features included, added graphic well aggregated contribution. difference plotBaselearner potentially multiple base learners based `feat` aggregated visualized plotBaselearner visualizes contribution one specific base learner. function also automatically decides whether given feature numeric categorical chooses appropriate technique (lines numeric horizontal lines categorical).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotPEUni.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize partial effect of a feature — plotPEUni","text":"","code":"plotPEUni(cboost, feat, npoints = 100L, individual = TRUE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotPEUni.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize partial effect of a feature — plotPEUni","text":"cboost [Compboost class] trained Compboost object. feat [character(1L)] Name feature. npoints [integer(1L)] Number points predicted lines (applies numerical features). individual [logical(1L)] Flag whether individual base learners added graphic .","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotPEUni.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize partial effect of a feature — plotPEUni","text":"ggplot object containing graphic.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotPEUni.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize partial effect of a feature — plotPEUni","text":"","code":"cboost = Compboost$new(data = iris, target = \"Petal.Length\",   loss = LossQuadratic$new()) cboost$addComponents(\"Sepal.Width\") cboost$train(500L) #>   1/500   risk = 1.5   #>  12/500   risk = 1.3   #>  24/500   risk = 1.2   #>  36/500   risk = 1.2   #>  48/500   risk = 1.1   #>  60/500   risk = 1.1   #>  72/500   risk = 1.1   #>  84/500   risk = 1.1   #>  96/500   risk = 1.1   #> 108/500   risk = 1.1   #> 120/500   risk = 1.1   #> 132/500   risk = 1.1   #> 144/500   risk = 1.1   #> 156/500   risk = 1.1   #> 168/500   risk = 1.1   #> 180/500   risk = 1.1   #> 192/500   risk = 1   #> 204/500   risk = 1   #> 216/500   risk = 1   #> 228/500   risk = 1   #> 240/500   risk = 1   #> 252/500   risk = 1   #> 264/500   risk = 1   #> 276/500   risk = 1   #> 288/500   risk = 1   #> 300/500   risk = 1   #> 312/500   risk = 1   #> 324/500   risk = 1   #> 336/500   risk = 1   #> 348/500   risk = 1   #> 360/500   risk = 1   #> 372/500   risk = 1   #> 384/500   risk = 1   #> 396/500   risk = 1   #> 408/500   risk = 1   #> 420/500   risk = 1   #> 432/500   risk = 1   #> 444/500   risk = 1   #> 456/500   risk = 1   #> 468/500   risk = 1   #> 480/500   risk = 1   #> 492/500   risk = 1   #>  #>  #> Train 500 iterations in 0 Seconds. #> Final risk based on the train set: 1 #>  plotPEUni(cboost, \"Sepal.Width\")"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotRisk.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize the risk — plotRisk","title":"Visualize the risk — plotRisk","text":"function visualizes risk training. validation data given, train risk plotted validation risk.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotRisk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize the risk — plotRisk","text":"","code":"plotRisk(cboost)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotRisk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize the risk — plotRisk","text":"cboost [Compboost class] trained Compboost object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotRisk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize the risk — plotRisk","text":"ggplot object containing graphic.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotRisk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize the risk — plotRisk","text":"","code":"cboost_no_valdat = boostSplines(data = iris, target = \"Sepal.Length\",   loss = LossQuadratic$new()) #>   1/100   risk = 0.31  time = 0    #>   2/100   risk = 0.29  time = 86    #>   4/100   risk = 0.25  time = 207    #>   6/100   risk = 0.21  time = 326    #>   8/100   risk = 0.18  time = 443    #>  10/100   risk = 0.16  time = 562    #>  12/100   risk = 0.14  time = 696    #>  14/100   risk = 0.13  time = 811    #>  16/100   risk = 0.11  time = 926    #>  18/100   risk = 0.1  time = 1048    #>  20/100   risk = 0.095  time = 1165    #>  22/100   risk = 0.088  time = 1280    #>  24/100   risk = 0.083  time = 1397    #>  26/100   risk = 0.078  time = 1513    #>  28/100   risk = 0.074  time = 1630    #>  30/100   risk = 0.071  time = 1747    #>  32/100   risk = 0.069  time = 1866    #>  34/100   risk = 0.067  time = 1983    #>  36/100   risk = 0.065  time = 2101    #>  38/100   risk = 0.063  time = 2221    #>  40/100   risk = 0.062  time = 2337    #>  42/100   risk = 0.061  time = 2454    #>  44/100   risk = 0.06  time = 2569    #>  46/100   risk = 0.059  time = 2685    #>  48/100   risk = 0.058  time = 2798    #>  50/100   risk = 0.057  time = 2915    #>  52/100   risk = 0.056  time = 3030    #>  54/100   risk = 0.056  time = 3147    #>  56/100   risk = 0.055  time = 3264    #>  58/100   risk = 0.054  time = 3381    #>  60/100   risk = 0.054  time = 3499    #>  62/100   risk = 0.053  time = 3616    #>  64/100   risk = 0.052  time = 3736    #>  66/100   risk = 0.052  time = 3857    #>  68/100   risk = 0.051  time = 3976    #>  70/100   risk = 0.051  time = 4095    #>  72/100   risk = 0.051  time = 4212    #>  74/100   risk = 0.05  time = 4329    #>  76/100   risk = 0.05  time = 4446    #>  78/100   risk = 0.049  time = 4562    #>  80/100   risk = 0.049  time = 4686    #>  82/100   risk = 0.049  time = 4802    #>  84/100   risk = 0.048  time = 4918    #>  86/100   risk = 0.048  time = 5035    #>  88/100   risk = 0.048  time = 5151    #>  90/100   risk = 0.048  time = 5268    #>  92/100   risk = 0.047  time = 5383    #>  94/100   risk = 0.047  time = 5497    #>  96/100   risk = 0.047  time = 5614    #>  98/100   risk = 0.047  time = 5734    #> 100/100   risk = 0.046  time = 5853    #>  #>  #> Train 100 iterations in 0 Seconds. #> Final risk based on the train set: 0.046 #>  plotRisk(cboost_no_valdat)   cboost_valdat = boostSplines(data = iris, target = \"Sepal.Length\",   loss = LossQuadratic$new(), oob_fraction = 0.3) #>   1/100   risk = 0.32  oob_risk = 0.28   time = 0    #>   2/100   risk = 0.3  oob_risk = 0.26   time = 73    #>   4/100   risk = 0.25  oob_risk = 0.23   time = 170    #>   6/100   risk = 0.21  oob_risk = 0.2   time = 266    #>   8/100   risk = 0.18  oob_risk = 0.18   time = 358    #>  10/100   risk = 0.16  oob_risk = 0.16   time = 454    #>  12/100   risk = 0.14  oob_risk = 0.15   time = 556    #>  14/100   risk = 0.12  oob_risk = 0.13   time = 646    #>  16/100   risk = 0.11  oob_risk = 0.12   time = 737    #>  18/100   risk = 0.1  oob_risk = 0.12   time = 830    #>  20/100   risk = 0.091  oob_risk = 0.11   time = 922    #>  22/100   risk = 0.084  oob_risk = 0.11   time = 1012    #>  24/100   risk = 0.078  oob_risk = 0.1   time = 1103    #>  26/100   risk = 0.073  oob_risk = 0.099   time = 1194    #>  28/100   risk = 0.069  oob_risk = 0.096   time = 1285    #>  30/100   risk = 0.066  oob_risk = 0.094   time = 1402    #>  32/100   risk = 0.063  oob_risk = 0.093   time = 1498    #>  34/100   risk = 0.061  oob_risk = 0.091   time = 1593    #>  36/100   risk = 0.059  oob_risk = 0.09   time = 1688    #>  38/100   risk = 0.057  oob_risk = 0.09   time = 1805    #>  40/100   risk = 0.056  oob_risk = 0.089   time = 1900    #>  42/100   risk = 0.055  oob_risk = 0.088   time = 1993    #>  44/100   risk = 0.053  oob_risk = 0.088   time = 2086    #>  46/100   risk = 0.052  oob_risk = 0.087   time = 2181    #>  48/100   risk = 0.051  oob_risk = 0.087   time = 2273    #>  50/100   risk = 0.05  oob_risk = 0.086   time = 2367    #>  52/100   risk = 0.049  oob_risk = 0.086   time = 2459    #>  54/100   risk = 0.048  oob_risk = 0.086   time = 2551    #>  56/100   risk = 0.048  oob_risk = 0.085   time = 2645    #>  58/100   risk = 0.047  oob_risk = 0.085   time = 2739    #>  60/100   risk = 0.046  oob_risk = 0.085   time = 2833    #>  62/100   risk = 0.046  oob_risk = 0.085   time = 2929    #>  64/100   risk = 0.045  oob_risk = 0.084   time = 3024    #>  66/100   risk = 0.045  oob_risk = 0.084   time = 3118    #>  68/100   risk = 0.044  oob_risk = 0.084   time = 3211    #>  70/100   risk = 0.044  oob_risk = 0.084   time = 3307    #>  72/100   risk = 0.043  oob_risk = 0.084   time = 3401    #>  74/100   risk = 0.043  oob_risk = 0.084   time = 3496    #>  76/100   risk = 0.042  oob_risk = 0.083   time = 3591    #>  78/100   risk = 0.042  oob_risk = 0.083   time = 3685    #>  80/100   risk = 0.042  oob_risk = 0.083   time = 3778    #>  82/100   risk = 0.041  oob_risk = 0.083   time = 3872    #>  84/100   risk = 0.041  oob_risk = 0.083   time = 3967    #>  86/100   risk = 0.041  oob_risk = 0.083   time = 4062    #>  88/100   risk = 0.041  oob_risk = 0.083   time = 4158    #>  90/100   risk = 0.04  oob_risk = 0.083   time = 4253    #>  92/100   risk = 0.04  oob_risk = 0.083   time = 4347    #>  94/100   risk = 0.04  oob_risk = 0.083   time = 4441    #>  96/100   risk = 0.04  oob_risk = 0.083   time = 4549    #>  98/100   risk = 0.039  oob_risk = 0.082   time = 4647    #> 100/100   risk = 0.039  oob_risk = 0.082   time = 4743    #>  #>  #> Train 100 iterations in 0 Seconds. #> Final risk based on the train set: 0.039 #>  plotRisk(cboost_valdat)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotTensor.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize bivariate tensor products — plotTensor","title":"Visualize bivariate tensor products — plotTensor","text":"function visualizes contribution bivariate tensor product.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotTensor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize bivariate tensor products — plotTensor","text":"","code":"plotTensor(cboost, tname, npoints = 100L, nbins = 15L)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotTensor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize bivariate tensor products — plotTensor","text":"cboost [Compboost class] trained Compboost object. tname [character(2L)] Name tensor base learner. npoints [integer(1L)] Number grid points per numerical feature. Note: two numerical features overall number grid points npoints^2. numerical categorical feature npoints * ncat ncat number categories. two categorical features ncat^2 grid points drawn. nbins [logical(1L)] Number bins surface. applies case two numerical features. smooth surface drawn nbins = NULL.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotTensor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize bivariate tensor products — plotTensor","text":"ggplot object containing graphic.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotTensor.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize bivariate tensor products — plotTensor","text":"","code":"cboost = Compboost$new(data = iris, target = \"Petal.Length\",       loss = LossQuadratic$new())  cboost$addBaselearner(\"Sepal.Width\", \"spline\", BaselearnerPSpline, df = 4) cboost$addBaselearner(\"Sepal.Length\", \"spline\", BaselearnerPSpline, df = 4) cboost$addBaselearner(\"Species\", \"ridge\", BaselearnerCategoricalRidge) cboost$addTensor(\"Sepal.Width\", \"Sepal.Length\", df1 = 4, df2 = 4) cboost$addTensor(\"Sepal.Width\", \"Species\", df1 = 4, df2 = 2)  cboost$train(1000L) #>    1/1000   risk = 1.4   #>   25/1000   risk = 0.18   #>   50/1000   risk = 0.054   #>   75/1000   risk = 0.041   #>  100/1000   risk = 0.039   #>  125/1000   risk = 0.038   #>  150/1000   risk = 0.037   #>  175/1000   risk = 0.036   #>  200/1000   risk = 0.036   #>  225/1000   risk = 0.035   #>  250/1000   risk = 0.035   #>  275/1000   risk = 0.035   #>  300/1000   risk = 0.034   #>  325/1000   risk = 0.034   #>  350/1000   risk = 0.034   #>  375/1000   risk = 0.033   #>  400/1000   risk = 0.033   #>  425/1000   risk = 0.033   #>  450/1000   risk = 0.033   #>  475/1000   risk = 0.032   #>  500/1000   risk = 0.032   #>  525/1000   risk = 0.032   #>  550/1000   risk = 0.032   #>  575/1000   risk = 0.032   #>  600/1000   risk = 0.031   #>  625/1000   risk = 0.031   #>  650/1000   risk = 0.031   #>  675/1000   risk = 0.031   #>  700/1000   risk = 0.031   #>  725/1000   risk = 0.031   #>  750/1000   risk = 0.031   #>  775/1000   risk = 0.031   #>  800/1000   risk = 0.03   #>  825/1000   risk = 0.03   #>  850/1000   risk = 0.03   #>  875/1000   risk = 0.03   #>  900/1000   risk = 0.03   #>  925/1000   risk = 0.03   #>  950/1000   risk = 0.03   #>  975/1000   risk = 0.03   #> 1000/1000   risk = 0.03   #>  #>  #> Train 1000 iterations in 0 Seconds. #> Final risk based on the train set: 0.03 #>   plotTensor(cboost, \"Sepal.Width_Species_tensor\")  plotTensor(cboost, \"Sepal.Width_Sepal.Length_tensor\")  plotTensor(cboost, \"Sepal.Width_Sepal.Length_tensor\", nbins = NULL)"}]
