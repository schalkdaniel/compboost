[{"path":[]},{"path":"https://schalkdaniel.github.io/compboost/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"interest fostering open welcoming environment, contributors maintainers pledge making participation project community harassment-free experience everyone, regardless age, body size, disability, ethnicity, gender identity expression, level experience, nationality, personal appearance, race, religion, sexual identity orientation.","code":""},{"path":"https://schalkdaniel.github.io/compboost/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes creating positive environment include: Using welcoming inclusive language respectful differing viewpoints experiences Gracefully accepting constructive criticism Focusing best community Showing empathy towards community members Examples unacceptable behavior participants include: use sexualized language imagery unwelcome sexual attention advances Trolling, insulting/derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical electronic address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://schalkdaniel.github.io/compboost/CODE_OF_CONDUCT.html","id":"our-responsibilities","dir":"","previous_headings":"","what":"Our Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Project maintainers responsible clarifying standards acceptable behavior expected take appropriate fair corrective action response instances unacceptable behavior. Project maintainers right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, ban temporarily permanently contributor behaviors deem inappropriate, threatening, offensive, harmful.","code":""},{"path":"https://schalkdaniel.github.io/compboost/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within project spaces public spaces individual representing project community. Examples representing project community include using official project e-mail address, posting via official social media account, acting appointed representative online offline event. Representation project may defined clarified project maintainers.","code":""},{"path":"https://schalkdaniel.github.io/compboost/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported contacting project team contact@danielschalk.com. project team review investigate complaints, respond way deems appropriate circumstances. project team obligated maintain confidentiality regard reporter incident. details specific enforcement policies may posted separately. Project maintainers follow enforce Code Conduct good faith may face temporary permanent repercussions determined members project’s leadership.","code":""},{"path":"https://schalkdaniel.github.io/compboost/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 1.4, available http://contributor-covenant.org/version/1/4","code":""},{"path":"https://schalkdaniel.github.io/compboost/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to compboost development","title":"Contributing to compboost development","text":", authors compboost R package, use guide used contributing development popular ggplot2 R package. document simply formal re-statement fact. goal guide help get contributing compboost quickly possible. guide divided two main pieces: Filing bug report feature request issue. Suggesting change via pull request.","code":""},{"path":"https://schalkdaniel.github.io/compboost/CONTRIBUTING.html","id":"issues","dir":"","previous_headings":"","what":"Issues","title":"Contributing to compboost development","text":"filing issue, important thing include minimal reproducible example can quickly verify problem, figure fix . three things need include make example reproducible: required packages, data, code. Packages loaded top script, ’s easy see ones example needs. easiest way include data use dput() generate R code recreate . example, recreate mtcars dataset R, ’d perform following steps: Run dput(mtcars) R Copy output reproducible script, type mtcars <- paste. even better can create data.frame() just handful rows columns still illustrates problem. Spend little bit time ensuring code easy others read: make sure ’ve used spaces variable names concise, informative use comments indicate problem lies best remove everything related problem. shorter code , easier understand. can check actually made reproducible example starting fresh R session pasting script . (Unless ’ve specifically asked , please don’t include output sessionInfo().)","code":""},{"path":"https://schalkdaniel.github.io/compboost/CONTRIBUTING.html","id":"pull-requests","dir":"","previous_headings":"","what":"Pull requests","title":"Contributing to compboost development","text":"contribute change compboost, follow steps: Create branch git make changes. Push branch github issue pull request (PR). Discuss pull request. Iterate either accept PR decide ’s good fit compboost. steps described detail . might feel overwhelming first time get set , gets easier practice. ’re familiar git GitHub, please start reading http://r-pkgs..co.nz/git.html Pull requests evaluated checklist: Motivation. pull request clearly concisely motivates need change. Plesae describe problem PR addresses show pull request solves concisely possible. Also include motivation NEWS new release compboost comes ’s easy users see ’s changed. Add item top file use markdown formatting. news item end (@yourGithubUsername, #the_issue_number). related changes. submit pull request, please check make sure haven’t accidentally included unrelated changes. make harder see exactly ’s changed, evaluate unexpected side effects. PR corresponds git branch, expect submit multiple changes make sure create multiple branches. multiple changes depend , start first one don’t submit others first one processed. ’re adding new parameters new function, ’ll also need document roxygen. Make sure re-run devtools::document() code submitting. seems like lot work don’t worry pull request isn’t perfect. ’s learning process. pull request process, unless ’ve submitted past ’s unlikely pull request accepted . Please don’t submit pull requests change existing behaviour. Instead, think can add new feature minimally invasive way.","code":""},{"path":"https://schalkdaniel.github.io/compboost/CONTRIBUTORS.html","id":null,"dir":"","previous_headings":"","what":"Contributors of compboost","title":"Contributors of compboost","text":"Committers people made substantial contribution project granted write access project. Daniel creator maintainer project. Janek gives feedback/guidance package design structure implements initial R API. Bernd gives feedback/guidance package design structure initiated project.","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/adv-extending-losses.html","id":"before-starting","dir":"Articles","previous_headings":"","what":"Before starting","title":"Extending compboost with losses","text":"Read use-case site get know define Compboost object using R6 interface.","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/adv-extending-losses.html","id":"what-is-needed","dir":"Articles","previous_headings":"","what":"What is Needed","title":"Extending compboost with losses","text":"compboost designed provide component-wise boosting framework maximal flexibility. vignette gives overview define custom losses R well C++ without recompiling whole package. custom losses can used training model /logging mechanisms. loss function training model boosting required differentiable. Hence, need define loss function gradient. , boosting initialized loss optimal constant. capture , define loss optimal constant function response vector. three components, quite easy define custom losses. showcase, rebuilding two different loss functions: quadratic loss easy example C++ Poisson loss counting data sophisticated loss example R","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/adv-extending-losses.html","id":"define-a-new-loss-in-r","dir":"Articles","previous_headings":"","what":"Define a new loss in R","title":"Extending compboost with losses","text":"example using VonBort dataset provided package vcd: “Data von Bortkiewicz (1898), given Andrews & Herzberg (1985), number deaths horse mule kicks 14 corps Prussian army.” like model deaths using Poisson regression boosting. means define proper loss function, gradient, constant initialization. scheme loss, gradient, constant initialization specify function following form: loss: function (truth, response) gradient: function (truth, response) constant initializer: function (truth)","code":"data(VonBort, package = \"vcd\")"},{"path":"https://schalkdaniel.github.io/compboost/articles/adv-extending-losses.html","id":"the-loss-function","dir":"Articles","previous_headings":"Define a new loss in R","what":"The loss function","title":"Extending compboost with losses","text":"\\[L(y,f) = -\\log\\left( \\exp(f)^y \\exp(\\exp(f)) \\right) - \\log(y!)\\]","code":"lossPoisson = function (truth, response) {   return(-log(exp(response)^truth * exp(-exp(response))) - gamma(truth + 1)) }"},{"path":"https://schalkdaniel.github.io/compboost/articles/adv-extending-losses.html","id":"the-gradient-of-the-loss-function","dir":"Articles","previous_headings":"Define a new loss in R","what":"The gradient of the loss function","title":"Extending compboost with losses","text":"\\[\\frac{\\partial}{\\partial f} L(y,f) = \\exp(f) - y\\]","code":"gradPoisson = function (truth, response) {   return(exp(response) - truth) }"},{"path":"https://schalkdaniel.github.io/compboost/articles/adv-extending-losses.html","id":"the-constant-initialization","dir":"Articles","previous_headings":"Define a new loss in R","what":"The constant initialization","title":"Extending compboost with losses","text":"\\[\\mathsf{arg min}_{c\\\\mathbb{R}} \\sum_{= 1}^n L\\left(y^{()}, c\\right) = \\log(\\bar{y})\\]","code":"constInitPoisson = function (truth) {   return(log(mean(truth))) }"},{"path":"https://schalkdaniel.github.io/compboost/articles/adv-extending-losses.html","id":"define-the-loss","dir":"Articles","previous_headings":"Define a new loss in R","what":"Define the loss","title":"Extending compboost with losses","text":"Finally, three components allows define LossCustom object:","code":"# Define custom loss: my_poisson_loss = LossCustom$new(lossPoisson, gradPoisson, constInitPoisson)"},{"path":"https://schalkdaniel.github.io/compboost/articles/adv-extending-losses.html","id":"train-a-model","dir":"Articles","previous_headings":"Define a new loss in R","what":"Train a model","title":"Extending compboost with losses","text":"loss object can used task requires loss object:","code":"cboost = Compboost$new(VonBort, \"deaths\", loss = my_poisson_loss) cboost$addBaselearner(\"year\", \"spline\", BaselearnerPSpline) cboost$train(500, trace = 0)"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-early-stopping.html","id":"before-starting","dir":"Articles","previous_headings":"","what":"Before Starting","title":"Early Stopping","text":"Read use-case get know define Compboost object using R6 interface","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-early-stopping.html","id":"data-titanic-passenger-survival-data-set","dir":"Articles","previous_headings":"","what":"Data: Titanic Passenger Survival Data Set","title":"Early Stopping","text":"use titanic dataset binary classification Survived. First store train test data two data frames remove rows contains missing values (NAs): later stopping split dataset train test:","code":"# Store train and test data: df = na.omit(titanic::titanic_train) df$Survived = factor(df$Survived, labels = c(\"no\", \"yes\")) set.seed(123) idx_train = sample(seq_len(nrow(df)), size = nrow(df) * 0.8) idx_test = setdiff(seq_len(nrow(df)), idx_train)"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-early-stopping.html","id":"defining-the-model","dir":"Articles","previous_headings":"","what":"Defining the Model","title":"Early Stopping","text":"define model use-case just train index without specifying --bag fraction:","code":"cboost = Compboost$new(data = df[idx_train, ], target = \"Survived\")  cboost$addBaselearner(\"Age\", \"spline\", BaselearnerPSpline) cboost$addBaselearner(\"Fare\", \"spline\", BaselearnerPSpline) cboost$addBaselearner(\"Sex\", \"ridge\", BaselearnerCategoricalRidge)"},{"path":[]},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-early-stopping.html","id":"how-does-it-work","dir":"Articles","previous_headings":"Early Stopping in Compboost","what":"How does it work?","title":"Early Stopping","text":"early stopping compboost done using logger objects. Logger executed iteration stores class dependent data runtime risk. Additionally, logger can declared stopper setting use_as_stopper = TRUE. declaring logger stopper, used stop algorithm logger-specific criteria reached. example, LoggerTime stop algorithm pre-defined runtime reached.","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-early-stopping.html","id":"example-with-runtime-stopping","dir":"Articles","previous_headings":"Early Stopping in Compboost","what":"Example with runtime stopping","title":"Early Stopping","text":"Now time define logger track runtime. mentioned , set use_as_stopper = TRUE. setting max_time define long want train model, 50000 microseconds: can see, fittings stopped early 500 train full 2000 iterations. logger data can accessed calling $getLoggerData():","code":"cboost$addLogger(logger = LoggerTime, use_as_stopper = TRUE, logger_id = \"time\",   max_time = 50000, time_unit = \"microseconds\")  cboost$train(2000, trace = 250) #>    1/2000   risk = 0.67  time = 0    #>  250/2000   risk = 0.5  time = 23478    #>  500/2000   risk = 0.48  time = 50001    #>  #>  #> Train 500 iterations in 0 Seconds. #> Final risk based on the train set: 0.48 cboost #>  #>  #> Component-Wise Gradient Boosting #>  #> Target variable: Survived #> Number of base-learners: 3 #> Learning rate: 0.05 #> Iterations: 500 #>  #> Offset: 0.423 #>  #> LossBinomial: L(y,x) = log(1 + exp(-2yf(x)) tail(cboost$getLoggerData()) #>     _iterations  time baselearner train_risk #> 496         495 49421  Age_spline  0.4820859 #> 497         496 49526   Sex_ridge  0.4820397 #> 498         497 49636  Age_spline  0.4820059 #> 499         498 49748 Fare_spline  0.4819657 #> 500         499 49886  Age_spline  0.4819323 #> 501         500 50001 Fare_spline  0.4818926"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-early-stopping.html","id":"loss-based-early-stopping","dir":"Articles","previous_headings":"","what":"Loss-Based Early Stopping","title":"Early Stopping","text":"machine learning, often like stop best model performance. need either tuning early stopping determine good number iterations \\(m\\). well-known procedure log --bag (oob) behavior model stop model performance starts get worse. required parameters logger loss \\(L\\) used stopping: \\[\\mathcal{R}_{\\text{emp}}^{[m]} = \\frac{1}{n}\\sum_{=1}^n L\\left(y^{()}, f^{[m]}(x^{()})\\right)\\] percentage performance increase lower boundary increase: \\[\\text{err}^{[m]} = \\frac{\\mathcal{R}_{\\text{emp}}^{[m- 1]} - \\mathcal{R}_{\\text{emp}}^{[m]}}{\\mathcal{R}_{\\text{emp}}^{[m - 1]}}\\]","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-early-stopping.html","id":"define-the-risk-logger","dir":"Articles","previous_headings":"Loss-Based Early Stopping","what":"Define the risk logger","title":"Early Stopping","text":"Since interested oob behavior, necessary prepare oob data response compboost. Therefore, possible use $prepareResponse() $prepareData() member functions create suitable objects: objects can add oob risk logger, declare stopper, train model: Note: use eps_for_break = 0 hard constrain stop training oob risk starts increase. Taking look logger data tells us stopped exactly first five differences bigger zero (oob risk iterations bigger previous ones):  Taking look 2000 iterations shows stopped quite good:  Note: can happen model’s oob behavior increases locally iterations starts decrease . capture , need “patience” parameter waits , let’s say, 5 iterations stops algorithm improvement 5 iterations smaller criteria. Setting parameter one can lead unstable results:","code":"oob_response = cboost$prepareResponse(df$Survived[idx_test]) oob_data = cboost$prepareData(df[idx_test,]) cboost$addLogger(logger = LoggerOobRisk, use_as_stopper = TRUE, logger_id = \"oob\",   used_loss = LossBinomial$new(), eps_for_break = 0, patience = 5, oob_data = oob_data,   oob_response = oob_response)  cboost$train(2000, trace = 250) #>    1/2000   risk = 0.67  oob = 0.68    #>  250/2000   risk = 0.5  oob = 0.49    #>  500/2000   risk = 0.48  oob = 0.48    #>  #>  #> Train 543 iterations in 0 Seconds. #> Final risk based on the train set: 0.48 tail(cboost$getLoggerData(), n = 10) #>     _iterations       oob baselearner train_risk #> 535         534 0.4784209   Sex_ridge  0.4806872 #> 536         535 0.4784327  Age_spline  0.4806586 #> 537         536 0.4784332 Fare_spline  0.4806242 #> 538         537 0.4784450  Age_spline  0.4805959 #> 539         538 0.4783287   Sex_ridge  0.4805564 #> 540         539 0.4783405  Age_spline  0.4805284 #> 541         540 0.4783415 Fare_spline  0.4804943 #> 542         541 0.4783534  Age_spline  0.4804666 #> 543         542 0.4783547 Fare_spline  0.4804330 #> 544         543 0.4783665  Age_spline  0.4804054 diff(tail(cboost$getLoggerData()$oob, n = 10)) #> [1]  1.180302e-05  5.400260e-07  1.179882e-05 -1.163699e-04  1.187324e-05 #> [6]  9.532968e-07  1.186647e-05  1.321357e-06  1.185917e-05 library(ggplot2)  ggplot(data = cboost$getLoggerData(), aes(x = `_iterations`, y = oob)) +   geom_line() +   xlab(\"Iteration\") +   ylab(\"Empirical Risk\") #> Warning: Removed 1 row containing missing values (`geom_line()`). cboost$train(2000, trace = 0) #>  #> You have already trained 543 iterations. #> Train 1457 additional iterations.  ggplot(data = cboost$getLoggerData(), aes(x = `_iterations`, y = oob)) +   geom_line() +   xlab(\"Iteration\") +   ylab(\"Empirical Risk\") #> Warning: Removed 1 row containing missing values (`geom_line()`). df = na.omit(titanic::titanic_train) df$Survived = factor(df$Survived, labels = c(\"no\", \"yes\"))  set.seed(123) idx_train = sample(seq_len(nrow(df)), size = nrow(df) * 0.8) idx_test = setdiff(seq_len(nrow(df)), idx_train)  cboost = Compboost$new(data = df[idx_train, ], target = \"Survived\", loss = LossBinomial$new())  cboost$addBaselearner(\"Age\", \"spline\", BaselearnerPSpline) cboost$addBaselearner(\"Fare\", \"spline\", BaselearnerPSpline) cboost$addBaselearner(\"Sex\", \"ridge\", BaselearnerCategoricalRidge)  oob_response = cboost$prepareResponse(df$Survived[idx_test]) oob_data = cboost$prepareData(df[idx_test,])  cboost$addLogger(logger = LoggerOobRisk, use_as_stopper = TRUE, logger_id = \"oob\",   used_loss = LossBinomial$new(), eps_for_break = 0, patience = 1, oob_data = oob_data,   oob_response = oob_response)  cboost$train(2000, trace = 0) #> Train 320 iterations in 0 Seconds. #> Final risk based on the train set: 0.49   library(ggplot2) ggplot(data = cboost$getLoggerData(), aes(x = `_iterations`, y = oob)) +   geom_line() +   xlab(\"Iteration\") +   ylab(\"Empirical Risk\") #> Warning: Removed 1 row containing missing values (`geom_line()`)."},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-early-stopping.html","id":"further-comments-on-risk-logging","dir":"Articles","previous_headings":"Loss-Based Early Stopping","what":"Further comments on risk logging","title":"Early Stopping","text":"Since can define many logger like, possible define multiple risk logger regarding different loss functions. also possible log performance measures risk logging mechanism. covered advanced topic.","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-early-stopping.html","id":"some-remarks","dir":"Articles","previous_headings":"","what":"Some remarks","title":"Early Stopping","text":"locally (): algorithm stops first stopping criteria logger reached globally (): algorithm stops stopping criteria reached arguments ignored logger set stopper, e.g. max_time time logger logger functionality summarized ","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-mlr-learners.html","id":"regression","dir":"Articles","previous_headings":"","what":"Regression","title":"mlr3 learners","text":"task, use Boston housing task accessible via tsk(\"boston_housing\"): key regr.compboost gives regression learner: important features Compboost can controlled via parameters. example, using early stopping requires set value oob_fraction number bigger 0. Just case, learner can trained early stopping:","code":"task = tsk(\"boston_housing\") task #> <TaskRegr:boston_housing> (506 x 19): Boston Housing Prices #> * Target: medv #> * Properties: - #> * Features (18): #>   - dbl (13): age, b, cmedv, crim, dis, indus, lat, lon, lstat, nox, #>     ptratio, rm, zn #>   - int (3): rad, tax, tract #>   - fct (2): chas, town lcb = lrn(\"regr.compboost\") lcb$param_set #> <ParamSet> #>                id    class lower upper nlevels default     parents  value #>  1:   baselearner ParamFct    NA    NA       3  spline             spline #>  2:      bin_root ParamInt     0   Inf     Inf       0                    #>  3:        degree ParamInt     1   Inf     Inf       3 baselearner        #>  4:            df ParamDbl     1   Inf     Inf       5                  5 #>  5:        df_cat ParamDbl     1   Inf     Inf       2                  2 #>  6:   differences ParamInt     1   Inf     Inf       2 baselearner        #>  7:    early_stop ParamLgl    NA    NA       2   FALSE              FALSE #>  8: eps_for_break ParamDbl  -Inf   Inf     Inf       0  early_stop        #>  9:  interactions ParamUty    NA    NA     Inf                            #> 10:    iterations ParamInt     1   Inf     Inf     100                100 #> 11: learning_rate ParamDbl     0   Inf     Inf    0.05                    #> 12:          loss ParamUty    NA    NA     Inf                            #> 13:       n_knots ParamInt     1   Inf     Inf      20 baselearner        #> 14:  oob_fraction ParamDbl     0     1     Inf     0.3                    #> 15:     optimizer ParamUty    NA    NA     Inf                            #> 16:      patience ParamInt     1   Inf     Inf       5  early_stop        #> 17:       penalty ParamDbl     0   Inf     Inf       0 baselearner        #> 18:   show_output ParamLgl    NA    NA       2   FALSE              FALSE  lcb$train(task) lcb$model #>  #>  #> Component-Wise Gradient Boosting #>  #> Target variable: medv #> Number of base-learners: 18 #> Learning rate: 0.05 #> Iterations: 100 #>  #> Offset: 22.5328 #>  #> LossQuadratic: L(y,x) = 0.5 * (y - f(x))^2 lcb = lrn(\"regr.compboost\", early_stop = TRUE) lcb$train(task) #> Error in get_private(learner)$.train(task): `oob_fraction > 0` required for early stopping.  lcb = lrn(\"regr.compboost\", oob_fraction = 0.3, early_stop = TRUE) lcb$train(task) head(lcb$model$logs) #>   _iterations oob_risk time  baselearner train_risk #> 1           0       NA   NA    intercept   44.14382 #> 2           1 33.86630    0 cmedv_spline   39.85069 #> 3           2 30.59209  409 cmedv_spline   35.97613 #> 4           3 27.63793  692 cmedv_spline   32.47935 #> 5           4 24.97256  959 cmedv_spline   29.32350 #> 6           5 22.56780 1234 cmedv_spline   26.47534"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-mlr-learners.html","id":"binary-classification","dir":"Articles","previous_headings":"","what":"Binary classification","title":"mlr3 learners","text":"Binary classification works way. use spam data set demo: , usual methods fields accessible:","code":"task = tsk(\"spam\") task #> <TaskClassif:spam> (4601 x 58): HP Spam Detection #> * Target: type #> * Properties: twoclass #> * Features (57): #>   - dbl (57): address, addresses, all, business, capitalAve, #>     capitalLong, capitalTotal, charDollar, charExclamation, charHash, #>     charRoundbracket, charSemicolon, charSquarebracket, conference, #>     credit, cs, data, direct, edu, email, font, free, george, hp, hpl, #>     internet, lab, labs, mail, make, meeting, money, num000, num1999, #>     num3d, num415, num650, num85, num857, order, original, our, over, #>     parts, people, pm, project, re, receive, remove, report, table, #>     technology, telnet, will, you, your lcb = lrn(\"classif.compboost\", iterations = 500L) lcb$train(task)  lcb$predict_type = \"prob\" pred = lcb$predict(task) pred$confusion #>          truth #> response  spam nonspam #>   spam    1415      82 #>   nonspam  398    2706 pred$score(msr(\"classif.auc\")) #> classif.auc  #>   0.9597973"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-mlr-learners.html","id":"using-compboost-in-parallel","dir":"Articles","previous_headings":"","what":"Using compboost in parallel","title":"mlr3 learners","text":"parallel execution compboost controlled optimizers. mlr3, optimizers can defined construction learner. Thus, compboost run parallel, define optimizer advance use construction:","code":"lcb$timings[\"train\"] #> train  #> 5.425  lcb_2c = lrn(\"classif.compboost\", iterations = 500L, optimizer = OptimizerCoordinateDescent$new(2)) lcb_2c$train(task) lcb_2c$timings[\"train\"] #> train  #> 3.037"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-mlr-learners.html","id":"using-different-losses","dir":"Articles","previous_headings":"","what":"Using different losses","title":"mlr3 learners","text":"parallel execution, losses can defined loss parameter value construction:","code":"task = tsk(\"boston_housing\") lcb_quantiles = lrn(\"regr.compboost\", loss = LossQuantile$new(0.1)) lcb_quantiles$train(task) lcb_quantiles$predict(task) #> <PredictionRegr> for 506 observations: #>     row_ids truth response #>           1  24.0 13.94270 #>           2  21.6 13.88014 #>           3  34.7 13.76084 #> ---                        #>         504  23.9 13.94210 #>         505  22.0 13.89905 #>         506  11.9 13.62323"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-obj-refs.html","id":"response-class","dir":"Articles","previous_headings":"","what":"Response Class","title":"Reference based objects","text":"target variable represented object inherits Response class. Depending target type like different transformations internally predicted scores. instance, binary classification task score \\(\\hat{f}(x) \\\\mathbb{R}\\) transformed \\([0,1]\\) scale using logistic function: \\[ \\hat{\\pi}(x) = \\frac{1}{1 + \\exp(-\\hat{f}(x))} \\] show references work , first define ResponseBinaryClassif object. Therefore, use mtcars dataset create new binary target variable fast \\(\\text{qsec} < 17\\) slow \\(\\text{qsec} \\geq 17\\) cars create response object: access underlying representation response class (binary variable) one can use $getResponse(). initialization new response object, prediction \\(\\hat{f} \\\\mathbb{R}\\) initialized zeros. can also use response object calculate transformed predictions \\(\\hat{\\pi} \\[0,1]\\): case binary classification, can use response object calculate predictions label basis using specified threshold \\(\\): \\[ \\hat{y} = 1 \\ \\ \\text{} \\ \\ \\hat{\\pi}(x) \\geq \\] default threshold 0.5: setting threshold 0.6 observe now class predicted negative: behavior nothing references moment. fitting component-wise boosting model, predictions adjusted Compboost object. reference comes : look predictions shows difference values training. fitting process, predictions response object updated model:","code":"df = mtcars[, c(\"mpg\", \"disp\", \"hp\", \"drat\", \"wt\")] df$qsec_cat = ifelse(mtcars$qsec < 17, \"fast\", \"slow\")  obj_response = ResponseBinaryClassif$new(\"qsec_cat\", \"fast\", df$qsec_cat) obj_response #>  #> Binary classification response of target \"qsec_cat\" and threshold 0.5 #> ResponseBinaryClassifPrinter knitr::kable(head(data.frame(   target = df$qsec_cat,   target_representation = obj_response$getResponse(),   prediction_initialization = obj_response$getPrediction(),   prediction_transformed = obj_response$getPredictionTransform() ))) obj_response$getThreshold() #> [1] 0.5 head(obj_response$getPredictionResponse()) #>      [,1] #> [1,]    1 #> [2,]    1 #> [3,]    1 #> [4,]    1 #> [5,]    1 #> [6,]    1 obj_response$setThreshold(0.6) head(obj_response$getPredictionResponse()) #>      [,1] #> [1,]   -1 #> [2,]   -1 #> [3,]   -1 #> [4,]   -1 #> [5,]   -1 #> [6,]   -1 cboost = boostSplines(data = df, target = obj_response,   iterations = 2000L, trace = 500L) #>    1/2000   risk = 0.59  time = 0    #>  500/2000   risk = 0.22  time = 15004    #> 1000/2000   risk = 0.15  time = 39184    #> 1500/2000   risk = 0.12  time = 73128    #> 2000/2000   risk = 0.1  time = 116319    #>  #>  #> Train 2000 iterations in 0 Seconds. #> Final risk based on the train set: 0.1 knitr::kable(head(data.frame(   target = df$qsec_cat,   prediction = obj_response$getPrediction(),   prediction_transformed = obj_response$getPredictionTransform(),   prediction_response = obj_response$getPredictionResponse() )))"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-production-model.html","id":"store-model-without-data","dir":"Articles","previous_headings":"","what":"Store model without data","title":"Production mode","text":", just call: Note: possible use functionality requires training data storing loading object without data. example, cboost$predict() now throws error:","code":"dat = mlr3::tsk(\"sonar\")$data() cboost = boostSplines(dat, \"Class\", oob_fraction = 0.3) #>   1/100   risk = 0.68  oob_risk = 0.69   time = 0    #>   2/100   risk = 0.68  oob_risk = 0.69   time = 1209    #>   4/100   risk = 0.67  oob_risk = 0.68   time = 3291    #>   6/100   risk = 0.67  oob_risk = 0.67   time = 5364    #>   8/100   risk = 0.66  oob_risk = 0.67   time = 19677    #>  10/100   risk = 0.65  oob_risk = 0.66   time = 21994    #>  12/100   risk = 0.65  oob_risk = 0.66   time = 24202    #>  14/100   risk = 0.64  oob_risk = 0.65   time = 26556    #>  16/100   risk = 0.64  oob_risk = 0.65   time = 28718    #>  18/100   risk = 0.63  oob_risk = 0.65   time = 30754    #>  20/100   risk = 0.62  oob_risk = 0.64   time = 32809    #>  22/100   risk = 0.62  oob_risk = 0.64   time = 34850    #>  24/100   risk = 0.62  oob_risk = 0.64   time = 37090    #>  26/100   risk = 0.61  oob_risk = 0.63   time = 39053    #>  28/100   risk = 0.61  oob_risk = 0.63   time = 41351    #>  30/100   risk = 0.6  oob_risk = 0.63   time = 43546    #>  32/100   risk = 0.6  oob_risk = 0.62   time = 45581    #>  34/100   risk = 0.6  oob_risk = 0.62   time = 47836    #>  36/100   risk = 0.59  oob_risk = 0.62   time = 50620    #>  38/100   risk = 0.59  oob_risk = 0.62   time = 52533    #>  40/100   risk = 0.59  oob_risk = 0.61   time = 54542    #>  42/100   risk = 0.58  oob_risk = 0.61   time = 56823    #>  44/100   risk = 0.58  oob_risk = 0.61   time = 58893    #>  46/100   risk = 0.58  oob_risk = 0.61   time = 60876    #>  48/100   risk = 0.57  oob_risk = 0.6   time = 62879    #>  50/100   risk = 0.57  oob_risk = 0.6   time = 64809    #>  52/100   risk = 0.57  oob_risk = 0.6   time = 66926    #>  54/100   risk = 0.57  oob_risk = 0.6   time = 69090    #>  56/100   risk = 0.56  oob_risk = 0.6   time = 71319    #>  58/100   risk = 0.56  oob_risk = 0.59   time = 73513    #>  60/100   risk = 0.56  oob_risk = 0.59   time = 75597    #>  62/100   risk = 0.55  oob_risk = 0.59   time = 77925    #>  64/100   risk = 0.55  oob_risk = 0.59   time = 79912    #>  66/100   risk = 0.55  oob_risk = 0.59   time = 81834    #>  68/100   risk = 0.55  oob_risk = 0.59   time = 83898    #>  70/100   risk = 0.54  oob_risk = 0.58   time = 85918    #>  72/100   risk = 0.54  oob_risk = 0.58   time = 88202    #>  74/100   risk = 0.54  oob_risk = 0.58   time = 90243    #>  76/100   risk = 0.54  oob_risk = 0.58   time = 92231    #>  78/100   risk = 0.53  oob_risk = 0.58   time = 94277    #>  80/100   risk = 0.53  oob_risk = 0.58   time = 96362    #>  82/100   risk = 0.53  oob_risk = 0.57   time = 98613    #>  84/100   risk = 0.53  oob_risk = 0.57   time = 100617    #>  86/100   risk = 0.53  oob_risk = 0.57   time = 102732    #>  88/100   risk = 0.52  oob_risk = 0.57   time = 105049    #>  90/100   risk = 0.52  oob_risk = 0.57   time = 107243    #>  92/100   risk = 0.52  oob_risk = 0.57   time = 109439    #>  94/100   risk = 0.52  oob_risk = 0.56   time = 111508    #>  96/100   risk = 0.51  oob_risk = 0.56   time = 113612    #>  98/100   risk = 0.51  oob_risk = 0.56   time = 115700    #> 100/100   risk = 0.51  oob_risk = 0.56   time = 117812    #>  #>  #> Train 100 iterations in 0 Seconds. #> Final risk based on the train set: 0.51  file = \"cboost.json\" cboost$saveToJson(file, rm_data = TRUE)  cboost_without_data = Compboost$new(file = file)  # The data field now just contains a dummy: cboost_without_data$data #>   V1 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V2 V20 V21 V22 V23 V24 V25 V26 V27 #> 1  0   0   0   0   0   0   0   0   0   0   0  0   0   0   0   0   0   0   0   0 #>   V28 V29 V3 V30 V31 V32 V33 V34 V35 V36 V37 V38 V39 V4 V40 V41 V42 V43 V44 V45 #> 1   0   0  0   0   0   0   0   0   0   0   0   0   0  0   0   0   0   0   0   0 #>   V46 V47 V48 V49 V5 V50 V51 V52 V53 V54 V55 V56 V57 V58 V59 V6 V60 V7 V8 V9 #> 1   0   0   0   0  0   0   0   0   0   0   0   0   0   0   0  0   0  0  0  0 cboost_without_data$predict() #> Error in eval(expr, envir, enclos): Production mode is on, this does not allow prediction on training data and hence also blocks the continuation of the training. This is most likely because the training data was removed to either store memory or due to privacy reasons."},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-production-model.html","id":"functionality-of-a-data-free-model","dir":"Articles","previous_headings":"","what":"Functionality of a data free model","title":"Production mode","text":"important functions still usable:","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-production-model.html","id":"extracting-feature-importance-","dir":"Articles","previous_headings":"Functionality of a data free model","what":"Extracting feature importance.","title":"Production mode","text":"","code":"vip = cboost_without_data$calculateFeatureImportance()"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-production-model.html","id":"predict-on-new-data","dir":"Articles","previous_headings":"Functionality of a data free model","what":"Predict on new data","title":"Production mode","text":"","code":"ndat = dat[1:10, ] cboost_without_data$predict(ndat) #>              [,1] #>  [1,] -0.40271012 #>  [2,] -0.68187743 #>  [3,] -0.75957011 #>  [4,]  0.11479825 #>  [5,]  0.01493262 #>  [6,]  0.38575212 #>  [7,] -0.30511673 #>  [8,]  0.48459556 #>  [9,] -0.03764836 #> [10,] -0.68916948"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-production-model.html","id":"visualize-partial-feature-effects-","dir":"Articles","previous_headings":"Functionality of a data free model","what":"Visualize partial feature effects.","title":"Production mode","text":"","code":"library(patchwork)  # Use most important base learner: bln = vip$baselearner[1] plotBaselearner(cboost_without_data, bln) + plotPEUni(cboost_without_data, strsplit(bln, \"_\")[[1]][1])"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-production-model.html","id":"get-logger-data","dir":"Articles","previous_headings":"Functionality of a data free model","what":"Get logger data","title":"Production mode","text":"","code":"head(cboost_without_data$getLoggerData()) #>   _iterations  oob_risk time baselearner train_risk #> 1           0        NA   NA   intercept  0.6871302 #> 2           1 0.6894456    0  V12_spline  0.6832722 #> 3           2 0.6863465 1209  V12_spline  0.6795051 #> 4           3 0.6833276 2243  V12_spline  0.6758265 #> 5           4 0.6803868 3291  V12_spline  0.6722345 #> 6           5 0.6775222 4338  V12_spline  0.6687269"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-production-model.html","id":"setting-the-model-to-a-previous-iteration-","dir":"Articles","previous_headings":"Functionality of a data free model","what":"Setting the model to a previous iteration.","title":"Production mode","text":"","code":"table(cboost_without_data$getSelectedBaselearner()) #>  #> V12_spline V21_spline V36_spline V45_spline V48_spline V52_spline V55_spline  #>         47         18          4          2          6          6         10  #>  V9_spline  #>          7 cboost_without_data$predict(ndat) #>              [,1] #>  [1,] -0.40271012 #>  [2,] -0.68187743 #>  [3,] -0.75957011 #>  [4,]  0.11479825 #>  [5,]  0.01493262 #>  [6,]  0.38575212 #>  [7,] -0.30511673 #>  [8,]  0.48459556 #>  [9,] -0.03764836 #> [10,] -0.68916948  # State after 50 iteration: cboost_without_data$train(50) table(cboost_without_data$getSelectedBaselearner()) #>  #> V12_spline V21_spline  V9_spline  #>         39          7          4 cboost_without_data$predict(ndat) #>             [,1] #>  [1,] -0.2091731 #>  [2,] -0.3681303 #>  [3,] -0.7220235 #>  [4,]  0.1584036 #>  [5,]  0.2593279 #>  [6,]  0.5385189 #>  [7,] -0.2544635 #>  [8,]  0.5329150 #>  [9,] -0.1226703 #> [10,] -0.4244483"},{"path":[]},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-production-model.html","id":"size","dir":"Articles","previous_headings":"Advantages","what":"Size","title":"Production mode","text":"size model JSON file much smaller data stored.","code":"file_full = \"cboost_full.json\" cboost$saveToJson(file_full)  file.info(file)$size / 1024^2 #> [1] 1.310376 file.info(file_full)$size / 1024^2 #> [1] 3.352608"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-production-model.html","id":"loading","dir":"Articles","previous_headings":"Advantages","what":"Loading","title":"Production mode","text":"Loading model much faster (maybe striking smaller models):","code":"system.time(Compboost$new(file = file)) #>    user  system elapsed  #>    0.13    0.00    0.13 system.time(Compboost$new(file = file_full)) #>    user  system elapsed  #>   0.193   0.000   0.193"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-production-model.html","id":"privacy","dir":"Articles","previous_headings":"Advantages","what":"Privacy","title":"Production mode","text":"Raw data shared unintentionally third parties. especially striking domains works sensitive data.","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-robust-reg.html","id":"how-does-it-work","dir":"Articles","previous_headings":"","what":"How does it work?","title":"Quantile/Robust regression","text":"Predicting quantiles controlled choice loss function. Quantile regression originally motivated general additive/linear models (see ). use define following loss function: \\[ L(y, f(x)) = h|y - f(x)| \\] \\[ h = \\left\\{ \\begin{array}{ccc} 2q       & \\ \\ \\text{} \\ \\ & y - f(x) > 0 \\\\ 2(1 - q) & \\ \\ \\text{} \\ \\ & \\text{otherwise} \\end{array} \\right. \\] \\(q\\) q-quantile. Visualizing loss \\(y - f(x)\\) shows , e.g., boosting 90 % quantile punishes residuals harder residuals smaller zeros leads optimization 90 % quantile:","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-robust-reg.html","id":"simulate-data","dir":"Articles","previous_headings":"","what":"Simulate data","title":"Quantile/Robust regression","text":"show effect quantile/robust regression simulate data follows sinus curve 20 outliers:","code":"nsim = 1000 noutlier = 20 outlier_mean = 1e6  x = runif(nsim, 0, 10) y = 3 + 2 * sin(x) + rnorm(nsim, 0, 1)  outlier_idx = sample(nsim, noutlier) y[outlier_idx] = sample(x = c(-1, 1), size = noutlier, replace = TRUE) * rnorm(noutlier, outlier_mean, 1)  df = data.frame(x = x, y = y) #> Warning: Removed 20 rows containing missing values (`geom_point()`)."},{"path":[]},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-robust-reg.html","id":"boosting-the-median","dir":"Articles","previous_headings":"How to use","what":"Boosting the median","title":"Quantile/Robust regression","text":"need use LossQuantile class generator. example, boost median (50 % quantile) pass 0.5 constructor: little side note: Boosting median 50 % quantile equivalent conduct boosting absolute loss. loss_quantile90 loss object can now used define train new Compboost object: Using quantiles, absolute loss median, also known robust regression. visualize effect outliers regression, also train model QuadraticLoss:  Boosting quadratic loss quite sensitive outliers. course, example exaggerated show effect boosting median.","code":"loss_quantile50 = LossQuantile$new(0.5) loss_quantile50 #> LossQuantile: L(y,x) = h|y - f(x)| #>  #> h = 2q        if  y - f(x) > 0 #>   h = 2(1 - q)  otherwise #>  #>   with quantile q = 0.5 cboost_quantile50 = boostSplines(data = df, target = \"y\", loss = loss_quantile50, iterations = 1000L, trace = 200L) #>    1/1000   risk = 2e+04  time = 0    #>  200/1000   risk = 2e+04  time = 16010    #>  400/1000   risk = 2e+04  time = 32985    #>  600/1000   risk = 2e+04  time = 51634    #>  800/1000   risk = 2e+04  time = 71827    #> 1000/1000   risk = 2e+04  time = 93614    #>  #>  #> Train 1000 iterations in 0 Seconds. #> Final risk based on the train set: 2e+04 cboost_mean = boostSplines(data = df, target = \"y\", iterations = 1000L, trace = 0) #> Train 1000 iterations in 0 Seconds. #> Final risk based on the train set: 9.8e+09  df_plot = data.frame(   feature = rep(x, times = 2),   preds = c(cboost_mean$predict(), cboost_quantile50$predict()),   estimator = rep(c(\"mean\", \"median\"), each = length(x)) )  library(ggsci)  gg1 = ggplot() +   geom_point(data = df, aes(x = x, y = y), show.legend = FALSE, alpha = 0.5) +   geom_line(data = df_plot, aes(x = feature, y = preds, color = estimator), size = 1.2) +   ggtitle(\"Full target range (with outliers)\") #> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. #> ℹ Please use `linewidth` instead.  gg2 = ggplot() +   geom_point(data = df, aes(x = x, y = y), show.legend = FALSE, alpha = 0.5) +   geom_line(data = df_plot, aes(x = feature, y = preds, color = estimator), size = 1.2) +   ggtitle(\"Real target range (without outliers)\") +   ylim(-2, 8)  (gg1 | gg2) +   plot_layout(guides = \"collect\") &   theme(legend.position = \"bottom\") &   theme_tufte() &   scale_color_jama() #> Warning: Removed 20 rows containing missing values (`geom_point()`). #> Warning: Removed 999 rows containing missing values (`geom_line()`)."},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-robust-reg.html","id":"boosting-arbitrary-quantiles","dir":"Articles","previous_headings":"How to use","what":"Boosting arbitrary quantiles","title":"Quantile/Robust regression","text":"Instead boosting median can boost quantile like. following example, boost 10 % 90 % quantile get “kind confidence interval”. careful using term confidence interval . predictions estimated independently may tighten boundaries:","code":"cboost = boostSplines(data = df, target = \"y\", iterations = 1000, trace = 0) #> Train 1000 iterations in 0 Seconds. #> Final risk based on the train set: 9.8e+09 cboost_10 = boostSplines(data = df, target = \"y\", loss = LossQuantile$new(0.1), iterations = 1000, trace = 0) #> Train 1000 iterations in 0 Seconds. #> Final risk based on the train set: 1.8e+04 cboost_50 = boostSplines(data = df, target = \"y\", loss = LossQuantile$new(0.5), iterations = 1000, trace = 0) #> Train 1000 iterations in 0 Seconds. #> Final risk based on the train set: 2e+04 cboost_90 = boostSplines(data = df, target = \"y\", loss = LossQuantile$new(0.9), iterations = 1000, trace = 0) #> Train 1000 iterations in 0 Seconds. #> Final risk based on the train set: 2.2e+04  df_pred = data.frame(   feat = rep(x, 4),   target = rep(y, 4),   pred = c(cboost$predict(), cboost_10$predict(), cboost_50$predict(), cboost_90$predict()),   estimator = rep(c(\"Mean\", \"10% Quantile\", \"Median\", \"90% Quantile\"), each = 1000L))  gg1 = ggplot() +   geom_point(data = df_pred, aes(x = feat, y = target), alpha = 0.2) +   geom_line(data = df_pred, aes(x = feat, y = pred, color = estimator), size = 1.2) +   xlab(\"Feature\") +   ylab(\"Target\") +   ggtitle(\"Full target range (with outliers)\")  gg2 = ggplot() +   geom_point(data = df_pred, aes(x = feat, y = target), alpha = 0.2) +   geom_line(data = df_pred, aes(x = feat, y = pred, color = estimator), size = 1.2) +   ylim(-2, 8) +   xlab(\"Feature\") +   ylab(\"\") +   ggtitle(\"Real target range (without outliers)\")  (gg1 | gg2) +   plot_layout(guides = \"collect\") &   theme_tufte() &   theme(legend.position = \"bottom\") &   scale_color_aaas() #> Warning: Removed 80 rows containing missing values (`geom_point()`). #> Warning: Removed 999 rows containing missing values (`geom_line()`)."},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-robust-reg.html","id":"comments","dir":"Articles","previous_headings":"","what":"Comments","title":"Quantile/Robust regression","text":"Boosting median technique get robust model. Nevertheless, boosting mean nice estimation properties. variance estimators introduced use quantile loss. get precise predictions, need data reduce variance. Another loss superior terms variance Huber loss LossHuber$new() uses quadratic approximation around zero linear extrapolation threshold \\(\\delta\\).","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-use-case.html","id":"data-titanic-passenger-survival-data-set","dir":"Articles","previous_headings":"","what":"Data: Titanic Passenger Survival Data Set","title":"Use-case","text":"use titanic dataset binary classification survived. First store train test data two data frames remove rows contains NAs: next step transform response factor intuitive levels:","code":"# Store train and test data: df_train = na.omit(titanic::titanic_train)  str(df_train) #> 'data.frame':    714 obs. of  12 variables: #>  $ PassengerId: int  1 2 3 4 5 7 8 9 10 11 ... #>  $ Survived   : int  0 1 1 1 0 0 0 1 1 1 ... #>  $ Pclass     : int  3 1 3 1 3 1 3 3 2 3 ... #>  $ Name       : chr  \"Braund, Mr. Owen Harris\" \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\" \"Heikkinen, Miss. Laina\" \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\" ... #>  $ Sex        : chr  \"male\" \"female\" \"female\" \"female\" ... #>  $ Age        : num  22 38 26 35 35 54 2 27 14 4 ... #>  $ SibSp      : int  1 1 0 1 0 0 3 0 1 1 ... #>  $ Parch      : int  0 0 0 0 0 0 1 2 0 1 ... #>  $ Ticket     : chr  \"A/5 21171\" \"PC 17599\" \"STON/O2. 3101282\" \"113803\" ... #>  $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ... #>  $ Cabin      : chr  \"\" \"C85\" \"\" \"C123\" ... #>  $ Embarked   : chr  \"S\" \"C\" \"S\" \"S\" ... #>  - attr(*, \"na.action\")= 'omit' Named int [1:177] 6 18 20 27 29 30 32 33 37 43 ... #>   ..- attr(*, \"names\")= chr [1:177] \"6\" \"18\" \"20\" \"27\" ... df_train$Survived = factor(df_train$Survived, labels = c(\"no\", \"yes\"))"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-use-case.html","id":"initializing-model","dir":"Articles","previous_headings":"","what":"Initializing Model","title":"Use-case","text":"Due R6 API necessary create new class object gets data, target character, used loss. Note important give initialized loss object: Use initialized object loss gives opportunity use loss initialized custom offset.","code":"cboost = Compboost$new(data = df_train, target = \"Survived\", oob_fraction = 0.3)"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-use-case.html","id":"adding-base-learner","dir":"Articles","previous_headings":"","what":"Adding Base-Learner","title":"Use-case","text":"Adding new base-learners also done giving character indicate feature. second argument important name identifier factory since can define multiple base-learner source.","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-use-case.html","id":"numerical-features","dir":"Articles","previous_headings":"Adding Base-Learner","what":"Numerical Features","title":"Use-case","text":"instance, can define spline linear base-learner feature: Additional arguments can specified naming base-learner: references base learner documentation see functionality project page.","code":"# Spline base-learner of age: cboost$addBaselearner(\"Age\", \"spline\", BaselearnerPSpline)  # Linear base-learner of age (degree = 1 with intercept is default): cboost$addBaselearner(\"Age\", \"linear\", BaselearnerPolynomial) # Spline base-learner of fare: cboost$addBaselearner(\"Fare\", \"spline\", BaselearnerPSpline, degree = 2,   n_knots = 14, penalty = 10, differences = 2)"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-use-case.html","id":"categorical-features","dir":"Articles","previous_headings":"Adding Base-Learner","what":"Categorical Features","title":"Use-case","text":"adding categorical features use dummy coded representation ridge penalty: Finally, can check factories registered:","code":"cboost$addBaselearner(\"Sex\", \"categorical\", BaselearnerCategoricalRidge) cboost$getBaselearnerNames() #> [1] \"Age_spline\"      \"Age_linear\"      \"Fare_spline\"     \"Sex_categorical\""},{"path":[]},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-use-case.html","id":"time-logger","dir":"Articles","previous_headings":"Define Logger","what":"Time logger","title":"Use-case","text":"logger logs elapsed time. time unit can one microseconds, seconds minutes. logger stops max_time reached. use logger stopper :","code":"cboost$addLogger(logger = LoggerTime, use_as_stopper = FALSE, logger_id = \"time\",   max_time = 0, time_unit = \"microseconds\")"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-use-case.html","id":"train-model-and-access-elements","dir":"Articles","previous_headings":"","what":"Train Model and Access Elements","title":"Use-case","text":"Objects Compboost class member functions getCoef(), getInbagRisk() predict() access results: obtain vector selected base learners use getSelectedBaselearner(): can also access predictions directly response object cboost$response cboost$response_oob. Note $response_oob created automatically defining oob_fraction within constructor:","code":"cboost$train(2000, trace = 250) #>    1/2000   risk = 0.68  oob_risk = 0.66   time = 0    #>  250/2000   risk = 0.51  oob_risk = 0.49   time = 25159    #>  500/2000   risk = 0.49  oob_risk = 0.48   time = 52318    #>  750/2000   risk = 0.48  oob_risk = 0.47   time = 81427    #> 1000/2000   risk = 0.48  oob_risk = 0.47   time = 113180    #> 1250/2000   risk = 0.48  oob_risk = 0.47   time = 148181    #> 1500/2000   risk = 0.48  oob_risk = 0.48   time = 186249    #> 1750/2000   risk = 0.48  oob_risk = 0.48   time = 229857    #> 2000/2000   risk = 0.48  oob_risk = 0.48   time = 271012    #>  #>  #> Train 2000 iterations in 0 Seconds. #> Final risk based on the train set: 0.48 cboost #>  #>  #> Component-Wise Gradient Boosting #>  #> Target variable: Survived #> Number of base-learners: 4 #> Learning rate: 0.05 #> Iterations: 2000 #>  #> Offset: 0.3228 #>  #> LossBinomial: L(y,x) = log(1 + exp(-2yf(x)) str(cboost$getCoef()) #> List of 4 #>  $ Age_spline     : num [1:24, 1] -4.445 -1.304 -0.947 1.357 1.05 ... #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerPSpline\" #>  $ Fare_spline    : num [1:17, 1] 1.417 0.264 -0.66 -1.519 -1.478 ... #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerPSpline\" #>  $ Sex_categorical: num [1:2, 1] 0.878 -1.352 #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : chr [1:2] \"male\" \"female\" #>   .. ..$ : NULL #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerCategoricalRidge\" #>  $ offset         : num 0.323 str(cboost$getInbagRisk()) #>  num [1:2001] 0.68 0.677 0.674 0.671 0.667 ... str(cboost$predict()) #>  num [1:500, 1] 2.04 -1.964 -0.506 -2.019 1.487 ... table(cboost$getSelectedBaselearner()) #>  #>      Age_spline     Fare_spline Sex_categorical  #>            1109             524             367 oob_label = cboost$response_oob$getResponse() oob_pred = cboost$response_oob$getPredictionResponse() table(true_label = oob_label, predicted = oob_pred) #>           predicted #> true_label  -1   1 #>         -1  53  27 #>         1   17 117"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-use-case.html","id":"retrain-the-model","dir":"Articles","previous_headings":"","what":"Retrain the Model","title":"Use-case","text":"continue training set whole model another iteration simply re-call train():","code":"cboost$train(3000) #>  #> You have already trained 2000 iterations. #> Train 1000 additional iterations. #>  #> 2025/3000   risk = 0.48  oob_risk = 0.48   time = 275655    #> 2100/3000   risk = 0.48  oob_risk = 0.48   time = 288533    #> 2175/3000   risk = 0.48  oob_risk = 0.48   time = 301739    #> 2250/3000   risk = 0.48  oob_risk = 0.48   time = 314631    #> 2325/3000   risk = 0.48  oob_risk = 0.48   time = 328466    #> 2400/3000   risk = 0.48  oob_risk = 0.48   time = 342130    #> 2475/3000   risk = 0.48  oob_risk = 0.48   time = 356032    #> 2550/3000   risk = 0.48  oob_risk = 0.48   time = 370296    #> 2625/3000   risk = 0.48  oob_risk = 0.48   time = 384437    #> 2700/3000   risk = 0.48  oob_risk = 0.48   time = 399191    #> 2775/3000   risk = 0.48  oob_risk = 0.48   time = 414673    #> 2850/3000   risk = 0.48  oob_risk = 0.48   time = 430071    #> 2925/3000   risk = 0.48  oob_risk = 0.48   time = 445474    #> 3000/3000   risk = 0.48  oob_risk = 0.48   time = 461400  str(cboost$getCoef()) #> List of 4 #>  $ Age_spline     : num [1:24, 1] -5.919 -0.982 -1.15 1.47 1.116 ... #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerPSpline\" #>  $ Fare_spline    : num [1:17, 1] 1.431 0.272 -0.661 -1.575 -1.523 ... #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerPSpline\" #>  $ Sex_categorical: num [1:2, 1] 0.886 -1.365 #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : chr [1:2] \"male\" \"female\" #>   .. ..$ : NULL #>   ..- attr(*, \"blclass\")= chr \"Rcpp_BaselearnerCategoricalRidge\" #>  $ offset         : num 0.323 str(cboost$getInbagRisk()) #>  num [1:3001] 0.68 0.677 0.674 0.671 0.667 ... table(cboost$getSelectedBaselearner()) #>  #>      Age_spline     Fare_spline Sex_categorical  #>            1967             637             396"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-use-case.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next steps","title":"Use-case","text":"look visualization capabilities package. See loss functions effect model training.","code":""},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-viz.html","id":"fit-compboost","dir":"Articles","previous_headings":"","what":"Fit compboost","title":"Visualizing a compboost model","text":"data set use mpg: want model miles per gallon (mpg). features include linear centered spline hp, wt, qsec. Additionally, add categorical base learner number cylinders cyl:","code":"mtcars$cyl = as.factor(mtcars$cyl)  set.seed(31415) cboost = Compboost$new(data = mtcars, target = \"mpg\", learning_rate = 0.02, oob_fraction = 0.2)  cboost$addComponents(\"hp\", df = 3) cboost$addComponents(\"wt\", df = 3) cboost$addComponents(\"qsec\", df = 3) cboost$addBaselearner(\"cyl\", \"ridge\", BaselearnerCategoricalRidge, df = 3)  cboost$train(500L, trace = 100L) #>   1/500   risk = 17  oob_risk = 17    #> 100/500   risk = 3  oob_risk = 2.8    #> 200/500   risk = 2.4  oob_risk = 3.2    #> 300/500   risk = 2.3  oob_risk = 3.2    #> 400/500   risk = 2.3  oob_risk = 3.1    #> 500/500   risk = 2.2  oob_risk = 3.1    #>  #>  #> Train 500 iterations in 0 Seconds. #> Final risk based on the train set: 2.2"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-viz.html","id":"visualize-risk-feature-importance-and-selection-traces","dir":"Articles","previous_headings":"","what":"Visualize risk, feature importance, and selection traces","title":"Visualizing a compboost model","text":"starting point analyzing component-wise boosting model take look train validation risk:  can see, best validation risk iteration 98. Hence, set model iteration: Next, interested important base learners/features:  last thing can get better overview model look features/base learners included model:","code":"plotRisk(cboost) m_optimal = which.min(cboost$getLoggerData()[[\"oob_risk\"]]) cboost$train(m_optimal) plotFeatureImportance(cboost) plotBaselearnerTraces(cboost)"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-viz.html","id":"visualize-base-learner-and-partial-effects","dir":"Articles","previous_headings":"","what":"Visualize base learner and partial effects","title":"Visualizing a compboost model","text":"Next, want deep dive effects individual features, .e, effect base learners. end, plot partial effects important feature wt:  observe clear negative trend, meaning increasing weight indicates lower mpg. Additionally, can visualize individual base learners. example categorical feature cyl:  , observe 4 cylinder indicates positive contribution mpg 6 8 cylinder reducing .","code":"plotPEUni(cboost, \"wt\") plotBaselearner(cboost, \"cyl_ridge\")"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-viz.html","id":"visualizing-individual-predictions","dir":"Articles","previous_headings":"","what":"Visualizing individual predictions","title":"Visualizing a compboost model","text":"predictions also want get idea specific contribution feature predicted score. Therefore, take look first observation validation data set:  can see, prediction dominated offset. remove figure set offset = FALSE:  wt hp positive contribution predicted score. means car requires less fuel 6 cylinder slightly increases mpg prediction.","code":"plotIndividualContribution(cboost, newdata = cboost$data_oob[1, ]) plotIndividualContribution(cboost, newdata = cboost$data_oob[1, ], offset = FALSE)"},{"path":"https://schalkdaniel.github.io/compboost/articles/basic-viz.html","id":"visualizing-tensor-products","dir":"Articles","previous_headings":"","what":"Visualizing tensor products","title":"Visualizing a compboost model","text":"last visualization convenience wrapper illustrate interactions included tensors. Therefore, add tensors model: Depending feature combination (numeric - numeric, numeric - categorical, categorical - categorical) different visualization technique used:","code":"mtcars$vs = as.factor(mtcars$vs) mtcars$gear = as.factor(mtcars$gear)  set.seed(31415) cboost = Compboost$new(data = mtcars, target = \"mpg\", oob_fraction = 0.2)  cboost$addTensor(\"wt\", \"qsec\", df = 2) cboost$addTensor(\"hp\", \"cyl\", df = 2) cboost$addTensor(\"gear\", \"vs\", df = 2)  cboost$train(500L, trace = 100L) #>   1/500   risk = 16  oob_risk = 16    #> 100/500   risk = 2.4  oob_risk = 4    #> 200/500   risk = 2.2  oob_risk = 4.4    #> 300/500   risk = 2.1  oob_risk = 4.4    #> 400/500   risk = 2.1  oob_risk = 4.5    #> 500/500   risk = 2  oob_risk = 4.5    #>  #>  #> Train 500 iterations in 0 Seconds. #> Final risk based on the train set: 2 table(cboost$getSelectedBaselearner()) #>  #> gear_vs_tensor  hp_cyl_tensor wt_qsec_tensor  #>            258            181             61 library(ggplot2)  gg1 = plotTensor(cboost, \"wt_qsec_tensor\") + ggtitle(\"Num - Num\") gg2 = plotTensor(cboost, \"hp_cyl_tensor\") + ggtitle(\"Num - Cat\") gg3 = plotTensor(cboost, \"gear_vs_tensor\") + ggtitle(\"Cat - Cat\")  library(patchwork)  gg1 | gg2 | gg3"},{"path":"https://schalkdaniel.github.io/compboost/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Daniel Schalk. Author, maintainer. Janek Thomas. Author. Bernd Bischl. Author.","code":""},{"path":"https://schalkdaniel.github.io/compboost/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Schalk D, Thomas J, Bischl B (2018). “compboost: Modular Framework Component-Wise Boosting.” JOSS, 3(30), 967. doi:10.21105/joss.00967, https://doi.org/10.21105/joss.00967.","code":"@Article{,   author = {Daniel Schalk and Janek Thomas and Bernd Bischl},   title = {compboost: Modular Framework for Component-Wise Boosting},   doi = {10.21105/joss.00967},   url = {https://doi.org/10.21105/joss.00967},   year = {2018},   publisher = {Journal of Open Source Software},   volume = {3},   number = {30},   pages = {967},   journal = {JOSS}, }"},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"compboost-fast-and-flexible-component-wise-boosting-framework-","dir":"","previous_headings":"","what":"C++ Implementation of Component-Wise Boosting","title":"C++ Implementation of Component-Wise Boosting","text":"Documentation | Contributors | Release Notes","code":""},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"C++ Implementation of Component-Wise Boosting","text":"Component-wise boosting applies boosting framework statistical models, e.g., general additive models using component-wise smoothing splines. Boosting kinds models maintains interpretability enables unbiased model selection high dimensional feature spaces. R package compboost alternative implementation component-wise boosting written C++ obtain high runtime performance full memory control. main idea provide modular class system can extended without editing source code. Therefore, possible use R functions well C++ functions custom base-learners, losses, logging mechanisms stopping criteria. introduction overview functionality visit project page.","code":""},{"path":[]},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"cran-version","dir":"","previous_headings":"Installation","what":"CRAN version:","title":"C++ Implementation of Component-Wise Boosting","text":"","code":"install.packages(\"compboost\")"},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"developer-version","dir":"","previous_headings":"Installation","what":"Developer version:","title":"C++ Implementation of Component-Wise Boosting","text":"","code":"devtools::install_github(\"schalkdaniel/compboost\")"},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"examples","dir":"","previous_headings":"","what":"Examples","title":"C++ Implementation of Component-Wise Boosting","text":"examples rendered using compboost 0.1.2. fastest way train Compboost model use wrapper functions boostLinear() boostSplines():  extensive examples use R6 interface visit project page.","code":"cboost = boostSplines(data = iris, target = \"Sepal.Length\",   oob_fraction = 0.3, iterations = 500L, trace = 100L)  ggrisk = plotRisk(cboost) ggpe = plotPEUni(cboost, \"Petal.Length\") ggicont =  plotIndividualContribution(cboost, iris[70, ], offset = FALSE)  library(patchwork)  ggrisk + ggpe + ggicont"},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"mlr-learner","dir":"","previous_headings":"","what":"mlr learner","title":"C++ Implementation of Component-Wise Boosting","text":"Compboost also ships mlr3 learners regression binary classification can used apply compboost within whole mlr3verse:","code":"ts = tsk(\"spam\") lcboost = lrn(\"classif.compboost\", iterations = 500L, bin_root = 2) lcboost$train(ts) lcboost$predict_type = \"prob\" lcboost$predict(ts) #> <PredictionClassif> for 4601 observations: #>     row_ids   truth response prob.spam prob.nonspam #>           1    spam     spam 0.5541776    0.4458224 #>           2    spam     spam 0.8637144    0.1362856 #>           3    spam     spam 0.8242054    0.1757946 #> ---                                                 #>        4599 nonspam  nonspam 0.2052921    0.7947079 #>        4600 nonspam  nonspam 0.2326375    0.7673625 #>        4601 nonspam  nonspam 0.2624331    0.7375669  # Access the `$model` field to access all the `compboost` functionality: plotBaselearnerTraces(lcboost$model) +   plotPEUni(lcboost$model, \"charDollar\")"},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"save-and-load-models","dir":"","previous_headings":"","what":"Save and load models","title":"C++ Implementation of Component-Wise Boosting","text":"usage C++ objects backend, possible use Rs save() method save models. Instead, use $saveToJson(\"mymodel.json\") save model mymodel.json Compboost$new(file = \"mymodel.json\") load model:","code":"cboost = boostSplines(iris, \"Sepal.Width\") cboost$saveToJson(\"mymodel.json\")  cboost_new = Compboost$new(file = \"mymodel.json\")  # Save the model without data: cboost$saveToJson(\"mymodel_without_data.json\", rm_data = TRUE)"},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"benchmark","dir":"","previous_headings":"","what":"Benchmark","title":"C++ Implementation of Component-Wise Boosting","text":"small benchmark conducted compare compboost mboost. purpose, runtime behavior memory consumption two packages compared. results benchmark can read . bigger benchmark adaptions increase runtime memory efficiency can found .","code":""},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"citing","dir":"","previous_headings":"","what":"Citing","title":"C++ Implementation of Component-Wise Boosting","text":"cite compboost publications, please use: Schalk et al., (2018). compboost: Modular Framework Component-Wise Boosting. Journal Open Source Software, 3(30), 967, https://doi.org/10.21105/joss.00967","code":"@article{schalk2018compboost,   author = {Daniel Schalk, Janek Thomas, Bernd Bischl},   title = {compboost: Modular Framework for Component-Wise Boosting},   URL = {https://doi.org/10.21105/joss.00967},   year = {2018},   publisher = {Journal of Open Source Software},   volume = {3},   number = {30},   pages = {967},   journal = {JOSS} }"},{"path":[]},{"path":"https://schalkdaniel.github.io/compboost/index.html","id":"on-your-local-machine","dir":"","previous_headings":"Testing","what":"On your local machine","title":"C++ Implementation of Component-Wise Boosting","text":"order test package functionality can use devtools test package local machine:","code":"devtools::test()"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalBinary.html","id":null,"dir":"Reference","previous_headings":"","what":"Base learner to encode one single class of a categorical feature — BaselearnerCategoricalBinary","title":"Base learner to encode one single class of a categorical feature — BaselearnerCategoricalBinary","text":"class create one-column one-hot encoded data matrix ones x == class_name zero otherwise.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalBinary.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Base learner to encode one single class of a categorical feature — BaselearnerCategoricalBinary","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalBinary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base learner to encode one single class of a categorical feature — BaselearnerCategoricalBinary","text":"data_source CategoricalDataRaw raw data object. Must object generated CategoricalDataRaw. class_name (character(1)) class binary vector created data representation. blearner_type (character(1))  Type base learner (specified, blearner_type = \"binary\" used). unique id base learner defined appending blearner_type feature name: paste0(data_source$getIdentifier(), \"_\", class_name, \"_\", blearner_type).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalBinary.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Base learner to encode one single class of a categorical feature — BaselearnerCategoricalBinary","text":"","code":"BaselearnerCategoricalBinary$new(data_source, class_name) BaselearnerCategoricalBinary$new(data_source, class_name, blearner_type)"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalBinary.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Base learner to encode one single class of a categorical feature — BaselearnerCategoricalBinary","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalBinary.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Base learner to encode one single class of a categorical feature — BaselearnerCategoricalBinary","text":"$summarizeFactory(): () -> () $transfromData(newdata): list(InMemoryData) -> matrix() $getMeta(): () -> list()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalBinary.html","id":"inherited-methods-from-baselearner","dir":"Reference","previous_headings":"","what":"Inherited methods from Baselearner","title":"Base learner to encode one single class of a categorical feature — BaselearnerCategoricalBinary","text":"$getData(): () -> matrix() $getDF(): () -> integer() $getPenalty(): () -> numeric() $getPenaltyMat(): () -> matrix() $getFeatureName(): () -> character() $getModelName(): () -> character() $getBaselearnerId(): () -> character()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalBinary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Base learner to encode one single class of a categorical feature — BaselearnerCategoricalBinary","text":"","code":"# Sample data: x = sample(c(\"one\",\"two\"), 20, TRUE) y = c(one = 0.8, two = -1.2)[x] + rnorm(20, 0, 0.2) dat = data.frame(x, y)  # S4 API: ds = CategoricalDataRaw$new(x, \"cat\") bl = BaselearnerCategoricalBinary$new(ds, \"one\")  bl$getData() #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #> [1,]    0    1    1    0    1    0    0    1    1     1     1     1     0     1 #>      [,15] [,16] [,17] [,18] [,19] [,20] #> [1,]     0     0     1     0     0     1 bl$summarizeFactory() #> Categorical base learner of feature cat and category one bl$transformData(list(ds)) #> $design #> 20 x 1 sparse Matrix of class \"dgCMatrix\" #>         #>  [1,] . #>  [2,] 1 #>  [3,] 1 #>  [4,] . #>  [5,] 1 #>  [6,] . #>  [7,] . #>  [8,] 1 #>  [9,] 1 #> [10,] 1 #> [11,] 1 #> [12,] 1 #> [13,] . #> [14,] 1 #> [15,] . #> [16,] . #> [17,] 1 #> [18,] . #> [19,] . #> [20,] 1 #>  bl$getBaselearnerId() #> [1] \"cat_one_binary\"  # R6 API: cboost = Compboost$new(dat, \"y\") cboost$addBaselearner(\"x\", \"binary\", BaselearnerCategoricalBinary) cboost$train(500, 0) #> Train 500 iterations in 0 Seconds. #> Final risk based on the train set: 0.0086 #>  table(cboost$getSelectedBaselearner()) #>  #> x_one_binary x_two_binary  #>          249          251  plotPEUni(cboost, \"x\", individual = FALSE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalRidge.html","id":null,"dir":"Reference","previous_headings":"","what":"One-hot encoded base learner for a categorical feature — BaselearnerCategoricalRidge","title":"One-hot encoded base learner for a categorical feature — BaselearnerCategoricalRidge","text":"base learner can used estimate effects categorical features. classes included similar linear model using one-hot encoded data matrix. Additionally, Ridge penalty allows unbiased feature selection.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalRidge.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"One-hot encoded base learner for a categorical feature — BaselearnerCategoricalRidge","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalRidge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"One-hot encoded base learner for a categorical feature — BaselearnerCategoricalRidge","text":"data_source CategoricalDataRaw Data container raw categorical feature. blearner_type (character(1))  Type base learner (specified, blearner_type = \"ridge\" used). unique id base learner defined appending blearner_type feature name: paste0(data_source$getIdentifier(), \"_\", blearner_type). df (numeric(1)) Degrees freedom base learner(s).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalRidge.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"One-hot encoded base learner for a categorical feature — BaselearnerCategoricalRidge","text":"","code":"BaselearnerCategoricalRidge$new(data_source, list(df)) BaselearnerCategoricalRidge$new(data_source, blearner_type, list(df))"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalRidge.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"One-hot encoded base learner for a categorical feature — BaselearnerCategoricalRidge","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalRidge.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"One-hot encoded base learner for a categorical feature — BaselearnerCategoricalRidge","text":"$summarizeFactory(): () -> () $transfromData(newdata): list(InMemoryData) -> matrix() $getMeta(): () -> list()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalRidge.html","id":"inherited-methods-from-baselearner","dir":"Reference","previous_headings":"","what":"Inherited methods from Baselearner","title":"One-hot encoded base learner for a categorical feature — BaselearnerCategoricalRidge","text":"$getData(): () -> matrix() $getDF(): () -> integer() $getPenalty(): () -> numeric() $getPenaltyMat(): () -> matrix() $getFeatureName(): () -> character() $getModelName(): () -> character() $getBaselearnerId(): () -> character()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCategoricalRidge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"One-hot encoded base learner for a categorical feature — BaselearnerCategoricalRidge","text":"","code":"# Sample data: x = sample(c(\"one\",\"two\"), 20, TRUE) y = c(one = 0.8, two = -1.2)[x] + rnorm(20, 0, 0.2) dat = data.frame(x, y)  # S4 API: ds = CategoricalDataRaw$new(x, \"cat\") bl = BaselearnerCategoricalRidge$new(ds, list(df = 1))  bl$getData() #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #> [1,]    1    0    0    0    1    0    1    1    1     0     1     1     0     0 #> [2,]    0    1    1    1    0    1    0    0    0     1     0     0     1     1 #>      [,15] [,16] [,17] [,18] [,19] [,20] #> [1,]     1     1     1     1     0     0 #> [2,]     0     0     0     0     1     1 bl$summarizeFactory() #> Categorical base learner of category cat  bl$getData() #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #> [1,]    1    0    0    0    1    0    1    1    1     0     1     1     0     0 #> [2,]    0    1    1    1    0    1    0    0    0     1     0     0     1     1 #>      [,15] [,16] [,17] [,18] [,19] [,20] #> [1,]     1     1     1     1     0     0 #> [2,]     0     0     0     0     1     1 bl$summarizeFactory() #> Categorical base learner of category cat bl$transformData(list(ds)) #> $design #> 20 x 2 sparse Matrix of class \"dgCMatrix\" #>           #>  [1,] 1 . #>  [2,] . 1 #>  [3,] . 1 #>  [4,] . 1 #>  [5,] 1 . #>  [6,] . 1 #>  [7,] 1 . #>  [8,] 1 . #>  [9,] 1 . #> [10,] . 1 #> [11,] 1 . #> [12,] 1 . #> [13,] . 1 #> [14,] . 1 #> [15,] 1 . #> [16,] 1 . #> [17,] 1 . #> [18,] 1 . #> [19,] . 1 #> [20,] . 1 #>  bl$getBaselearnerId() #> [1] \"cat_cat\"  # R6 API: cboost = Compboost$new(dat, \"y\") cboost$addBaselearner(\"x\", \"binary\", BaselearnerCategoricalRidge) cboost$train(100, 0) #> Train 100 iterations in 0 Seconds. #> Final risk based on the train set: 0.027 #>  table(cboost$getSelectedBaselearner()) #>  #> x_binary  #>      100  plotPEUni(cboost, \"x\", individual = FALSE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCentered.html","id":null,"dir":"Reference","previous_headings":"","what":"Centering a base learner by another one — BaselearnerCentered","title":"Centering a base learner by another one — BaselearnerCentered","text":"base learner subtracts effect two base learners (usually defined feature). subtracting effects, one able predict one. becomes handy decomposing effects , e.g., linear non-linear component non-linear component capable capture linear part hence selected linear effect estimated.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCentered.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Centering a base learner by another one — BaselearnerCentered","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCentered.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Centering a base learner by another one — BaselearnerCentered","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCentered.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Centering a base learner by another one — BaselearnerCentered","text":"$summarizeFactory(): () -> () $transfromData(newdata): list(InMemoryData) -> matrix() $getMeta(): () -> list() $getRotation(): () -> matrix()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCentered.html","id":"inherited-methods-from-baselearner","dir":"Reference","previous_headings":"","what":"Inherited methods from Baselearner","title":"Centering a base learner by another one — BaselearnerCentered","text":"$getData(): () -> matrix() $getDF(): () -> integer() $getPenalty(): () -> numeric() $getPenaltyMat(): () -> matrix() $getFeatureName(): () -> character() $getModelName(): () -> character() $getBaselearnerId(): () -> character()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCentered.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Centering a base learner by another one — BaselearnerCentered","text":"","code":"# Sample data: x = runif(100, 0, 10) y = 2 * sin(x) + 2 * x + rnorm(100, 0, 0.5) dat = data.frame(x, y)  # S4 wrapper  # Create new data object, a matrix is required as input: data_mat = cbind(x) data_source = InMemoryData$new(data_mat, \"x\")  # Prerequisite: Create a linear and spline base learner: bl_lin = BaselearnerPolynomial$new(data_source,   list(degree = 1, intercept = TRUE)) bl_sp = BaselearnerPSpline$new(data_source,   list(n_knots = 15, df = 5))  # Now, subtract the linear effect from the spline: bl_ctr = BaselearnerCentered$new(bl_sp, bl_lin, \"ctr\")  # Recognize, that the data matrix of this base learner has # `nrow(bl_sp$getData()) - ncol(bl_lin$getData())` columns: dim(bl_ctr$getData()) #> [1] 100  17 str(bl_ctr$getMeta()) #> List of 4 #>  $ df         : num [1, 1] 5 #>  $ penalty    : num [1, 1] 52.9 #>  $ penalty_mat: num [1:17, 1:17] 8.293 -2.948 2.15 0.611 0.344 ... #>  $ rotation   : num [1:19, 1:17] -0.215 -0.367 0.848 -0.207 -0.139 ...  # The data matrix is created by rotating the spline data matrix: all.equal(t(bl_sp$getData()) %*% bl_ctr$getRotation(), bl_ctr$getData()) #> [1] TRUE  # Transform \"new data\". Internally, the basis of the spline is build and # then rotated by the rotation matrix to subtract the linear part: newdata = list(InMemoryData$new(cbind(rnorm(5)), \"x\")) bl_ctr$transformData(newdata) #> $design #>            [,1]       [,2]       [,3]       [,4]       [,5]        [,6] #> [1,] -0.1348350  0.3125475  0.2270293 -0.1197891 -0.1076054 -0.08474486 #> [2,]  0.2180550  0.2977064 -0.1311777 -0.1309194 -0.1092384 -0.08518136 #> [3,] -0.1390538 -0.4110045 -0.2711517 -0.1986033 -0.1572176 -0.11314308 #> [4,] -0.1390538 -0.4110045 -0.2711517 -0.1986033 -0.1572176 -0.11314308 #> [5,] -0.1390538 -0.4110045 -0.2711517 -0.1986033 -0.1572176 -0.11314308 #>             [,7]        [,8]        [,9]       [,10]       [,11]       [,12] #> [1,] -0.09158301 -0.04339972 -0.04784337 -0.05417366 -0.04541494 -0.02164030 #> [2,] -0.09104019 -0.04254708 -0.04530304 -0.04932203 -0.03864223 -0.01532888 #> [3,] -0.10952707 -0.04442306 -0.02886567 -0.00787070  0.02739079  0.05179881 #> [4,] -0.10952707 -0.04442306 -0.02886567 -0.00787070  0.02739079  0.05179881 #> [5,] -0.10952707 -0.04442306 -0.02886567 -0.00787070  0.02739079  0.05179881 #>             [,13]       [,14]      [,15]      [,16]       [,17] #> [1,] -0.005792368 0.009000833 0.02947063 0.02731179 0.003802385 #> [2,]  0.002173466 0.019250239 0.04316781 0.03705710 0.005042536 #> [3,]  0.092716378 0.140198125 0.20951727 0.15712723 0.020410574 #> [4,]  0.092716378 0.140198125 0.20951727 0.15712723 0.020410574 #> [5,]  0.092716378 0.140198125 0.20951727 0.15712723 0.020410574 #>   # R6 wrapper  # Compboost has a wrapper called `$addComponents()` that automatically cboost = Compboost$new(dat, \"y\")  # creates and adds the linear base learner and a centered base learner # as above (the `...` args are passed to `BaselearnerPSpline$new(): cboost$addComponents(\"x\", n_knots = 10, df = 5, bin_root = 2)  # Note that we have used binning to save memory, hence the data matrix # is reduced to 10 observations: dim(cboost$baselearner_list$x_x_spline_centered$factory$getData()) #> [1] 10 12  cboost$train(200, 0) #> Train 200 iterations in 0 Seconds. #> Final risk based on the train set: 0.39 #>   library(ggplot2)  plotPEUni(cboost, \"x\") +   geom_point(data = dat, aes(x = x, y = y - c(cboost$offset)), alpha = 0.2)"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustom.html","id":null,"dir":"Reference","previous_headings":"","what":"Custom base learner using R functions. — BaselearnerCustom","title":"Custom base learner using R functions. — BaselearnerCustom","text":"class defines custom base learner factory passing R functions instantiation, fitting, predicting.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustom.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Custom base learner using R functions. — BaselearnerCustom","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustom.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Custom base learner using R functions. — BaselearnerCustom","text":"data_source (InMemoryData) Uninitialized data object used store meta data. Note: moment, just memory storing supported, see ?InMemorydata details. instantiate_fun (function)R function transform source data. train_fun (function)R function train base learner target data. predict_fun (function)R function predict object returned train_fun. param_fun (function)R function extract parameter object returned train.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustom.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Custom base learner using R functions. — BaselearnerCustom","text":"","code":"BaselearnerCustom$new(data_source, list(instantiate_fun,   train_fun, predict_fun, param_fun))"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustom.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Custom base learner using R functions. — BaselearnerCustom","text":"function must following structure: instantiateData(X) { ... return (X_trafo) } matrix argument X matrix return object. train(y, X) { ... return (SEXP) } vector argument y matrix argument X. target data used X y contains response. function can return R object stored within SEXP. predict(model, newdata) { ... return (prediction) } returned object train function passed model argument newdata contains new matrix used predicting. extractParameter() { ... return (parameters) } , model contains object returned train. returned object must matrix containing estimated parameter. parameter estimated one can return NA. example see Examples.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustom.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Custom base learner using R functions. — BaselearnerCustom","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustom.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Custom base learner using R functions. — BaselearnerCustom","text":"$summarizeFactory(): () -> () $transfromData(newdata): list(InMemoryData) -> matrix() $getMeta(): () -> list()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustom.html","id":"inherited-methods-from-baselearner","dir":"Reference","previous_headings":"","what":"Inherited methods from Baselearner","title":"Custom base learner using R functions. — BaselearnerCustom","text":"$getData(): () -> matrix() $getDF(): () -> integer() $getPenalty(): () -> numeric() $getPenaltyMat(): () -> matrix() $getFeatureName(): () -> character() $getModelName(): () -> character() $getBaselearnerId(): () -> character()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerCustom.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Custom base learner using R functions. — BaselearnerCustom","text":"","code":"# Sample data: data_mat = cbind(1, 1:10) y = 2 + 3 * 1:10  # Create new data object: data_source = InMemoryData$new(data_mat, \"my_data_name\")  instantiateDataFun = function (X) {   return(X) } # Ordinary least squares estimator: trainFun = function (y, X) {   return(solve(t(X) %*% X) %*% t(X) %*% y) } predictFun = function (model, newdata) {   return(as.matrix(newdata %*% model)) } extractParameter = function (model) {   return(as.matrix(model)) }  # Create new custom linear base learner factory: custom_lin_factory = BaselearnerCustom$new(data_source,   list(instantiate_fun = instantiateDataFun, train_fun = trainFun,     predict_fun = predictFun, param_fun = extractParameter))  # Get the transformed data: custom_lin_factory$getData() #>       [,1] [,2] #>  [1,]    1    1 #>  [2,]    1    2 #>  [3,]    1    3 #>  [4,]    1    4 #>  [5,]    1    5 #>  [6,]    1    6 #>  [7,]    1    7 #>  [8,]    1    8 #>  [9,]    1    9 #> [10,]    1   10  # Summarize factory: custom_lin_factory$summarizeFactory() #> Custom base learner Factory: #> \t- Name of the used data: my_data_name #> \t- Factory creates the following base learner: custom"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPSpline.html","id":null,"dir":"Reference","previous_headings":"","what":"Non-parametric B or P-spline base learner — BaselearnerPSpline","title":"Non-parametric B or P-spline base learner — BaselearnerPSpline","text":"BaselearnerPSpline creates spline base learner object. object calculates B-spline basis functions case P-splines also penalty. Instead defining penalty term directly, one consider restrict flexibility setting degrees freedom.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPSpline.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Non-parametric B or P-spline base learner — BaselearnerPSpline","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPSpline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Non-parametric B or P-spline base learner — BaselearnerPSpline","text":"data_source (InMemoryData)  Data object contains raw data (see ?InMemoryData). blearner_type (character(1))  Type base learner (specified, blearner_type = \"spline\" used). unique id base learner defined appending blearner_type feature name: paste0(data_source$getIdentifier(), \"_\", blearner_type). degree (integer(1)) Degree piecewise polynomial (default degree = 3 cubic splines). n_knots (integer(1)) Number inner knots (default n_knots = 20). inner knots expanded degree - 1 additional knots side prevent unstable behavior edges. penalty (numeric(1)) Penalty term P-splines (default penalty = 2). Set zero B-splines. differences (integer(1)) number differences penalized. higher value leads smoother curves. df (numeric(1)) Degrees freedom base learner(s). bin_root (integer(1)) binning root reduce data \\(n^{1/\\text{binroot}}\\) data points (default bin_root = 1, means binning applied). value bin_root = 2 suggested best approximation error (cf. Wood et al. (2017) Generalized additive models gigadata: modeling UK black smoke network daily data).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPSpline.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Non-parametric B or P-spline base learner — BaselearnerPSpline","text":"","code":"BaselearnerPSpline$new(data_source, list(degree, n_knots, penalty, differences, df, bin_root)) BaselearnerPSpline$new(data_source, blearner_type, list(degree, n_knots, penalty, differences, df, bin_root))"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPSpline.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Non-parametric B or P-spline base learner — BaselearnerPSpline","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPSpline.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Non-parametric B or P-spline base learner — BaselearnerPSpline","text":"$summarizeFactory(): () -> () $transfromData(newdata): list(InMemoryData) -> matrix() $getMeta(): () -> list()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPSpline.html","id":"inherited-methods-from-baselearner","dir":"Reference","previous_headings":"","what":"Inherited methods from Baselearner","title":"Non-parametric B or P-spline base learner — BaselearnerPSpline","text":"$getData(): () -> matrix() $getDF(): () -> integer() $getPenalty(): () -> numeric() $getPenaltyMat(): () -> matrix() $getFeatureName(): () -> character() $getModelName(): () -> character() $getBaselearnerId(): () -> character()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPSpline.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Non-parametric B or P-spline base learner — BaselearnerPSpline","text":"data matrix instantiated transposed sparse matrix due performance reasons. member function $getData() accounts  $transformData() returns raw data matrix p x n matrix.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPSpline.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Non-parametric B or P-spline base learner — BaselearnerPSpline","text":"","code":"# Sample data: x = runif(100, 0, 10) y = sin(x) + rnorm(100, 0, 0.2) dat = data.frame(x, y)  # S4 wrapper  # Create new data object, a matrix is required as input: data_mat = cbind(x) data_source = InMemoryData$new(data_mat, \"my_data_name\")  # Create new linear base learner factory: bl_sp_df2 = BaselearnerPSpline$new(data_source,   list(n_knots = 10, df = 2, bin_root = 2)) bl_sp_df5 = BaselearnerPSpline$new(data_source,   list(n_knots = 15, df = 5))  # Get the transformed data: dim(bl_sp_df2$getData()) #> [1] 14 10 dim(bl_sp_df5$getData()) #> [1]  19 100  # Summarize factory: bl_sp_df2$summarizeFactory() #> Spline factory of degree 3 #> \t- Name of the used data: my_data_name #> \t- Factory creates the following base learner: spline_degree_3  # Get full meta data such as penalty term or matrix as well as knots: str(bl_sp_df2$getMeta()) #> List of 8 #>  $ df         : num 2 #>  $ penalty    : num 1546751 #>  $ penalty_mat: num [1:14, 1:14] 1 -2 1 0 0 0 0 0 0 0 ... #>  $ degree     : num 3 #>  $ n_knots    : num 10 #>  $ differences: num 2 #>  $ bin_root   : num 0 #>  $ knots      : num [1:18, 1] -2.6165 -1.7237 -0.831 0.0617 0.9544 ... bl_sp_df2$getPenalty() #>         [,1] #> [1,] 1546751 bl_sp_df5$getPenalty() # The penalty here is smaller due to more flexibility #>          [,1] #> [1,] 50.94736  # Transform \"new data\": newdata = list(InMemoryData$new(cbind(rnorm(5)), \"my_data_name\")) bl_sp_df2$transformData(newdata) #> $design #> 5 x 14 sparse Matrix of class \"dgCMatrix\" #>                                                                     #> [1,] 0.1666667 0.6666667 0.1666667 .            . . . . . . . . . . #> [2,] 0.1666667 0.6666667 0.1666667 .            . . . . . . . . . . #> [3,] 0.1407161 0.6637401 0.1955162 2.751059e-05 . . . . . . . . . . #> [4,] 0.1666667 0.6666667 0.1666667 .            . . . . . . . . . . #> [5,] 0.1666667 0.6666667 0.1666667 .            . . . . . . . . . . #>  bl_sp_df5$transformData(newdata) #> $design #> 5 x 19 sparse Matrix of class \"dgCMatrix\" #>                                                                               #> [1,] 0.1666667 0.6666667 0.1666667 .            . . . . . . . . . . . . . . . #> [2,] 0.1666667 0.6666667 0.1666667 .            . . . . . . . . . . . . . . . #> [3,] 0.1298705 0.6605543 0.2094905 8.466068e-05 . . . . . . . . . . . . . . . #> [4,] 0.1666667 0.6666667 0.1666667 .            . . . . . . . . . . . . . . . #> [5,] 0.1666667 0.6666667 0.1666667 .            . . . . . . . . . . . . . . . #>   # R6 wrapper  cboost_df2 = Compboost$new(dat, \"y\") cboost_df2$addBaselearner(\"x\", \"sp\", BaselearnerPSpline,   n_knots = 10, df = 2, bin_root = 2) cboost_df2$train(200, 0) #> Train 200 iterations in 0 Seconds. #> Final risk based on the train set: 0.22 #>   cboost_df5 = Compboost$new(dat, \"y\") cboost_df5$addBaselearner(\"x\", \"sp\", BaselearnerPSpline,   n_knots = 15, df = 5) cboost_df5$train(200, 0) #> Train 200 iterations in 0 Seconds. #> Final risk based on the train set: 0.018 #>   # Access base learner directly from the API (n = sqrt(100) = 10 with binning): str(cboost_df2$baselearner_list$x_sp$factory$getData()) #>  num [1:14, 1:10] 0.167 0.667 0.167 0 0 ... str(cboost_df5$baselearner_list$x_sp$factory$getData()) #>  num [1:19, 1:100] 0.000776 0.261751 0.641121 0.096353 0 ...  gg_df2 = plotPEUni(cboost_df2, \"x\") gg_df5 = plotPEUni(cboost_df5, \"x\")  library(ggplot2) library(patchwork)  (gg_df2 | gg_df5) &   geom_point(data = dat, aes(x = x, y = y - c(cboost_df2$offset)), alpha = 0.2)"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPolynomial.html","id":null,"dir":"Reference","previous_headings":"","what":"Polynomial base learner — BaselearnerPolynomial","title":"Polynomial base learner — BaselearnerPolynomial","text":"[BaselearnerPolynomial] creates polynomial base learner object. base learner takes one feature calculates polynomials (intercept) \\(1 + x + x^2 + \\dots + x^d\\) given degree \\(d\\).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPolynomial.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Polynomial base learner — BaselearnerPolynomial","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPolynomial.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Polynomial base learner — BaselearnerPolynomial","text":"data_source (InMemoryData)  Data object contains raw data (see ?InMemoryData). blearner_type (character(1))  Type base learner (specified, blearner_type = paste0(\"poly\", d) used). unique id base learner defined appending blearner_type feature name: paste0(data_source$getIdentifier(), \"_\", blearner_type). degree (integer(1)) Polynomial degree. intercept (logical(1)) Polynomial degree. bin_root (integer(1)) binning root reduce data \\(n^{1/\\text{binroot}}\\) data points (default bin_root = 1, means binning applied). value bin_root = 2 suggested best approximation error (cf. Wood et al. (2017) Generalized additive models gigadata: modeling UK black smoke network daily data).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPolynomial.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Polynomial base learner — BaselearnerPolynomial","text":"","code":"BaselearnerPolynomial$new(data_source, list(degree, intercept, bin_root)) BaselearnerPolynomial$new(data_source, blearner_type, list(degree, intercept, bin_root))"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPolynomial.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Polynomial base learner — BaselearnerPolynomial","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPolynomial.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Polynomial base learner — BaselearnerPolynomial","text":"$summarizeFactory(): () -> () $transfromData(newdata): list(InMemoryData) -> matrix() $getMeta(): () -> list()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPolynomial.html","id":"inherited-methods-from-baselearner","dir":"Reference","previous_headings":"","what":"Inherited methods from Baselearner","title":"Polynomial base learner — BaselearnerPolynomial","text":"$getData(): () -> matrix() $getDF(): () -> integer() $getPenalty(): () -> numeric() $getPenaltyMat(): () -> matrix() $getFeatureName(): () -> character() $getModelName(): () -> character() $getBaselearnerId(): () -> character()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerPolynomial.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Polynomial base learner — BaselearnerPolynomial","text":"","code":"# Sample data: x = runif(100) y = 1 + 2*x + rnorm(100, 0, 0.2) dat = data.frame(x, y)  # S4 wrapper  # Create new data object, a matrix is required as input: data_mat = cbind(x) data_source = InMemoryData$new(data_mat, \"my_data_name\")  # Create new linear base learner factory: bl_lin = BaselearnerPolynomial$new(data_source,   list(degree = 1)) bl_cub = BaselearnerPolynomial$new(data_source,   list(intercept = FALSE, degree = 3, bin_root = 2))  # Get the transformed data: head(bl_lin$getData()) #>      [,1]      [,2] #> [1,]    1 0.1822540 #> [2,]    1 0.5759809 #> [3,]    1 0.5296214 #> [4,]    1 0.1155333 #> [5,]    1 0.5017006 #> [6,]    1 0.5330134 head(bl_cub$getData()) #>            [,1]        [,2]        [,3] #> [1,] 0.04751434 0.002257613 0.000107269 #> [2,] 0.15285466 0.023364547 0.003571380 #> [3,] 0.25819498 0.066664647 0.017212477 #> [4,] 0.36353530 0.132157913 0.048044066 #> [5,] 0.46887562 0.219844343 0.103079652 #> [6,] 0.57421593 0.329723939 0.189332740  # Summarize factory: bl_lin$summarizeFactory() #> Linear base learner factory: #> \t- Name of the used data: my_data_name #> \t- Factory creates the following base learner: poly1  # Transform \"new data\": newdata = list(InMemoryData$new(cbind(rnorm(5)), \"my_data_name\")) bl_lin$transformData(newdata) #> $design #>      [,1]       [,2] #> [1,]    1  0.2606059 #> [2,]    1 -0.9792610 #> [3,]    1 -0.1534696 #> [4,]    1  0.1841760 #> [5,]    1  1.6480863 #>  bl_cub$transformData(newdata) #> $design #>            [,1]       [,2]         [,3] #> [1,]  0.2606059 0.06791542  0.017699158 #> [2,] -0.9792610 0.95895220 -0.939064540 #> [3,] -0.1534696 0.02355291 -0.003614655 #> [4,]  0.1841760 0.03392081  0.006247399 #> [5,]  1.6480863 2.71618855  4.476513218 #>   # R6 wrapper  cboost_lin = Compboost$new(dat, \"y\") cboost_lin$addBaselearner(\"x\", \"lin\", BaselearnerPolynomial, degree = 1) cboost_lin$train(100, 0) #> Train 100 iterations in 0 Seconds. #> Final risk based on the train set: 0.025 #>   cboost_cub = Compboost$new(dat, \"y\") cboost_cub$addBaselearner(\"x\", \"cubic\", BaselearnerPolynomial,   intercept = FALSE, degree = 3, bin_root = 2) cboost_cub$train(100, 0) #> Train 100 iterations in 0 Seconds. #> Final risk based on the train set: 0.04 #>   # Access base learner directly from the API (n = sqrt(100) = 10 with binning): head(cboost_lin$baselearner_list$x_lin$factory$getData()) #>      [,1]      [,2] #> [1,]    1 0.1822540 #> [2,]    1 0.5759809 #> [3,]    1 0.5296214 #> [4,]    1 0.1155333 #> [5,]    1 0.5017006 #> [6,]    1 0.5330134 cboost_cub$baselearner_list$x_cubic$factory$getData() #>             [,1]        [,2]        [,3] #>  [1,] 0.04751434 0.002257613 0.000107269 #>  [2,] 0.15285466 0.023364547 0.003571380 #>  [3,] 0.25819498 0.066664647 0.017212477 #>  [4,] 0.36353530 0.132157913 0.048044066 #>  [5,] 0.46887562 0.219844343 0.103079652 #>  [6,] 0.57421593 0.329723939 0.189332740 #>  [7,] 0.67955625 0.461796701 0.313816835 #>  [8,] 0.78489657 0.616062627 0.483545444 #>  [9,] 0.89023689 0.792521720 0.705532071 #> [10,] 0.99557721 0.991173977 0.986790221  gg_lin = plotPEUni(cboost_lin, \"x\") gg_cub = plotPEUni(cboost_cub, \"x\")  library(ggplot2) library(patchwork)  (gg_lin | gg_cub) &   geom_point(data = dat, aes(x = x, y = y - c(cboost_lin$offset)), alpha = 0.2)"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerTensor.html","id":null,"dir":"Reference","previous_headings":"","what":"Row-wise tensor product base learner — BaselearnerTensor","title":"Row-wise tensor product base learner — BaselearnerTensor","text":"class combines base learners. base learner defined data matrix calculated row-wise tensor product two data matrices given base learners combine.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerTensor.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Row-wise tensor product base learner — BaselearnerTensor","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerTensor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Row-wise tensor product base learner — BaselearnerTensor","text":"blearner1 (Baselearner*) First base learner. blearner2 (Baselearner*) Second base learner. blearner_type (character(1))  Type base learner (specified, blearner_type = \"spline\" used). unique id base learner defined appending blearner_type feature name: paste0(blearner1$getDataSource()getIdentifier(), \"_\", blearner2$getDataSource()getIdentifier(), \"_\", blearner_type). anisotrop (logical(1)) Defines penalty added . anisotrop = TRUE, marginal effects penalized defined underlying factories. anisotrop = FALSE, isotropic penalty used, means directions gets penalized equally.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerTensor.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Row-wise tensor product base learner — BaselearnerTensor","text":"","code":"BaselearnerTensor$new(blearner1, blearner2, blearner_type) BaselearnerTensor$new(blearner1, blearner2, blearner_type, anisotrop)"},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerTensor.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Row-wise tensor product base learner — BaselearnerTensor","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerTensor.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Row-wise tensor product base learner — BaselearnerTensor","text":"$summarizeFactory(): () -> () $transfromData(newdata): list(InMemoryData) -> matrix() $getMeta(): () -> list()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerTensor.html","id":"inherited-methods-from-baselearner","dir":"Reference","previous_headings":"","what":"Inherited methods from Baselearner","title":"Row-wise tensor product base learner — BaselearnerTensor","text":"$getData(): () -> matrix() $getDF(): () -> integer() $getPenalty(): () -> numeric() $getPenaltyMat(): () -> matrix() $getFeatureName(): () -> character() $getModelName(): () -> character() $getBaselearnerId(): () -> character()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BaselearnerTensor.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Row-wise tensor product base learner — BaselearnerTensor","text":"","code":"# Sample data: x1 = runif(100, 0, 10) x2 = runif(100, 0, 10) y = sin(x1) * cos(x2) + rnorm(100, 0, 0.2) dat = data.frame(x1, x2, y)  # S4 wrapper  # Create new data object, a matrix is required as input: ds1 = InMemoryData$new(cbind(x1), \"x1\") ds2 = InMemoryData$new(cbind(x2), \"x2\")  # Create new linear base learner factory: bl1 = BaselearnerPSpline$new(ds1, \"sp\", list(n_knots = 10, df = 5)) bl2 = BaselearnerPSpline$new(ds2, \"sp\", list(n_knots = 10, df = 5))  tensor = BaselearnerTensor$new(bl1, bl2, \"row_tensor\")  # Get the transformed data: dim(tensor$getData()) #> [1] 196 100  # Get full meta data such as penalty term or matrix as well as knots: str(tensor$getMeta()) #> List of 3 #>  $ df         : num [1:2, 1] 5 5 #>  $ penalty    : num [1:2, 1] 15.6 14.8 #>  $ penalty_mat: num [1:196, 1:196] 30.4 -29.7 14.8 0 0 ...  # Transform \"new data\": newdata = list(InMemoryData$new(cbind(runif(5)), \"x1\"),   InMemoryData$new(cbind(runif(5)), \"x2\")) str(tensor$transformData(newdata)) #> List of 1 #>  $ design:Formal class 'dgCMatrix' [package \"Matrix\"] with 6 slots #>   .. ..@ i       : int [1:76] 0 1 2 3 4 0 1 2 3 4 ... #>   .. ..@ p       : int [1:197] 0 5 10 15 19 19 19 19 19 19 ... #>   .. ..@ Dim     : int [1:2] 5 196 #>   .. ..@ Dimnames:List of 2 #>   .. .. ..$ : NULL #>   .. .. ..$ : NULL #>   .. ..@ x       : num [1:76] 8.98e-04 2.71e-03 5.61e-06 1.50e-03 8.59e-05 ... #>   .. ..@ factors : list()  # R6 wrapper  cboost = Compboost$new(dat, \"y\") cboost$addTensor(\"x1\", \"x2\", df = 5) cboost$train(50, 0) #> Train 50 iterations in 0 Seconds. #> Final risk based on the train set: 0.09 #>   table(cboost$getSelectedBaselearner()) #>  #> x1_x2_tensor  #>           50  plotTensor(cboost, \"x1_x2_tensor\")"},{"path":"https://schalkdaniel.github.io/compboost/reference/BlearnerFactoryList.html","id":null,"dir":"Reference","previous_headings":"","what":"Base learner factory list to define the set of base learners — BlearnerFactoryList","title":"Base learner factory list to define the set of base learners — BlearnerFactoryList","text":"BlearnerFactoryList creates object base learner factories can registered. object can passed compboost set base learner used optimizer get new best base learner.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BlearnerFactoryList.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Base learner factory list to define the set of base learners — BlearnerFactoryList","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BlearnerFactoryList.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Base learner factory list to define the set of base learners — BlearnerFactoryList","text":"","code":"BlearnerFactoryList$new()"},{"path":"https://schalkdaniel.github.io/compboost/reference/BlearnerFactoryList.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Base learner factory list to define the set of base learners — BlearnerFactoryList","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BlearnerFactoryList.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Base learner factory list to define the set of base learners — BlearnerFactoryList","text":"registerFactory(BaselearnerFactory) Takes object class BaseLearnerFactory adds factory set base learner. printRegisteredFactories() Get registered factories. clearRegisteredFactories() Remove registered factories. Note factories deleted, just removed map. getModelFrame() Get target data matrix parsed one big matrix. getNumberOfRegisteredFactories() Get number registered factories.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/BlearnerFactoryList.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Base learner factory list to define the set of base learners — BlearnerFactoryList","text":"","code":"# Sample data: data_mat = cbind(1:10)  # Create new data object: data_source = InMemoryData$new(data_mat, \"my_data_name\")  lin_factory = BaselearnerPolynomial$new(data_source,   list(degree = 1, intercept = TRUE)) poly_factory = BaselearnerPolynomial$new(data_source,   list(degree = 2, intercept = TRUE))  # Create new base learner list: my_bl_list = BlearnerFactoryList$new()  # Register factories: my_bl_list$registerFactory(lin_factory) my_bl_list$registerFactory(poly_factory)  # Get registered factories: my_bl_list$printRegisteredFactories() #> Registered base-learner: #> \t- my_data_name_poly1 #> \t- my_data_name_poly2  # Get all target data matrices in one big matrix: my_bl_list$getModelFrame() #> $colnames #> [1] \"my_data_name_poly1x11\" \"my_data_name_poly1x12\" \"my_data_name_poly2x11\" #> [4] \"my_data_name_poly2x12\" \"my_data_name_poly2x13\" #>  #> $model_frame #>       [,1] [,2] [,3] [,4] [,5] #>  [1,]    1    1    1    1    1 #>  [2,]    1    2    1    2    4 #>  [3,]    1    3    1    3    9 #>  [4,]    1    4    1    4   16 #>  [5,]    1    5    1    5   25 #>  [6,]    1    6    1    6   36 #>  [7,]    1    7    1    7   49 #>  [8,]    1    8    1    8   64 #>  [9,]    1    9    1    9   81 #> [10,]    1   10    1   10  100 #>   # Clear list: my_bl_list$clearRegisteredFactories()  # Get number of registered factories: my_bl_list$getNumberOfRegisteredFactories() #> [1] 0"},{"path":"https://schalkdaniel.github.io/compboost/reference/CategoricalDataRaw.html","id":null,"dir":"Reference","previous_headings":"","what":"Data class for categorical variables — CategoricalDataRaw","title":"Data class for categorical variables — CategoricalDataRaw","text":"CategoricalDataRaw creates data object can used source object instantiate categorical base learner.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/CategoricalDataRaw.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Data class for categorical variables — CategoricalDataRaw","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/CategoricalDataRaw.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Data class for categorical variables — CategoricalDataRaw","text":"x (character()) Categorical vector. data_identifier (character(1)) Data id, e.g. feature name.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/CategoricalDataRaw.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data class for categorical variables — CategoricalDataRaw","text":"","code":"CategoricalDataRaw$new(x, data_identifier)"},{"path":"https://schalkdaniel.github.io/compboost/reference/CategoricalDataRaw.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Data class for categorical variables — CategoricalDataRaw","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/CategoricalDataRaw.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Data class for categorical variables — CategoricalDataRaw","text":"$getData(): () -> stop() Throws error representation calculated. $getRawData(): () -> character() $getIdentifier(): () -> character(1)","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/CategoricalDataRaw.html","id":"inherited-methods-from-data","dir":"Reference","previous_headings":"","what":"Inherited methods from Data","title":"Data class for categorical variables — CategoricalDataRaw","text":"$getDataType(): () -> character(1)","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/CategoricalDataRaw.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Data class for categorical variables — CategoricalDataRaw","text":"","code":"# Sample data: x = sample(c(\"one\",\"two\", \"three\"), 20, TRUE)  # Create new data object: data_obj = CategoricalDataRaw$new(x, \"cat_raw\")  # Get data and identifier: data_obj$getRawData() #>  [1] \"one\"   \"two\"   \"three\" \"three\" \"three\" \"three\" \"one\"   \"one\"   \"three\" #> [10] \"one\"   \"one\"   \"three\" \"one\"   \"three\" \"three\" \"one\"   \"one\"   \"three\" #> [19] \"one\"   \"two\"   data_obj$getIdentifier() #> [1] \"cat_raw\""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":null,"dir":"Reference","previous_headings":"","what":"Component-wise boosting — Compboost","title":"Component-wise boosting — Compboost","text":"Fit component-wise boosting model (Buehlmann (2003)). class wraps S4 class system Compboost_internal internal model representation exposed Rcpp. two convenient wrapper boostLinear() boostSplines() also creating objects class. Visualizing internals see plotBaselearnerTraces(), plotBaselearner(), plotFeatureImportance(), plotPEUni(), plotTensor(), plotRisk(). Visualizing contribution one new observation see plotIndividualContribution().","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Component-wise boosting — Compboost","text":"Buehlmann, Peter, Yu, Bin (2003). “Boosting L2 loss: regression classification.” Journal American Statistical Association, 98(462), 324--339. doi:10.1198/016214503000125 .","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Component-wise boosting — Compboost","text":"data (data.frame()) data used training model. Note: oob_fraction set, input data split data data_oob. Hence, data contains subset input data train model. data_oob (data.frame()) --bag data set used risk logging early stopping. data_oob split input data (see data field). oob_fraction (numeric(1)) fraction nrow(input data) defining number observations data_oob. response (ResponseRegr | ResponseBinaryClassif) S4 response object. See ?ResponseRegr ?ResponseBinaryClassif help. object holds current prediction, pseudo residuals functions transform scores. Note: response corresponds data field holds predictions data.frame. response_oob (ResponseRegr | ResponseBinaryClassif) S4 response object. See ?ResponseRegr ?ResponseBinaryClassif help. response data_oob. target (character(1)) Name target variable data. id (character(1)) Name data object defined $new(data, ...). optimizer (OptimizerCoordinateDescent | OptimizerCoordinateDescentLineSearch | OptimizerAGBM | OptimizerCosineAnnealing) initialized S4 optimizer object (requires call Optimizer*$new(..). See respective help page information. loss (LossQuadratic | LossBinomial | LossHuber | LossAbsolute | LossQuantile) initialized S4 loss object (requires call Loss*$new(...)). See respective help page information. learning_rate (numeric(1)) learning rate model. Note: optimizer dynamically vary learning rate. model (Compboost_internal) internal Compboost object exported Rcpp. See ?Compboost_internal details. bl_factory_list ([BlearnerFactoryList) container base learners. See ?BlearnerFactoryList details. positive (character(1)) positive class case binary classification. stop_all (logical(1)) Indicator whether stopper must return TRUE early stop algorithm. Comparable () stop_all = TRUE () stop_all = FALSE. early_stop (logical(1)) Indicator whether early stopping used .","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"active-bindings","dir":"Reference","previous_headings":"","what":"Active bindings","title":"Component-wise boosting — Compboost","text":"offset (numeric()) Offset estimated model. baselearner_list (list()) Named list names $getBaselearnerNames(). elements contains \"feature\" (character(1)): name feature data. \"factory\" (Baselearner*): raw base learner  factoryobject. See ?Baselearner* details. boost_intercept (logical(1)) Logical value indicating whether intercept base learner added $addIntercept() . logs (data.frame) Basic information risk, selected base learner etc. iteration. oob_data set, information validation/oob risk also logged. applies time logging etc. Note: Using field logs internally set updated call $getLoggerData(). Hence, cashes logged data set instead recalculating data set done $getLoggerData(). idx_oob (integer()) index vector used split data data = data[idx_train, ] data_oob = data[idx_oob, ]. Note: oob_fraction ignored argument set. idx_train (integer()) index vector used split data data = data[idx_train, ] data_oob = data[idx_oob, ]. Note: oob_fraction ignored argument set.","code":""},{"path":[]},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Component-wise boosting — Compboost","text":"Compboost$new() Compboost$addLogger() Compboost$getCurrentIteration() Compboost$addIntercept() Compboost$addBaselearner() Compboost$rmBaselearner() Compboost$addTensor() Compboost$addComponents() Compboost$train() Compboost$prepareData() Compboost$prepareResponse() Compboost$predict() Compboost$predictIndividual() Compboost$transformData() Compboost$getInbagRisk() Compboost$getSelectedBaselearner() Compboost$print() Compboost$getCoef() Compboost$getEstimatedCoef() Compboost$getBaselearnerNames() Compboost$getLoggerData() Compboost$calculateFeatureImportance() Compboost$saveToJson() Compboost$clone()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Component-wise boosting — Compboost","text":"Creates new instance R6 class.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$new(   data = NULL,   target = NULL,   optimizer = NULL,   loss = NULL,   learning_rate = 0.05,   positive = NULL,   oob_fraction = NULL,   early_stop = FALSE,   idx_oob = NULL,   stop_args = list(eps_for_break = 0, patience = 10L),   file = NULL )"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"data (data.frame) data set build object. Note: data set completely used training .null(idx_oob). Otherwise, data set split data = data[idx_train, ] data_oob = data[idx_oob, ]. target (character(1)) Character indicating name target variable. optimizer (OptimizerCoordinateDescent | OptimizerCoordinateDescentLineSearch | OptimizerAGBM | OptimizerCosineAnnealing) initialized S4 optimizer object (requires call Optimizer*.new(..). See respective help page information. loss (LossQuadratic | LossBinomial | LossHuber | LossAbsolute | LossQuantile) initialized S4 loss object (requires call Loss*$new(...)). See respective help page information. learning_rate (numeric(1)) Learning rate model (default 0.05). positive (character(1)) name positive class (case binary classification). oob_fraction (numeric(1)) fraction nrow(input data) defining number observations data_oob. argument ignored idx_oob set. early_stop (logical(1)) Indicator whether early stopping used . idx_oob (integer()) index vector used split data data = data[idx_train, ] data_oob = data[idx_oob, ]. Note: oob_fraction ignored argument set. stop_args (list(integer(1), integer(1)))list containing two elements patience eps_for_break used early stopping. file (character(1) File model loaded. NULL, data target must defined.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-addlogger-","dir":"Reference","previous_headings":"","what":"Method addLogger()","title":"Component-wise boosting — Compboost","text":"Add logger model.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$addLogger(logger, use_as_stopper = FALSE, logger_id, ...)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"logger (LoggerIteration | LoggerTime | LoggerInbagRisk | LoggerOobRisk) uninitialized logger. use_as_stopper (logical(1)) Indicator defining logger stopper considering early stopping. logger_id (character(1)) id logger. allows define two logger type (e.g. risk logging) different arguments. ... Additional arguments passed loger$new(logger_id, use_as_stopper, ...).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-getcurrentiteration-","dir":"Reference","previous_headings":"","what":"Method getCurrentIteration()","title":"Component-wise boosting — Compboost","text":"Get number current iteration.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$getCurrentIteration()"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"integer(1) value.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-addintercept-","dir":"Reference","previous_headings":"","what":"Method addIntercept()","title":"Component-wise boosting — Compboost","text":"functions adds base learner adjusts intercept (selected). Adding intercept base learner may necessary, e.g., adding linear effects without intercept.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$addIntercept(id = \"intercept\", data_source = InMemoryData)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"id (character(1)) id base learner (default \"intercept\"). data_source (InMemoryData) Uninitialized data object used store meta data. Note: moment, just memory storing supported, see ?InMemorydata details. data_source (InMemoryData) Uninitialized data object used store meta data. Note: moment, just memory storing supported, see ?InMemorydata details.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-addbaselearner-","dir":"Reference","previous_headings":"","what":"Method addBaselearner()","title":"Component-wise boosting — Compboost","text":"Add base learner one feature model considered iteration. Using $addBaselearner() just allows including univariate features. See $addTensor() bivariate effect modelling $addComponents() effect decomposition.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$addBaselearner(   feature,   id,   bl_factory,   data_source = InMemoryData,   ... )"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"feature (character(1)) Name feature, must column data. feature (character(1)) Name feature, must column data. id (character(1)) name base learner. bl_factory (BaselearnerPolynomial | BaselearnerPSpline | BaselearnerCategoricalBinary | BaselearnerCategoricalRidge) Uninitialized base learner class. See respective help page details. data_source (InMemoryData) Uninitialized data object used store meta data. Note: moment, just memory storing supported, see ?InMemorydata details. data_source (InMemoryData) Uninitialized data object used store meta data. Note: moment, just memory storing supported, see ?InMemorydata details. ... argument spassed $new(...) constructor bl_factory.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-rmbaselearner-","dir":"Reference","previous_headings":"","what":"Method rmBaselearner()","title":"Component-wise boosting — Compboost","text":"Remove base learner model.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$rmBaselearner(blname)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"blname (character(1)) Name base learner removed. Must element $getBaselearnerNames().","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-addtensor-","dir":"Reference","previous_headings":"","what":"Method addTensor()","title":"Component-wise boosting — Compboost","text":"Add row-wise tensor product features. Note: base learner pre-defined type feature. Numerical features uses BaselearnerPSpline categorical features included using BaselearnerCategoricalRidge base learner. include arbitrary tensor product requires use S4 API using BaselearnerTensor two base learners type.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$addTensor(   feature1,   feature2,   df = NULL,   df1 = NULL,   df2 = NULL,   isotrop = FALSE,   ... )"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"feature1 (character(1)) Name first feature. Must element names(data). feature2 (character(1)) Name second feature. Must element names(data). df (numeric(1)) degrees freedom used base learner (parameter overwrites df1 df2). df1 (numeric(1)) degrees freedom used first base learner. df2 (numeric(1)) degrees freedom used first base learner. isotrop (logical(1)) Indicator two penalties combined, isotrop == TRUE, total degrees freedom uniformly distributed dimensions isotrop == FALSE allows define strong two dimensions penalized. ... Additional arguments passed $new() constructor BaselearnerPSpline class.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-addcomponents-","dir":"Reference","previous_headings":"","what":"Method addComponents()","title":"Component-wise boosting — Compboost","text":"Add effect individual components. linear term added well non-linear term without linear effect. ensures linear component selected prior non-linear effect. non-linear effect included deviation linear effect required. Note: Internally, BaselearnerPolynomial degree one BaselearnerCentered added. Centering base learner makes design matrix dense hence memory filled fast. Considering binning may option reduce memory consumption.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$addComponents(feature, ...)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"feature (character(1)) Name feature, must column data. feature (character(1)) Name feature, must column data. ... Additional arguments passed $new() constructor BaselearnerPSpline class.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-train-","dir":"Reference","previous_headings":"","what":"Method train()","title":"Component-wise boosting — Compboost","text":"Start fitting model.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$train(iteration = 100, trace = -1)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"iteration (integer(1)) maximal number iteration. algorithm can stopped earlier early stopping active. trace (integer(1)) number integers status fitting printed screen. default trace = -1 internally uses trace = round(iteration / 40). silently fit model use trace = 0.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-preparedata-","dir":"Reference","previous_headings":"","what":"Method prepareData()","title":"Component-wise boosting — Compboost","text":"Internally, base learner build InMemoryData object. methods (e.g. adding LoggerOobRisk) requires pass data list(InMemoryData | CategoricalDataRaw) data objects elements. function converts given data.frame format.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$prepareData(newdata)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"newdata (data.frame) New data set structure data. newdata (data.frame) New data set structure data. newdata (data.frame) New data set structure data. newdata (data.frame) New data set structure data.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"list(InMemoryData | CategoricalDataRaw) data container elements. Numeric features wrapped InMemoryData categorical features included CategoricalDataRaw.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-prepareresponse-","dir":"Reference","previous_headings":"","what":"Method prepareResponse()","title":"Component-wise boosting — Compboost","text":"$prepareData() response. Internally, vectorToResponse() used generate ResponseRegr ResponseBinaryClassif object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$prepareResponse(response)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"response (vector()) vector type numberic categorical transformed response object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"ResponseRegr | ResponseBinaryClassif object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-predict-","dir":"Reference","previous_headings":"","what":"Method predict()","title":"Component-wise boosting — Compboost","text":"Calculate predictions.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$predict(newdata = NULL, as_response = FALSE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"newdata (data.frame) New data set structure data. newdata (data.frame) New data set structure data. newdata (data.frame) New data set structure data. newdata (data.frame) New data set structure data. as_response (logical(1)) case binary classification, as_response = TRUE returns predictions response, .e. classes.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"Vector predictions.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-predictindividual-","dir":"Reference","previous_headings":"","what":"Method predictIndividual()","title":"Component-wise boosting — Compboost","text":"$predict() returns sum base learner predictions, function returns list predictions base learner.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$predictIndividual(newdata)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-11","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"newdata (data.frame) New data set structure data. newdata (data.frame) New data set structure data. newdata (data.frame) New data set structure data. newdata (data.frame) New data set structure data.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"Named list() included base learner names names base learner predictions elements.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-transformdata-","dir":"Reference","previous_headings":"","what":"Method transformData()","title":"Component-wise boosting — Compboost","text":"Get design matrices (subset) base learners new data.frame.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$transformData(newdata, blnames = NULL)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-12","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"newdata (data.frame) New data set structure data. newdata (data.frame) New data set structure data. newdata (data.frame) New data set structure data. newdata (data.frame) New data set structure data. blnames (character()) Names base learners design matrices returned. .null(blnames), compboost tries guess base learners constructed based feature names newdata.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"list(matrix | Matrix::Matrix) matrices elements.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-getinbagrisk-","dir":"Reference","previous_headings":"","what":"Method getInbagRisk()","title":"Component-wise boosting — Compboost","text":"Return training risk iteration.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$getInbagRisk()"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"numeric() vector risk values NULL $train() called previously.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-getselectedbaselearner-","dir":"Reference","previous_headings":"","what":"Method getSelectedBaselearner()","title":"Component-wise boosting — Compboost","text":"Get vector name selected base learner iteration.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$getSelectedBaselearner()"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"character() vector base learner names.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-print-","dir":"Reference","previous_headings":"","what":"Method print()","title":"Component-wise boosting — Compboost","text":"Printer object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$print()"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"Invisibly returns object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-getcoef-","dir":"Reference","previous_headings":"","what":"Method getCoef()","title":"Component-wise boosting — Compboost","text":"Get estimated coefficients.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-17","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$getCoef()"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"list(pars, offset) estimated coefficients/parameters intercept/offset.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-getestimatedcoef-","dir":"Reference","previous_headings":"","what":"Method getEstimatedCoef()","title":"Component-wise boosting — Compboost","text":"DEPRICATED use $getCoef() instead. Get estimated coefficients.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-18","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$getEstimatedCoef()"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-10","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"list(pars, offset) estimated coefficients/parameters intercept/offset.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-getbaselearnernames-","dir":"Reference","previous_headings":"","what":"Method getBaselearnerNames()","title":"Component-wise boosting — Compboost","text":"Get names registered base learners.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-19","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$getBaselearnerNames()"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-11","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"charcter() base learner names.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-getloggerdata-","dir":"Reference","previous_headings":"","what":"Method getLoggerData()","title":"Component-wise boosting — Compboost","text":"Get logged information.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-20","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$getLoggerData()"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-12","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"data.frame logging information.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-calculatefeatureimportance-","dir":"Reference","previous_headings":"","what":"Method calculateFeatureImportance()","title":"Component-wise boosting — Compboost","text":"Calculate feature important based training risk. Note early stopping used get adequate importance measures.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-21","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$calculateFeatureImportance(   num_feats = NULL,   aggregate_bl_by_feat = FALSE )"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-13","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"num_feats (integer(1)) number considered features, num_feats important feature names respective value returned. num_feats = NULL, features considered. aggregate_bl_by_feat (logical(1)) Indicator whether importance aggregated based feature level. example, adding components included two different base learners feature. aggregate_bl_by_feat == TRUE, importance two base learners aggregated instead considering individually.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"returns-13","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting — Compboost","text":"Named numeric() vector length num_feats (least num_feats selected) importance values elements.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-savetojson-","dir":"Reference","previous_headings":"","what":"Method saveToJson()","title":"Component-wise boosting — Compboost","text":"Save Compboost object JSON file. underlying C++ objects, possible use R's native load save methods.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-22","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$saveToJson(file, rm_data = FALSE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-14","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"file (character(1)) Name/path file. rm_data (logical(1)) Remove data model. applies training data, response, well test data response used test risk logging. Note: data removed, continuation training possible reloading. Also, everything related predictions based training data throws error.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Component-wise boosting — Compboost","text":"objects class cloneable method.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"usage-23","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting — Compboost","text":"","code":"Compboost$clone(deep = FALSE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"arguments-15","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting — Compboost","text":"deep Whether make deep clone.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Component-wise boosting — Compboost","text":"","code":"cboost = Compboost$new(mtcars, \"mpg\", loss = LossQuadratic$new(), oob_fraction = 0.3) cboost$addBaselearner(\"hp\", \"spline\", BaselearnerPSpline, degree = 3,   n_knots = 10, df = 3, differences = 2) cboost$addBaselearner(\"wt\", \"spline\", BaselearnerPSpline) cboost$train(1000, 0) #> Train 1000 iterations in 0 Seconds. #> Final risk based on the train set: 0.53 #>   table(cboost$getSelectedBaselearner()) #>  #> hp_spline wt_spline  #>       497       503  head(cboost$logs) #>   _iterations oob_risk baselearner train_risk #> 1           0       NA   intercept  14.651002 #> 2           1 22.45430   wt_spline  13.486880 #> 3           2 20.21669   wt_spline  12.434072 #> 4           3 18.23173   wt_spline  11.481820 #> 5           4 16.47340   wt_spline  10.620408 #> 6           5 14.91826   wt_spline   9.841063 names(cboost$baselearner_list) #> [1] \"hp_spline\" \"wt_spline\"  # Access information about the a base learner in the list: cboost$baselearner_list$hp_spline$factory$getDF() #>      [,1] #> [1,]    3 cboost$baselearner_list$hp_spline$factory$getPenalty() #>         [,1] #> [1,] 67.1566 plotBaselearner(cboost, \"hp_spline\")"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost_internal.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal Compboost Class\n\nThis class is the raw C++ pendant and still at a very high-level.\nIt is the base for the Compboost R6 class and provides\nmany convenient wrapper to access data and execute methods by calling\nthe C++ methods. — Compboost_internal","title":"Internal Compboost Class\n\nThis class is the raw C++ pendant and still at a very high-level.\nIt is the base for the Compboost R6 class and provides\nmany convenient wrapper to access data and execute methods by calling\nthe C++ methods. — Compboost_internal","text":"Internal Compboost Class class raw C++ pendant still high-level. base Compboost R6 class provides many convenient wrapper access data execute methods calling C++ methods.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost_internal.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Internal Compboost Class\n\nThis class is the raw C++ pendant and still at a very high-level.\nIt is the base for the Compboost R6 class and provides\nmany convenient wrapper to access data and execute methods by calling\nthe C++ methods. — Compboost_internal","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost_internal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal Compboost Class\n\nThis class is the raw C++ pendant and still at a very high-level.\nIt is the base for the Compboost R6 class and provides\nmany convenient wrapper to access data and execute methods by calling\nthe C++ methods. — Compboost_internal","text":"oob_response (ResponseRegr | ResponseBinaryClassif) response object containing target variable. learning_rate (numeric(1)) learning rate. stop_if_all_stopper_fulfilled (logical(1)) Boolean indicate stopping strategy used. TRUE, algorithm stops conditions loggers stopping apply. factory_list (BlearnerFactoryList) List base learner factories one base learner selected iteration using loss (LossQuadratic | LossBinomial | LossHuber | LossAbsolute | LossQuantile) initialized S4 loss object (requires call Loss*$new(...)). See respective help page information. logger_list (LoggerList) LoggerList object loggers. optimizer (OptimizerCoordinateDescent | OptimizerCoordinateDescentLineSearch | OptimizerAGBM | OptimizerCosineAnnealing) initialized S4 optimizer object (requires call Optimizer*.new(..). See respective help page information.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost_internal.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal Compboost Class\n\nThis class is the raw C++ pendant and still at a very high-level.\nIt is the base for the Compboost R6 class and provides\nmany convenient wrapper to access data and execute methods by calling\nthe C++ methods. — Compboost_internal","text":"","code":"Compboost$new(response, learning_rate, stop_if_all_stopper_fulfilled,   factory_list, loss, logger_list, optimizer)"},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost_internal.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Internal Compboost Class\n\nThis class is the raw C++ pendant and still at a very high-level.\nIt is the base for the Compboost R6 class and provides\nmany convenient wrapper to access data and execute methods by calling\nthe C++ methods. — Compboost_internal","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost_internal.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Internal Compboost Class\n\nThis class is the raw C++ pendant and still at a very high-level.\nIt is the base for the Compboost R6 class and provides\nmany convenient wrapper to access data and execute methods by calling\nthe C++ methods. — Compboost_internal","text":"$train(): () -> () $continueTraining(): () -> () $getLearningRate(): () -> numeric(1) $getPrediction(): () -> matrix() $getSelectedBaselearner(): () -> character() $getLoggerData(): () -> list(character(), matrix()) $getEstimatedParameter(): () -> list(matrix()) $getParameterAtIteration(): () -> list(matrix()) $getParameterMatrix(): () -> matrix() $predictFactoryTrainData(): () -> matrix() $predictFactoryNewData(): list(Data*) -> matrix() $predictIndividualTrainData(): () -> list(matrix()) Get linear contribution base learner training data. $predictIndividual(): list(Data*) -> list(matrix()) Get linear contribution base learner new data. $predict(): list(Data*), logical(1) -> matrix() $summarizeCompboost(): () -> () $isTrained(): () -> logical(1) $setToIteration(): () -> () $saveJson(): () -> () $getOffset(): () -> numeric(1) | matrix() $getRiskVector(): () -> numeric() $getResponse(): () -> Response* $getOptimizer(): () -> Optimizer* $getLoss(): () -> Loss* $getLoggerList(): () -> LoggerList $getBaselearnerList(): () -> BlearnerFactoryList $useGlobalStopping(): () -> logical(1)* $getFactoryMap(): () -> list(Baselearner*) $getDataMap(): () -> list(Data*)","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/Compboost_internal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Internal Compboost Class\n\nThis class is the raw C++ pendant and still at a very high-level.\nIt is the base for the Compboost R6 class and provides\nmany convenient wrapper to access data and execute methods by calling\nthe C++ methods. — Compboost_internal","text":"","code":"# Some data: df = mtcars df$mpg_cat = ifelse(df$mpg > 20, \"high\", \"low\")  # # Create new variable to check the polynomial base learner with degree 2: # df$hp2 = df[[\"hp\"]]^2  # Data for the baselearner are matrices: X_hp = as.matrix(df[[\"hp\"]]) X_wt = as.matrix(df[[\"wt\"]])  # Target variable: response = ResponseBinaryClassif$new(\"mpg_cat\", \"high\", df[[\"mpg_cat\"]])  data_source_hp = InMemoryData$new(X_hp, \"hp\") data_source_wt = InMemoryData$new(X_wt, \"wt\")  # List for oob logging: oob_data = list(data_source_hp, data_source_wt)  # List to test prediction on newdata: test_data = oob_data  # Factories: linear_factory_hp = BaselearnerPolynomial$new(data_source_hp,   list(degree = 1, intercept = TRUE)) linear_factory_wt = BaselearnerPolynomial$new(data_source_wt,   list(degree = 1, intercept = TRUE)) quadratic_factory_hp = BaselearnerPolynomial$new(data_source_hp,   list(degree = 2, intercept = TRUE)) spline_factory_wt = BaselearnerPSpline$new(data_source_wt,   list(degree = 3, n_knots = 10, penalty = 2, differences = 2))  # Create new factory list: factory_list = BlearnerFactoryList$new()  # Register factories: factory_list$registerFactory(linear_factory_hp) factory_list$registerFactory(linear_factory_wt) factory_list$registerFactory(quadratic_factory_hp) factory_list$registerFactory(spline_factory_wt)  # Define loss: loss_bin = LossBinomial$new()  # Define optimizer: optimizer = OptimizerCoordinateDescent$new()  ## Logger  # Define logger. We want just the iterations as stopper but also track the # time, inbag risk and oob risk: log_iterations  = LoggerIteration$new(\" iteration_logger\", TRUE, 500) log_time        = LoggerTime$new(\"time_logger\", FALSE, 500, \"microseconds\")  # Define new logger list: logger_list = LoggerList$new()  # Register the logger: logger_list$registerLogger(log_iterations) logger_list$registerLogger(log_time)  # Run compboost: # --------------  # Initialize object: cboost = Compboost_internal$new(   response      = response,   learning_rate = 0.05,   stop_if_all_stopper_fulfilled = FALSE,   factory_list = factory_list,   loss         = loss_bin,   logger_list  = logger_list,   optimizer    = optimizer )  # Train the model (we want to print the trace): cboost$train(trace = 50) #>   1/500   risk = 0.68  time_logger = 0    #>  50/500   risk = 0.42  time_logger = 558    #> 100/500   risk = 0.31  time_logger = 1175    #> 150/500   risk = 0.25  time_logger = 1868    #> 200/500   risk = 0.21  time_logger = 2678    #> 250/500   risk = 0.19  time_logger = 3506    #> 300/500   risk = 0.17  time_logger = 4460    #> 350/500   risk = 0.16  time_logger = 5521    #> 400/500   risk = 0.15  time_logger = 6718    #> 450/500   risk = 0.14  time_logger = 7969    #> 500/500   risk = 0.13  time_logger = 9348    #>  #>  #> Train 500 iterations in 0 Seconds. #> Final risk based on the train set: 0.13 #>  cboost #>  #> Compboost object with: #> \t- Learning Rate: 0.05 #> \t- Are all logger used as stopper: 0 #> \t- Model is already trained with 500 iterations/fitted baselearner #> \t- Actual state is at iteration 500 #>  #>  #>   # Get estimated parameter: cboost$getEstimatedParameter() #> $hp_poly2 #>               [,1] #> [1,]  6.0660349854 #> [2,] -0.0645193461 #> [3,]  0.0001267875 #>  #> $wt_spline_degree_3 #>             [,1] #>  [1,]  1.9370515 #>  [2,]  1.8485620 #>  [3,]  1.8081122 #>  [4,]  1.8647942 #>  [5,]  1.7149991 #>  [6,]  1.0827258 #>  [7,] -0.4203328 #>  [8,] -1.8130157 #>  [9,] -2.0641036 #> [10,] -2.0050272 #> [11,] -1.8182042 #> [12,] -1.5569128 #> [13,] -1.2730290 #> [14,] -0.9904981 #>   # Get trace of selected base learner: cboost$getSelectedBaselearner() #>   [1] \"wt_spline_degree_3\" \"wt_spline_degree_3\" \"wt_spline_degree_3\" #>   [4] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #>   [7] \"hp_poly2\"           \"wt_spline_degree_3\" \"wt_spline_degree_3\" #>  [10] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #>  [13] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #>  [16] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #>  [19] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #>  [22] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #>  [25] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #>  [28] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #>  [31] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #>  [34] \"wt_spline_degree_3\" \"wt_spline_degree_3\" \"hp_poly2\"           #>  [37] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #>  [40] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #>  [43] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #>  [46] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #>  [49] \"wt_spline_degree_3\" \"wt_spline_degree_3\" \"hp_poly2\"           #>  [52] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #>  [55] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #>  [58] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #>  [61] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #>  [64] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #>  [67] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #>  [70] \"wt_spline_degree_3\" \"wt_spline_degree_3\" \"hp_poly2\"           #>  [73] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #>  [76] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #>  [79] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #>  [82] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #>  [85] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #>  [88] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #>  [91] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #>  [94] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #>  [97] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [100] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [103] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [106] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [109] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [112] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [115] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [118] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [121] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [124] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [127] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [130] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [133] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [136] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [139] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [142] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [145] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [148] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [151] \"wt_spline_degree_3\" \"hp_poly2\"           \"hp_poly2\"           #> [154] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [157] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [160] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [163] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [166] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [169] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [172] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [175] \"hp_poly2\"           \"hp_poly2\"           \"wt_spline_degree_3\" #> [178] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [181] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [184] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [187] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [190] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [193] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [196] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [199] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [202] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [205] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [208] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [211] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [214] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [217] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [220] \"hp_poly2\"           \"hp_poly2\"           \"wt_spline_degree_3\" #> [223] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [226] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [229] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [232] \"wt_spline_degree_3\" \"hp_poly2\"           \"hp_poly2\"           #> [235] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [238] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [241] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [244] \"hp_poly2\"           \"hp_poly2\"           \"wt_spline_degree_3\" #> [247] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [250] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [253] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [256] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [259] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [262] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [265] \"wt_spline_degree_3\" \"hp_poly2\"           \"hp_poly2\"           #> [268] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [271] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [274] \"wt_spline_degree_3\" \"hp_poly2\"           \"hp_poly2\"           #> [277] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [280] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [283] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [286] \"hp_poly2\"           \"hp_poly2\"           \"wt_spline_degree_3\" #> [289] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [292] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [295] \"hp_poly2\"           \"hp_poly2\"           \"wt_spline_degree_3\" #> [298] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [301] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [304] \"hp_poly2\"           \"hp_poly2\"           \"wt_spline_degree_3\" #> [307] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [310] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [313] \"hp_poly2\"           \"hp_poly2\"           \"wt_spline_degree_3\" #> [316] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [319] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [322] \"hp_poly2\"           \"hp_poly2\"           \"wt_spline_degree_3\" #> [325] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [328] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [331] \"hp_poly2\"           \"hp_poly2\"           \"wt_spline_degree_3\" #> [334] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [337] \"wt_spline_degree_3\" \"hp_poly2\"           \"hp_poly2\"           #> [340] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [343] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [346] \"wt_spline_degree_3\" \"hp_poly2\"           \"hp_poly2\"           #> [349] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [352] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [355] \"wt_spline_degree_3\" \"hp_poly2\"           \"hp_poly2\"           #> [358] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [361] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [364] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [367] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [370] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [373] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [376] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [379] \"hp_poly2\"           \"hp_poly2\"           \"wt_spline_degree_3\" #> [382] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [385] \"wt_spline_degree_3\" \"hp_poly2\"           \"hp_poly2\"           #> [388] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [391] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [394] \"wt_spline_degree_3\" \"hp_poly2\"           \"hp_poly2\"           #> [397] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [400] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [403] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [406] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [409] \"hp_poly2\"           \"hp_poly2\"           \"wt_spline_degree_3\" #> [412] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [415] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [418] \"hp_poly2\"           \"hp_poly2\"           \"wt_spline_degree_3\" #> [421] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [424] \"wt_spline_degree_3\" \"hp_poly2\"           \"hp_poly2\"           #> [427] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [430] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [433] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [436] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [439] \"hp_poly2\"           \"hp_poly2\"           \"wt_spline_degree_3\" #> [442] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [445] \"wt_spline_degree_3\" \"hp_poly2\"           \"hp_poly2\"           #> [448] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [451] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [454] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [457] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [460] \"hp_poly2\"           \"hp_poly2\"           \"wt_spline_degree_3\" #> [463] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [466] \"wt_spline_degree_3\" \"hp_poly2\"           \"hp_poly2\"           #> [469] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [472] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [475] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [478] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [481] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [484] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [487] \"wt_spline_degree_3\" \"hp_poly2\"           \"wt_spline_degree_3\" #> [490] \"hp_poly2\"           \"hp_poly2\"           \"wt_spline_degree_3\" #> [493] \"hp_poly2\"           \"wt_spline_degree_3\" \"hp_poly2\"           #> [496] \"wt_spline_degree_3\" \"hp_poly2\"           \"hp_poly2\"           #> [499] \"wt_spline_degree_3\" \"hp_poly2\"            # Set to iteration 200: cboost$setToIteration(200, 30)  # Get new parameter values: cboost$getEstimatedParameter() #> $hp_poly2 #>               [,1] #> [1,]  3.649127e+00 #> [2,] -3.820112e-02 #> [3,]  7.376905e-05 #>  #> $wt_spline_degree_3 #>             [,1] #>  [1,]  1.3479210 #>  [2,]  1.3418629 #>  [3,]  1.3528398 #>  [4,]  1.3714923 #>  [5,]  1.2096167 #>  [6,]  0.7153452 #>  [7,] -0.2712226 #>  [8,] -1.1840205 #>  [9,] -1.4335470 #> [10,] -1.4475909 #> [11,] -1.3450002 #> [12,] -1.1727850 #> [13,] -0.9769323 #> [14,] -0.7809478 #>"},{"path":"https://schalkdaniel.github.io/compboost/reference/InMemoryData.html","id":null,"dir":"Reference","previous_headings":"","what":"Store data in RAM — InMemoryData","title":"Store data in RAM — InMemoryData","text":"data container stores vector RAM makes accessible Compboost.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/InMemoryData.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Store data in RAM — InMemoryData","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/InMemoryData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Store data in RAM — InMemoryData","text":"data_mat (matrix()) data matrix. data_identifier (character(1)) Data id, e.g. feature name.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/InMemoryData.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Store data in RAM — InMemoryData","text":"","code":"InMemoryData$new() InMemoryData$new(data_mat, data_identifier) InMemoryData$new(data_mat, data_identifier, use_sparse)"},{"path":"https://schalkdaniel.github.io/compboost/reference/InMemoryData.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Store data in RAM — InMemoryData","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/InMemoryData.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Store data in RAM — InMemoryData","text":"$getData(): () -> matrix() $getIdentifier(): () -> character(1)","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/InMemoryData.html","id":"inherited-methods-from-data","dir":"Reference","previous_headings":"","what":"Inherited methods from Data","title":"Store data in RAM — InMemoryData","text":"$getDataType(): () -> character(1)","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/InMemoryData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Store data in RAM — InMemoryData","text":"","code":"# Sample data: data_mat = cbind(rnorm(10))  # Create new data object: data_obj = InMemoryData$new(data_mat, \"my_data_name\")  # Get data and identifier: data_obj$getData() #>             [,1] #>  [1,]  2.5662912 #>  [2,] -0.4618534 #>  [3,] -2.8675352 #>  [4,] -0.6225646 #>  [5,] -0.6603442 #>  [6,]  0.7647128 #>  [7,]  0.6706849 #>  [8,]  1.4133255 #>  [9,] -0.5433604 #> [10,] -1.6106776 data_obj$getIdentifier() #> [1] \"my_data_name\""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerInbagRisk.html","id":null,"dir":"Reference","previous_headings":"","what":"Log the train risk. — LoggerInbagRisk","title":"Log the train risk. — LoggerInbagRisk","text":"class logs train risk specific loss function.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerInbagRisk.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Log the train risk. — LoggerInbagRisk","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerInbagRisk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log the train risk. — LoggerInbagRisk","text":"logger_id (character(1)) Identifier logger. param-use_as_stopper (logical(1)) Boolean indicate logger also used stopper. loss (LossQuadratic | LossBinomial | LossHuber | LossAbsolute | LossQuantile) initialized S4 loss object (requires call Loss*$new(...)). See respective help page information. eps_for_break (numeric(1)) argument becomes active loss also used stopper. relative improvement logged inbag risk falls boundary stopper returns TRUE.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerInbagRisk.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log the train risk. — LoggerInbagRisk","text":"","code":"LoggerInbagRisk$new(logger_id, use_as_stopper, loss, eps_for_break, patience)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerInbagRisk.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Log the train risk. — LoggerInbagRisk","text":"logger computes risk training data \\(\\mathcal{D} = \\{(x^{()},\\ y^{()})\\ |\\ \\\\{1, \\dots, n\\}\\}\\) stores vector. empirical risk \\(\\mathcal{R}_\\mathrm{emp}\\) iteration \\(m\\) calculated : $$   \\mathcal{R}_\\mathrm{emp}^{[m]} = \\frac{1}{n}\\sum\\limits_{= 1}^n L(y^{()}, \\hat{f}^{[m]}(x^{()})) $$ Note: \\(m=0\\) \\(\\hat{f}\\) just offset. implementation calculate \\(\\mathcal{R}_\\mathrm{emp}^{[m]}\\) done two steps: Calculate vector risk_temp losses every observation given response \\(y^{()}\\) prediction \\(\\hat{f}^{[m]}(x^{()})\\). Average risk_temp. procedure ensures, possible e.g. use AUC arbitrary performance measure risk logging. gives just one value risk_temp therefore average equals loss function. just value (like AUC) value returned.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerInbagRisk.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Log the train risk. — LoggerInbagRisk","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerInbagRisk.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Log the train risk. — LoggerInbagRisk","text":"$summarizeLogger(): () -> ()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerInbagRisk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Log the train risk. — LoggerInbagRisk","text":"","code":"# Used loss: log_bin = LossBinomial$new()  # Define logger: log_inbag_risk = LoggerInbagRisk$new(\"inbag\", FALSE, log_bin, 0.05, 5)  # Summarize logger: log_inbag_risk$summarizeLogger() #> Inbag risk logger: #> \t- Use logger as stopper: 0"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerIteration.html","id":null,"dir":"Reference","previous_headings":"","what":"Logger class to log the current iteration — LoggerIteration","title":"Logger class to log the current iteration — LoggerIteration","text":"Logger class log current iteration","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerIteration.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Logger class to log the current iteration — LoggerIteration","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerIteration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logger class to log the current iteration — LoggerIteration","text":"logger_id (character(1)) Identifier logger. param-use_as_stopper (logical(1)) Boolean indicate logger also used stopper. max_iterations (integer(1)) logger used stopper argument defines maximal iterations.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerIteration.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Logger class to log the current iteration — LoggerIteration","text":"","code":"LoggerIterationWrapper$new(logger_id, use_as_stopper, max_iterations)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerIteration.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Logger class to log the current iteration — LoggerIteration","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerIteration.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Logger class to log the current iteration — LoggerIteration","text":"$summarizeLogger(): () -> ()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerIteration.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Logger class to log the current iteration — LoggerIteration","text":"","code":"# Define logger: log_iters = LoggerIteration$new(\"iterations\", FALSE, 100)  # Summarize logger: log_iters$summarizeLogger() #> Iteration logger: #> \t- Maximal iterations: 100 #> \t- Use logger as stopper: 0"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerList.html","id":null,"dir":"Reference","previous_headings":"","what":"Collect loggers — LoggerList","title":"Collect loggers — LoggerList","text":"class collects loggers used algorithm takes care stopping strategies tracing.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerList.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Collect loggers — LoggerList","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerList.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Collect loggers — LoggerList","text":"","code":"LoggerList$new()"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerList.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Collect loggers — LoggerList","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerList.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Collect loggers — LoggerList","text":"$registerLogger(): Logger* -> () $printRegisteredLogger(): () -> () $clearRegisteredLogger(): () -> () $getNumberOfRegisteredLogger(): () -> integer(1) $getNamesOfRegisteredLogger(): () -> character() $isStopper(): () -> logical()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerList.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Collect loggers — LoggerList","text":"","code":"# Define logger: log_iters = LoggerIteration$new(\"iteration\", TRUE, 100) log_time = LoggerTime$new(\"time\", FALSE, 20, \"minutes\")  # Create logger list: logger_list = LoggerList$new()  # Register new loggeR: logger_list$registerLogger(log_iters) logger_list$registerLogger(log_time)  # Print registered logger: logger_list$printRegisteredLogger() #> Registered Logger: #> \t>>iteration<< Logger #> \t>>time<< Logger  # Remove all logger: logger_list$clearRegisteredLogger()  # Get number of registered logger: logger_list$getNumberOfRegisteredLogger() #> [1] 0"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerOobRisk.html","id":null,"dir":"Reference","previous_headings":"","what":"Log the validation/test/out-of-bag risk — LoggerOobRisk","title":"Log the validation/test/out-of-bag risk — LoggerOobRisk","text":"class logs bag risk specific loss function.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerOobRisk.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Log the validation/test/out-of-bag risk — LoggerOobRisk","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerOobRisk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log the validation/test/out-of-bag risk — LoggerOobRisk","text":"logger_id (character(1)) Identifier logger. param-use_as_stopper (logical(1)) Boolean indicate logger also used stopper. loss (LossQuadratic | LossBinomial | LossHuber | LossAbsolute | LossQuantile) initialized S4 loss object (requires call Loss*$new(...)). See respective help page information. eps_for_break (numeric(1)) argument used loss also used stopper. relative improvement logged inbag risk falls boundary stopper returns TRUE. oob_data (list()) list contains data source objects corresponds source data registered factory. source data objects contain bag data. data used calculate prediction step. oob_response (ResponseRegr | ResponseBinaryClassif) response object used predictions validation data.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerOobRisk.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log the validation/test/out-of-bag risk — LoggerOobRisk","text":"","code":"LoggerOobRisk$new(logger_id, use_as_stopper, loss, eps_for_break,   patience, oob_data, oob_response)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerOobRisk.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Log the validation/test/out-of-bag risk — LoggerOobRisk","text":"logger computes risk given new dataset \\(\\mathcal{D}_\\mathrm{oob} = \\{(x^{()},\\ y^{()})\\ |\\ \\I_\\mathrm{oob}\\}\\) stores vector. OOB risk \\(\\mathcal{R}_\\mathrm{oob}\\) iteration \\(m\\) calculated : $$   \\mathcal{R}_\\mathrm{oob}^{[m]} = \\frac{1}{|\\mathcal{D}_\\mathrm{oob}|}\\sum\\limits_{(x,y) \\\\mathcal{D}_\\mathrm{oob}}   L(y, \\hat{f}^{[m]}(x)) $$ Note: \\(m=0\\) \\(\\hat{f}\\) just offset. implementation calculate \\(\\mathcal{R}_\\mathrm{emp}^{[m]}\\) done two steps: Calculate vector risk_temp losses every observation given response \\(y^{()}\\) prediction \\(\\hat{f}^{[m]}(x^{()})\\). Average risk_temp. procedure ensures, possible e.g. use AUC arbitrary performance measure risk logging. gives just one value \\(risk_temp\\) therefore average equals loss function. just value (like AUC) value returned.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerOobRisk.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Log the validation/test/out-of-bag risk — LoggerOobRisk","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerOobRisk.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Log the validation/test/out-of-bag risk — LoggerOobRisk","text":"$summarizeLogger(): () -> ()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerOobRisk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Log the validation/test/out-of-bag risk — LoggerOobRisk","text":"","code":"# Define data: X1 = cbind(1:10) X2 = cbind(10:1) data_source1 = InMemoryData$new(X1, \"x1\") data_source2 = InMemoryData$new(X2, \"x2\")  oob_list = list(data_source1, data_source2)  set.seed(123) y_oob = rnorm(10)  # Used loss: log_bin = LossBinomial$new()  # Define response object of oob data: oob_response = ResponseRegr$new(\"oob_response\", as.matrix(y_oob))  # Define logger: log_oob_risk = LoggerOobRisk$new(\"oob\", FALSE, log_bin, 0.05, 5, oob_list, oob_response)  # Summarize logger: log_oob_risk$summarizeLogger() #> Out of bag risk logger: #> \t- Epsilon used to stop algorithm: 2.19882e+199 #> \t- Use logger as stopper: 115"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerTime.html","id":null,"dir":"Reference","previous_headings":"","what":"Log the runtime — LoggerTime","title":"Log the runtime — LoggerTime","text":"class logs runtime algorithm. logger also can used stop algorithm defined time budget. available time units : minutes seconds microseconds","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerTime.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Log the runtime — LoggerTime","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerTime.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log the runtime — LoggerTime","text":"logger_id (character(1)) Identifier logger. param-use_as_stopper (logical(1)) Boolean indicate logger also used stopper. max_time (integer(1)) logger used stopper argument contains maximal time available train model. time_unit (character(1)) unit time measured. Choices minutes, seconds microseconds.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerTime.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log the runtime — LoggerTime","text":"","code":"LoggerTime$new(logger_id, use_as_stopper, max_time, time_unit)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerTime.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Log the runtime — LoggerTime","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerTime.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Log the runtime — LoggerTime","text":"$summarizeLogger(): () -> ()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LoggerTime.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Log the runtime — LoggerTime","text":"","code":"# Define logger: log_time = LoggerTime$new(\"time_minutes\", FALSE, 20, \"minutes\")  # Summarize logger: log_time$summarizeLogger() #> Time logger: #> \t- Tracked time unit: minutes"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossAbsolute.html","id":null,"dir":"Reference","previous_headings":"","what":"Absolute loss for regression tasks. — LossAbsolute","title":"Absolute loss for regression tasks. — LossAbsolute","text":"loss can used regression \\(y \\\\mathrm{R}\\).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossAbsolute.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Absolute loss for regression tasks. — LossAbsolute","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossAbsolute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Absolute loss for regression tasks. — LossAbsolute","text":"offset (numeric(1) | matrix()) Numerical value matrix set custom offset. used, value returned instead loss optimal initialization.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossAbsolute.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Absolute loss for regression tasks. — LossAbsolute","text":"Loss Function: $$   L(y, f(x)) = | y - f(x)| $$ Gradient: $$   \\frac{\\delta}{\\delta f(x)}\\ L(y, f(x)) = -\\mathrm{sign}(y - f(x)) $$ Initialization: $$   \\hat{f}^{[0]}(x) = \\mathrm{arg~min}_{c\\R}\\ \\frac{1}{n}\\sum\\limits_{=1}^n   L(y^{()}, c) = \\mathrm{median}(y) $$","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossAbsolute.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Absolute loss for regression tasks. — LossAbsolute","text":"","code":"LossAbsolute$new() LossAbsolute$new(offset)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossAbsolute.html","id":"inherited-methods-from-loss","dir":"Reference","previous_headings":"","what":"Inherited methods from Loss","title":"Absolute loss for regression tasks. — LossAbsolute","text":"$loss(): matrix(), matrix() -> matrix() $gradient(): matrix(), matrix() -> matrix() $constInit(): matrix() -> matrix() $calculatePseudoResiduals(): matrix(), matrix() -> matrix() $getLossType(): () -> character(1)","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossAbsolute.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Absolute loss for regression tasks. — LossAbsolute","text":"","code":"# Create new loss object: absolute_loss = LossAbsolute$new() absolute_loss #> LossAbsolute: L(y,x) = |y - f(x)| #>"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossBinomial.html","id":null,"dir":"Reference","previous_headings":"","what":"0-1 Loss for binary classification derived of the binomial distribution — LossBinomial","title":"0-1 Loss for binary classification derived of the binomial distribution — LossBinomial","text":"loss can used binary classification. coding chosen acts \\(y \\\\{-1, 1\\}\\).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossBinomial.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"0-1 Loss for binary classification derived of the binomial distribution — LossBinomial","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossBinomial.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"0-1 Loss for binary classification derived of the binomial distribution — LossBinomial","text":"offset (numeric(1) | matrix()) Numerical value matrix set custom offset. used, value returned instead loss optimal initialization.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossBinomial.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"0-1 Loss for binary classification derived of the binomial distribution — LossBinomial","text":"Loss Function: $$   L(y, f(x)) = \\log(1 + \\mathrm{exp}(-2yf(x))) $$ Gradient: $$   \\frac{\\delta}{\\delta f(x)}\\ L(y, f(x)) = - \\frac{y}{1 + \\mathrm{exp}(2yf)} $$ Initialization: $$   \\hat{f}^{[0]}(x) = \\frac{1}{2}\\mathrm{log}(p / (1 - p)) $$ $$   p = \\frac{1}{n}\\sum\\limits_{=1}^n\\mathrm{1}_{\\{y^{()} = 1\\}} $$","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossBinomial.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"0-1 Loss for binary classification derived of the binomial distribution — LossBinomial","text":"","code":"LossBinomial$new() LossBinomial$new(offset)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossBinomial.html","id":"inherited-methods-from-loss","dir":"Reference","previous_headings":"","what":"Inherited methods from Loss","title":"0-1 Loss for binary classification derived of the binomial distribution — LossBinomial","text":"$loss(): matrix(), matrix() -> matrix() $gradient(): matrix(), matrix() -> matrix() $constInit(): matrix() -> matrix() $calculatePseudoResiduals(): matrix(), matrix() -> matrix() $getLossType(): () -> character(1)","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossBinomial.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"0-1 Loss for binary classification derived of the binomial distribution — LossBinomial","text":"","code":"# Create new loss object: bin_loss = LossBinomial$new() bin_loss #> LossBinomial: L(y,x) = log(1 + exp(-2yf(x)) #>"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustom.html","id":null,"dir":"Reference","previous_headings":"","what":"Create LossCustom by using R functions. — LossCustom","title":"Create LossCustom by using R functions. — LossCustom","text":"LossCustom creates custom loss using Rcpp::Function set R functions.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustom.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Create LossCustom by using R functions. — LossCustom","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustom.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create LossCustom by using R functions. — LossCustom","text":"lossFun (function)R function calculate loss. gradientFun (function)R function calculate gradient. initFun (function)R function calculate constant initialization.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustom.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create LossCustom by using R functions. — LossCustom","text":"","code":"LossCustom$new(lossFun, gradientFun, initFun)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustom.html","id":"inherited-methods-from-loss","dir":"Reference","previous_headings":"","what":"Inherited methods from Loss","title":"Create LossCustom by using R functions. — LossCustom","text":"$loss(): matrix(), matrix() -> matrix() $gradient(): matrix(), matrix() -> matrix() $constInit(): matrix() -> matrix() $calculatePseudoResiduals(): matrix(), matrix() -> matrix() $getLossType(): () -> character(1)","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustom.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create LossCustom by using R functions. — LossCustom","text":"functions must following structure: lossFun(truth, prediction) { ... return (loss) } vector argument truth containing real values vector predictions prediction. function must return vector containing loss component. gradientFun(truth, prediction) { ... return (grad) } vector argument truth containing real values vector predictions prediction. function must return vector containing gradient loss component. initFun(truth) { ... return (init) } vector argument truth containing real values. function must return numeric value containing offset constant initialization.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossCustom.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create LossCustom by using R functions. — LossCustom","text":"","code":"# Loss function: myLoss = function (true_values, prediction) {   return (0.5 * (true_values - prediction)^2) } # Gradient of loss function: myGradient = function (true_values, prediction) {   return (prediction - true_values) } # Constant initialization: myConstInit = function (true_values) {   return (mean(true_values)) }  # Create new custom quadratic loss: my_loss = LossCustom$new(myLoss, myGradient, myConstInit)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossHuber.html","id":null,"dir":"Reference","previous_headings":"","what":"Huber loss for regression tasks. — LossHuber","title":"Huber loss for regression tasks. — LossHuber","text":"loss can used regression \\(y \\\\mathrm{R}\\).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossHuber.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Huber loss for regression tasks. — LossHuber","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossHuber.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Huber loss for regression tasks. — LossHuber","text":"offset (numeric(1) | matrix()) Numerical value matrix set custom offset. used, value returned instead loss optimal initialization. delta (numeric(1)) Numerical value greater 0 specify interval around 0 quadratic error measuring (default delta = 1).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossHuber.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Huber loss for regression tasks. — LossHuber","text":"Loss Function: $$   L(y, f(x)) = 0.5(y - f(x))^2 \\ \\ \\mathrm{} \\ \\ |y - f(x)| < d $$ $$   L(y, f(x)) = d|y - f(x)| - 0.5d^2 \\ \\ \\mathrm{otherwise} $$ Gradient: $$   \\frac{\\delta}{\\delta f(x)}\\ L(y, f(x)) = f(x) - y \\ \\ \\mathrm{} \\ \\ |y - f(x)| < d $$ $$   \\frac{\\delta}{\\delta f(x)}\\ L(y, f(x)) = -d\\mathrm{sign}(y - f(x)) \\ \\ \\mathrm{otherwise} $$","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossHuber.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Huber loss for regression tasks. — LossHuber","text":"","code":"LossHuber$new() LossHuber$new(delta) LossHuber$new(offset, delta)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossHuber.html","id":"inherited-methods-from-loss","dir":"Reference","previous_headings":"","what":"Inherited methods from Loss","title":"Huber loss for regression tasks. — LossHuber","text":"$loss(): matrix(), matrix() -> matrix() $gradient(): matrix(), matrix() -> matrix() $constInit(): matrix() -> matrix() $calculatePseudoResiduals(): matrix(), matrix() -> matrix() $getLossType(): () -> character(1)","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossHuber.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Huber loss for regression tasks. — LossHuber","text":"","code":"# Create new loss object: huber_loss = LossHuber$new() huber_loss #> LossHuber: L(y,x) = if (y - f(x) < d) { 0.5(y - f(x))^2 } else { d|y - f(x)| - 0.5d^2 } #>  #>   with delta d = 1"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuadratic.html","id":null,"dir":"Reference","previous_headings":"","what":"Quadratic loss for regression tasks. — LossQuadratic","title":"Quadratic loss for regression tasks. — LossQuadratic","text":"loss can used regression \\(y \\\\mathrm{R}\\).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuadratic.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Quadratic loss for regression tasks. — LossQuadratic","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuadratic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quadratic loss for regression tasks. — LossQuadratic","text":"offset (numeric(1) | matrix()) Numerical value matrix set custom offset. used, value returned instead loss optimal initialization.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuadratic.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Quadratic loss for regression tasks. — LossQuadratic","text":"Loss Function: $$   L(y, f(x)) = \\frac{1}{2}( y - f(x))^2 $$ Gradient: $$   \\frac{\\delta}{\\delta f(x)}\\ L(y, f(x)) = f(x) - y $$ Initialization: $$   \\hat{f}^{[0]}(x) = \\mathrm{arg~min}{c\\\\mathrm{R}}{\\mathrm{arg~min}}\\ \\frac{1}{n}\\sum\\limits_{=1}^n   L\\left(y^{()}, c\\right) = \\bar{y} $$","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuadratic.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quadratic loss for regression tasks. — LossQuadratic","text":"","code":"LossQuadratic$new() LossQuadratic$new(offset)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuadratic.html","id":"inherited-methods-from-loss","dir":"Reference","previous_headings":"","what":"Inherited methods from Loss","title":"Quadratic loss for regression tasks. — LossQuadratic","text":"$loss(): matrix(), matrix() -> matrix() $gradient(): matrix(), matrix() -> matrix() $constInit(): matrix() -> matrix() $calculatePseudoResiduals(): matrix(), matrix() -> matrix() $getLossType(): () -> character(1)","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuadratic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Quadratic loss for regression tasks. — LossQuadratic","text":"","code":"# Create new loss object: quadratic_loss = LossQuadratic$new() quadratic_loss #> LossQuadratic: L(y,x) = 0.5 * (y - f(x))^2 #>"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuantile.html","id":null,"dir":"Reference","previous_headings":"","what":"Quantile loss for regression tasks. — LossQuantile","title":"Quantile loss for regression tasks. — LossQuantile","text":"loss can used regression \\(y \\\\mathrm{R}\\).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuantile.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Quantile loss for regression tasks. — LossQuantile","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuantile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quantile loss for regression tasks. — LossQuantile","text":"offset (numeric(1) | matrix()) Numerical value matrix set custom offset. used, value returned instead loss optimal initialization. quantile (numeric(1)) Numerical value 0 1 defines quantile modeled.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuantile.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Quantile loss for regression tasks. — LossQuantile","text":"Loss Function: $$   L(y, f(x)) = h| y - f(x)| $$ Gradient: $$   \\frac{\\delta}{\\delta f(x)}\\ L(y, f(x)) = -h\\mathrm{sign}( y - f(x)) $$ Initialization: $$   \\hat{f}^{[0]}(x) = \\mathrm{arg~min}_{c\\R}\\ \\frac{1}{n}\\sum\\limits_{=1}^n   L(y^{()}, c) = \\mathrm{quantile}(y, q) $$","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuantile.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quantile loss for regression tasks. — LossQuantile","text":"","code":"LossAbsolute$new() LossAbsolute$new(quantile) LossAbsolute$new(offset, quantile)"},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuantile.html","id":"inherited-methods-from-loss","dir":"Reference","previous_headings":"","what":"Inherited methods from Loss","title":"Quantile loss for regression tasks. — LossQuantile","text":"$loss(): matrix(), matrix() -> matrix() $gradient(): matrix(), matrix() -> matrix() $constInit(): matrix() -> matrix() $calculatePseudoResiduals(): matrix(), matrix() -> matrix() $getLossType(): () -> character(1)","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/LossQuantile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Quantile loss for regression tasks. — LossQuantile","text":"","code":"# Create new loss object: quadratic_loss = LossQuadratic$new() quadratic_loss #> LossQuadratic: L(y,x) = 0.5 * (y - f(x))^2 #>"},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerAGBM.html","id":null,"dir":"Reference","previous_headings":"","what":"Nesterovs momentum — OptimizerAGBM","title":"Nesterovs momentum — OptimizerAGBM","text":"class defines new object conduct Nesterovs momentum function space.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerAGBM.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Nesterovs momentum — OptimizerAGBM","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerAGBM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Nesterovs momentum — OptimizerAGBM","text":"ncores (integer(1)) Number cores base learner fitting distributed. momentum (numeric(1)) Momentum term used accelerate fitting process. chosen large, algorithm trains faster also tends overfit faster.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerAGBM.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Nesterovs momentum — OptimizerAGBM","text":"","code":"OptimizerAGBM$new(momentum) OptimizerAGBM$new(momentum, ncores)"},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerAGBM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Nesterovs momentum — OptimizerAGBM","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerAGBM.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Nesterovs momentum — OptimizerAGBM","text":"$getOptimizerType(): () -> character(1) $getStepSize(): () -> numeric() $getMomentumParameter(): () -> numeric(1) $getSelectedMomentumBaselearner(): () -> character() $getParameterMatrix(): () -> list(matrix() $getErrorCorrectedPseudoResiduals(): () -> matrix()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerAGBM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Nesterovs momentum — OptimizerAGBM","text":"","code":"optimizer = OptimizerAGBM$new(0.1)"},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescent.html","id":null,"dir":"Reference","previous_headings":"","what":"Coordinate descent — OptimizerCoordinateDescent","title":"Coordinate descent — OptimizerCoordinateDescent","text":"class defines new object conduct gradient descent function space. component-wise structure, like block-wise coordinate descent.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescent.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Coordinate descent — OptimizerCoordinateDescent","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescent.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coordinate descent — OptimizerCoordinateDescent","text":"ncores (integer(1)) Number cores base learner fitting distributed.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescent.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coordinate descent — OptimizerCoordinateDescent","text":"","code":"OptimizerCoordinateDescent$new() OptimizerCoordinateDescent$new(ncores)"},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescent.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Coordinate descent — OptimizerCoordinateDescent","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescent.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Coordinate descent — OptimizerCoordinateDescent","text":"$getOptimizerType(): () -> character(1) $getStepSize(): () -> numeric()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescent.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Coordinate descent — OptimizerCoordinateDescent","text":"","code":"# Define optimizer: optimizer = OptimizerCoordinateDescent$new()"},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescentLineSearch.html","id":null,"dir":"Reference","previous_headings":"","what":"Coordinate descent with line search — OptimizerCoordinateDescentLineSearch","title":"Coordinate descent with line search — OptimizerCoordinateDescentLineSearch","text":"OptimizerCoordinateDescent line search iteration.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescentLineSearch.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Coordinate descent with line search — OptimizerCoordinateDescentLineSearch","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescentLineSearch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coordinate descent with line search — OptimizerCoordinateDescentLineSearch","text":"ncores (integer(1)) Number cores base learner fitting distributed.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescentLineSearch.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coordinate descent with line search — OptimizerCoordinateDescentLineSearch","text":"","code":"OptimizerCoordinateDescentLineSearch$new() OptimizerCoordinateDescentLineSearch$new(ncores)"},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescentLineSearch.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Coordinate descent with line search — OptimizerCoordinateDescentLineSearch","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescentLineSearch.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Coordinate descent with line search — OptimizerCoordinateDescentLineSearch","text":"$getOptimizerType(): () -> character(1) $getStepSize(): () -> numeric()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCoordinateDescentLineSearch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Coordinate descent with line search — OptimizerCoordinateDescentLineSearch","text":"","code":"# Define optimizer: optimizer = OptimizerCoordinateDescentLineSearch$new()"},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCosineAnnealing.html","id":null,"dir":"Reference","previous_headings":"","what":"Coordinate descent with cosine annealing — OptimizerCosineAnnealing","title":"Coordinate descent with cosine annealing — OptimizerCosineAnnealing","text":"OptimizerCoordinateDescent cosine annealing scheduler adjust learning rate fitting process.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCosineAnnealing.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Coordinate descent with cosine annealing — OptimizerCosineAnnealing","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCosineAnnealing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coordinate descent with cosine annealing — OptimizerCosineAnnealing","text":"ncores (integer(1)) Number cores base learner fitting distributed. nu_min (numeric(1)) Minimal learning rate. nu_max (numeric(1)) Maximal learning rate. cycles (integer(1)) Number annealing cycles form nu_max nu_min 1 anneal_anneal_iter_max. anneal_iter_max (integer(1))\\cr Maximal number iterations annealing conducted. nu_minis used fixed learning rate afteranneal_iter_max`.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCosineAnnealing.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coordinate descent with cosine annealing — OptimizerCosineAnnealing","text":"","code":"OptimizerCosineAnnealing$new() OptimizerCosineAnnealing$new(ncores) OptimizerCosineAnnealing$new(nu_min, nu_max, cycles, anneal_iter_max, cycles) OptimizerCosineAnnealing$new(nu_min, nu_max, cycles, anneal_iter_max, cycles, ncores)"},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCosineAnnealing.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Coordinate descent with cosine annealing — OptimizerCosineAnnealing","text":"class contain public fields.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCosineAnnealing.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Coordinate descent with cosine annealing — OptimizerCosineAnnealing","text":"$getOptimizerType(): () -> character(1) $getStepSize(): () -> numeric()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/OptimizerCosineAnnealing.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Coordinate descent with cosine annealing — OptimizerCosineAnnealing","text":"","code":"# Define optimizer: optimizer = OptimizerCosineAnnealing$new()"},{"path":"https://schalkdaniel.github.io/compboost/reference/ResponseBinaryClassif.html","id":null,"dir":"Reference","previous_headings":"","what":"Create response object for binary classification. — ResponseBinaryClassif","title":"Create response object for binary classification. — ResponseBinaryClassif","text":"ResponseBinaryClassif creates response object used target fitting process.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/ResponseBinaryClassif.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Create response object for binary classification. — ResponseBinaryClassif","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/ResponseBinaryClassif.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create response object for binary classification. — ResponseBinaryClassif","text":"","code":"ResponseBinaryClassif$new(target_name, pos_class, response) ResponseBinaryClassif$new(target_name, pos_class, response, weights)"},{"path":"https://schalkdaniel.github.io/compboost/reference/ResponseBinaryClassif.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create response object for binary classification. — ResponseBinaryClassif","text":"","code":"response_binary = ResponseBinaryClassif$new(\"target\", \"A\", sample(c(\"A\", \"B\"), 10, TRUE)) response_binary$getResponse() #>       [,1] #>  [1,]    1 #>  [2,]   -1 #>  [3,]    1 #>  [4,]    1 #>  [5,]    1 #>  [6,]    1 #>  [7,]   -1 #>  [8,]   -1 #>  [9,]    1 #> [10,]   -1 response_binary$getPrediction() #>       [,1] #>  [1,]    0 #>  [2,]    0 #>  [3,]    0 #>  [4,]    0 #>  [5,]    0 #>  [6,]    0 #>  [7,]    0 #>  [8,]    0 #>  [9,]    0 #> [10,]    0 response_binary$getPredictionTransform() # Applies sigmoid to prediction scores #>       [,1] #>  [1,]  0.5 #>  [2,]  0.5 #>  [3,]  0.5 #>  [4,]  0.5 #>  [5,]  0.5 #>  [6,]  0.5 #>  [7,]  0.5 #>  [8,]  0.5 #>  [9,]  0.5 #> [10,]  0.5 response_binary$getPredictionResponse()  # Categorizes depending on the transformed predictions #>       [,1] #>  [1,]    1 #>  [2,]    1 #>  [3,]    1 #>  [4,]    1 #>  [5,]    1 #>  [6,]    1 #>  [7,]    1 #>  [8,]    1 #>  [9,]    1 #> [10,]    1 response_binary$getTargetName() #> [1] \"target\" response_binary$setThreshold(0.7) response_binary$getThreshold() #> [1] 0.7 response_binary$getPositiveClass() #> [1] \"A\""},{"path":"https://schalkdaniel.github.io/compboost/reference/ResponseRegr.html","id":null,"dir":"Reference","previous_headings":"","what":"Create response object for regression. — ResponseRegr","title":"Create response object for regression. — ResponseRegr","text":"ResponseRegr creates response object used target fitting process.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/ResponseRegr.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Create response object for regression. — ResponseRegr","text":"S4 object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/ResponseRegr.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create response object for regression. — ResponseRegr","text":"","code":"ResponseRegr$new(target_name, response) ResponseRegr$new(target_name, response, weights)"},{"path":"https://schalkdaniel.github.io/compboost/reference/ResponseRegr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create response object for regression. — ResponseRegr","text":"","code":"response_regr = ResponseRegr$new(\"target\", cbind(rnorm(10))) response_regr$getResponse() #>             [,1] #>  [1,]  1.7869131 #>  [2,]  0.4978505 #>  [3,] -1.9666172 #>  [4,]  0.7013559 #>  [5,] -0.4727914 #>  [6,] -1.0678237 #>  [7,] -0.2179749 #>  [8,] -1.0260044 #>  [9,] -0.7288912 #> [10,] -0.6250393 response_regr$getTargetName() #> [1] \"target\""},{"path":"https://schalkdaniel.github.io/compboost/reference/boostLinear.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper to boost linear models for each feature. — boostLinear","title":"Wrapper to boost linear models for each feature. — boostLinear","text":"wrapper function automatically initializes model adding numerical features linear base-learner. Categorical features dummy encoded inserted using another linear base-learners without intercept. function boostLinear also train model. returned object object Compboost class. object can used analyses (see ?Compboost details).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/boostLinear.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper to boost linear models for each feature. — boostLinear","text":"","code":"boostLinear(   data,   target,   optimizer = NULL,   loss = NULL,   learning_rate = 0.05,   iterations = 100,   trace = -1,   intercept = TRUE,   data_source = InMemoryData,   oob_fraction = NULL )"},{"path":"https://schalkdaniel.github.io/compboost/reference/boostLinear.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper to boost linear models for each feature. — boostLinear","text":"data (data.frame) data frame containing data. target (character(1) | ResponseRegr | ResponseBinaryClassif) Character value containing target variable response object. Note loss must match data type target. optimizer (OptimizerCoordinateDescent | OptimizerCoordinateDescentLineSearch | OptimizerAGBM | OptimizerCosineAnnealing) initialized S4 optimizer object (requires call Optimizer*.new(..). See respective help page information. loss (LossQuadratic | LossBinomial | LossHuber | LossAbsolute | LossQuantile) initialized S4 loss object (requires call Loss*$new(...)). See respective help page information. learning_rate (numeric(1)) Learning rate shrink parameter step. iterations (integer(1)) Number iterations trained. iterations == 0, untrained object returned. can useful base learners (e.g. interaction via tensor base learner) added. trace (integer(1)) Integer indicating often trace printed. Specifying trace = 10, every 10th iteration printed. trace printed set trace = 0. Default -1 means total 40 iterations printed. intercept (logical(1)) Internally used BaselearnerPolynomial. logical value indicates feature get intercept (default TRUE). data_source (Data*) Uninitialized Data* object used store data. moment just memory training supported. oob_fraction (numeric(1)) Fraction much data used track bag risk.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/boostLinear.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper to boost linear models for each feature. — boostLinear","text":"model Compboost class. model R6 object can used retraining, predicting, plotting, anything described ?Compboost.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/boostLinear.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wrapper to boost linear models for each feature. — boostLinear","text":"","code":"mod = boostLinear(data = iris, target = \"Sepal.Length\", loss = LossQuadratic$new(),   oob_fraction = 0.3) #>   1/100   risk = 0.32  oob_risk = 0.29    #>   2/100   risk = 0.3  oob_risk = 0.27    #>   4/100   risk = 0.26  oob_risk = 0.23    #>   6/100   risk = 0.23  oob_risk = 0.2    #>   8/100   risk = 0.2  oob_risk = 0.18    #>  10/100   risk = 0.18  oob_risk = 0.16    #>  12/100   risk = 0.16  oob_risk = 0.14    #>  14/100   risk = 0.15  oob_risk = 0.13    #>  16/100   risk = 0.14  oob_risk = 0.12    #>  18/100   risk = 0.13  oob_risk = 0.11    #>  20/100   risk = 0.12  oob_risk = 0.11    #>  22/100   risk = 0.11  oob_risk = 0.1    #>  24/100   risk = 0.11  oob_risk = 0.096    #>  26/100   risk = 0.1  oob_risk = 0.093    #>  28/100   risk = 0.099  oob_risk = 0.09    #>  30/100   risk = 0.097  oob_risk = 0.088    #>  32/100   risk = 0.094  oob_risk = 0.086    #>  34/100   risk = 0.092  oob_risk = 0.084    #>  36/100   risk = 0.091  oob_risk = 0.082    #>  38/100   risk = 0.089  oob_risk = 0.08    #>  40/100   risk = 0.087  oob_risk = 0.078    #>  42/100   risk = 0.086  oob_risk = 0.076    #>  44/100   risk = 0.084  oob_risk = 0.075    #>  46/100   risk = 0.083  oob_risk = 0.073    #>  48/100   risk = 0.081  oob_risk = 0.071    #>  50/100   risk = 0.08  oob_risk = 0.07    #>  52/100   risk = 0.079  oob_risk = 0.069    #>  54/100   risk = 0.078  oob_risk = 0.067    #>  56/100   risk = 0.077  oob_risk = 0.066    #>  58/100   risk = 0.076  oob_risk = 0.065    #>  60/100   risk = 0.075  oob_risk = 0.064    #>  62/100   risk = 0.074  oob_risk = 0.063    #>  64/100   risk = 0.073  oob_risk = 0.062    #>  66/100   risk = 0.073  oob_risk = 0.061    #>  68/100   risk = 0.072  oob_risk = 0.06    #>  70/100   risk = 0.071  oob_risk = 0.06    #>  72/100   risk = 0.07  oob_risk = 0.059    #>  74/100   risk = 0.07  oob_risk = 0.058    #>  76/100   risk = 0.069  oob_risk = 0.057    #>  78/100   risk = 0.069  oob_risk = 0.057    #>  80/100   risk = 0.068  oob_risk = 0.056    #>  82/100   risk = 0.068  oob_risk = 0.056    #>  84/100   risk = 0.067  oob_risk = 0.055    #>  86/100   risk = 0.067  oob_risk = 0.055    #>  88/100   risk = 0.066  oob_risk = 0.054    #>  90/100   risk = 0.066  oob_risk = 0.054    #>  92/100   risk = 0.066  oob_risk = 0.053    #>  94/100   risk = 0.065  oob_risk = 0.053    #>  96/100   risk = 0.065  oob_risk = 0.053    #>  98/100   risk = 0.065  oob_risk = 0.052    #> 100/100   risk = 0.064  oob_risk = 0.052    #>  #>  #> Train 100 iterations in 0 Seconds. #> Final risk based on the train set: 0.064 #>  mod$getBaselearnerNames() #> [1] \"Sepal.Width_linear\"  \"Petal.Length_linear\" \"Petal.Width_linear\"  #> [4] \"Species_ridge\"       mod$getEstimatedCoef() #> Depricated, use `$getCoef()` instead. #> $Petal.Length_linear #>            [,1] #> [1,] -1.6141315 #> [2,]  0.4206597 #> attr(,\"blclass\") #> [1] \"Rcpp_BaselearnerPolynomial\" #>  #> $Sepal.Width_linear #>            [,1] #> [1,] -1.1423498 #> [2,]  0.3755377 #> attr(,\"blclass\") #> [1] \"Rcpp_BaselearnerPolynomial\" #>  #> $offset #> [1] 5.871429 #>  table(mod$getSelectedBaselearner()) #>  #> Petal.Length_linear  Sepal.Width_linear  #>                  66                  34  mod$predict() #>            [,1] #>   [1,] 5.018253 #>   [2,] 4.830484 #>   [3,] 4.863525 #>   [4,] 5.055806 #>   [5,] 4.980699 #>   [6,] 5.022765 #>   [7,] 4.792930 #>   [8,] 4.910104 #>   [9,] 5.135426 #>  [10,] 5.064831 #>  [11,] 4.704286 #>  [12,] 5.121890 #>  [13,] 5.126402 #>  [14,] 5.018253 #>  [15,] 5.257112 #>  [16,] 5.172980 #>  [17,] 4.887543 #>  [18,] 5.069343 #>  [19,] 4.914616 #>  [20,] 5.064831 #>  [21,] 5.060319 #>  [22,] 4.980699 #>  [23,] 5.285641 #>  [24,] 4.821459 #>  [25,] 4.976187 #>  [26,] 4.525542 #>  [27,] 4.863525 #>  [28,] 5.102385 #>  [29,] 5.341244 #>  [30,] 4.830484 #>  [31,] 5.215046 #>  [32,] 4.905591 #>  [33,] 5.135426 #>  [34,] 6.293769 #>  [35,] 5.661323 #>  [36,] 6.101488 #>  [37,] 6.059422 #>  [38,] 6.331322 #>  [39,] 5.404415 #>  [40,] 6.139041 #>  [41,] 5.338332 #>  [42,] 6.008331 #>  [43,] 6.181107 #>  [44,] 5.718382 #>  [45,] 6.130017 #>  [46,] 6.134529 #>  [47,] 5.853604 #>  [48,] 5.694364 #>  [49,] 6.335835 #>  [50,] 6.115024 #>  [51,] 6.143553 #>  [52,] 6.012843 #>  [53,] 6.185619 #>  [54,] 6.344859 #>  [55,] 6.096975 #>  [56,] 5.563654 #>  [57,] 5.572679 #>  [58,] 5.769472 #>  [59,] 6.274264 #>  [60,] 6.134529 #>  [61,] 5.829587 #>  [62,] 6.176595 #>  [63,] 5.773984 #>  [64,] 5.895670 #>  [65,] 6.008331 #>  [66,] 6.012843 #>  [67,] 5.315771 #>  [68,] 5.891158 #>  [69,] 6.878180 #>  [70,] 6.274264 #>  [71,] 6.723453 #>  [72,] 6.559701 #>  [73,] 6.681387 #>  [74,] 7.017915 #>  [75,] 5.946760 #>  [76,] 6.854163 #>  [77,] 6.493618 #>  [78,] 6.462032 #>  [79,] 6.157090 #>  [80,] 6.555189 #>  [81,] 6.993897 #>  [82,] 6.044429 #>  [83,] 6.714428 #>  [84,] 6.227685 #>  [85,] 6.984873 #>  [86,] 6.190132 #>  [87,] 6.751982 #>  [88,] 6.840626 #>  [89,] 6.185619 #>  [90,] 6.302793 #>  [91,] 6.522147 #>  [92,] 6.732477 #>  [93,] 7.234213 #>  [94,] 6.311817 #>  [95,] 6.807585 #>  [96,] 6.747470 #>  [97,] 6.592743 #>  [98,] 6.260727 #>  [99,] 6.550677 #> [100,] 6.424479 #> [101,] 6.274264 #> [102,] 6.798560 #> [103,] 6.751982 #> [104,] 6.428991 #> [105,] 6.663338"},{"path":"https://schalkdaniel.github.io/compboost/reference/boostSplines.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper to boost general additive models for each feature. — boostSplines","title":"Wrapper to boost general additive models for each feature. — boostSplines","text":"wrapper function automatically initializes model adding numerical features spline base-learner. Categorical features dummy encoded inserted using another linear base-learners without intercept. function boostSplines also train model. returned object object Compboost class. object can used analyses (see ?Compboost details).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/boostSplines.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper to boost general additive models for each feature. — boostSplines","text":"","code":"boostSplines(   data,   target,   optimizer = NULL,   loss = NULL,   learning_rate = 0.05,   iterations = 100,   trace = -1,   degree = 3,   n_knots = 20,   penalty = 2,   df = 0,   differences = 2,   data_source = InMemoryData,   oob_fraction = NULL,   bin_root = 0,   cache_type = \"inverse\",   stop_args = list(),   df_cat = 1,   stop_time = \"microseconds\",   additional_risk_logs = list() )"},{"path":"https://schalkdaniel.github.io/compboost/reference/boostSplines.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper to boost general additive models for each feature. — boostSplines","text":"data (data.frame()) data frame containing data. target (character(1) | ResponseRegr | ResponseBinaryClassif) Character value containing target variable response object. Note loss must match data type target. optimizer (OptimizerCoordinateDescent | OptimizerCoordinateDescentLineSearch | OptimizerAGBM | OptimizerCosineAnnealing) initialized S4 optimizer object (requires call Optimizer*.new(..). See respective help page information. loss (LossQuadratic | LossBinomial | LossHuber | LossAbsolute | LossQuantile) initialized S4 loss object (requires call Loss*$new(...)). See respective help page information. learning_rate (numeric(1)) Learning rate shrink parameter step. iterations (integer(1)) Number iterations trained. iterations == 0, untrained object returned. can useful base learners (e.g. interaction via tensor base learner) added. trace (integer(1)) Integer indicating often trace printed. Specifying trace = 10, every 10th iteration printed. trace printed set trace = 0. Default -1 means total 40 iterations printed. degree (integer(1))cr Polynomial degree splines. n_knots (integer(1)) Number equidistant \"inner knots\". actual number used knots also depend polynomial degree. penalty (numeric(1)) Penalty term p-splines. penalty equals 0, ordinary b-splines fitted. higher penalty, higher smoothness. df (numeric(1)) Degrees freedom base learner(s). differences (integer(1)) Number differences used penalization. higher difference, higher smoothness. data_source (Data*) Uninitialized Data* object used store data. moment just memory training supported. oob_fraction (numeric(1)) Fraction much data used track bag risk. bin_root (integer(1)) binning root reduce data \\(n^{1/\\text{binroot}}\\) data points (default bin_root = 1, means binning applied). value bin_root = 2 suggested best approximation error (cf. Wood et al. (2017) Generalized additive models gigadata: modeling UK black smoke network daily data). cache_type (character(1)) String indicate method used estimate parameter iteration. Default cache_type = \"cholesky\" computes Cholesky decomposition, caches , reuses matrix . option use cache_type = \"inverse\" caches inverse. stop_args (list(2)) List containing two elements patience eps_for_break can set use early stopping left data setting oob_fraction. df_cat (numeric(1)) Degrees freedom categorical base-learner. stop_time (character(1)) Unit measured time. additional_risk_logs (list(Logger)) Additional logger passed Compboost object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/boostSplines.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper to boost general additive models for each feature. — boostSplines","text":"model Compboost class. model R6 object can used retraining, predicting, plotting, anything described ?Compboost.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/boostSplines.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wrapper to boost general additive models for each feature. — boostSplines","text":"","code":"mod = boostSplines(data = iris, target = \"Sepal.Length\", loss = LossQuadratic$new(),   oob_fraction = 0.3) #>   1/100   risk = 0.31  oob_risk = 0.32   time = 0    #>   2/100   risk = 0.28  oob_risk = 0.29   time = 19252    #>   4/100   risk = 0.24  oob_risk = 0.26   time = 50336    #>   6/100   risk = 0.21  oob_risk = 0.23   time = 80941    #>   8/100   risk = 0.18  oob_risk = 0.2   time = 108687    #>  10/100   risk = 0.15  oob_risk = 0.18   time = 108846    #>  12/100   risk = 0.13  oob_risk = 0.17   time = 108949    #>  14/100   risk = 0.12  oob_risk = 0.15   time = 109047    #>  16/100   risk = 0.11  oob_risk = 0.14   time = 109145    #>  18/100   risk = 0.095  oob_risk = 0.13   time = 109247    #>  20/100   risk = 0.087  oob_risk = 0.13   time = 109347    #>  22/100   risk = 0.08  oob_risk = 0.12   time = 109446    #>  24/100   risk = 0.074  oob_risk = 0.11   time = 109670    #>  26/100   risk = 0.069  oob_risk = 0.11   time = 109818    #>  28/100   risk = 0.066  oob_risk = 0.11   time = 109928    #>  30/100   risk = 0.062  oob_risk = 0.11   time = 110079    #>  32/100   risk = 0.06  oob_risk = 0.11   time = 110187    #>  34/100   risk = 0.058  oob_risk = 0.1   time = 110303    #>  36/100   risk = 0.056  oob_risk = 0.1   time = 110403    #>  38/100   risk = 0.055  oob_risk = 0.1   time = 110503    #>  40/100   risk = 0.053  oob_risk = 0.1   time = 110602    #>  42/100   risk = 0.052  oob_risk = 0.1   time = 110729    #>  44/100   risk = 0.051  oob_risk = 0.1   time = 110832    #>  46/100   risk = 0.05  oob_risk = 0.1   time = 110934    #>  48/100   risk = 0.049  oob_risk = 0.1   time = 111036    #>  50/100   risk = 0.048  oob_risk = 0.1   time = 111136    #>  52/100   risk = 0.048  oob_risk = 0.1   time = 111236    #>  54/100   risk = 0.047  oob_risk = 0.1   time = 111338    #>  56/100   risk = 0.046  oob_risk = 0.1   time = 111437    #>  58/100   risk = 0.046  oob_risk = 0.1   time = 111537    #>  60/100   risk = 0.045  oob_risk = 0.099   time = 111639    #>  62/100   risk = 0.044  oob_risk = 0.099   time = 111741    #>  64/100   risk = 0.044  oob_risk = 0.099   time = 111843    #>  66/100   risk = 0.043  oob_risk = 0.099   time = 111947    #>  68/100   risk = 0.043  oob_risk = 0.099   time = 112049    #>  70/100   risk = 0.043  oob_risk = 0.099   time = 112152    #>  72/100   risk = 0.042  oob_risk = 0.099   time = 112255    #>  74/100   risk = 0.042  oob_risk = 0.098   time = 112359    #>  76/100   risk = 0.041  oob_risk = 0.098   time = 112461    #>  78/100   risk = 0.041  oob_risk = 0.098   time = 112565    #>  80/100   risk = 0.041  oob_risk = 0.098   time = 112670    #>  82/100   risk = 0.04  oob_risk = 0.098   time = 112775    #>  84/100   risk = 0.04  oob_risk = 0.098   time = 112898    #>  86/100   risk = 0.04  oob_risk = 0.098   time = 113008    #>  88/100   risk = 0.039  oob_risk = 0.098   time = 113117    #>  90/100   risk = 0.039  oob_risk = 0.097   time = 113226    #>  92/100   risk = 0.039  oob_risk = 0.097   time = 113338    #>  94/100   risk = 0.039  oob_risk = 0.097   time = 113448    #>  96/100   risk = 0.038  oob_risk = 0.097   time = 113572    #>  98/100   risk = 0.038  oob_risk = 0.097   time = 113684    #> 100/100   risk = 0.038  oob_risk = 0.097   time = 113795    #>  #>  #> Train 100 iterations in 0 Seconds. #> Final risk based on the train set: 0.038 #>  mod$getBaselearnerNames() #> [1] \"Sepal.Width_spline\"  \"Petal.Length_spline\" \"Petal.Width_spline\"  #> [4] \"Species_ridge\"       mod$getEstimatedCoef() #> Depricated, use `$getCoef()` instead. #> $Petal.Length_spline #>               [,1] #>  [1,] -1.020038063 #>  [2,] -0.842700185 #>  [3,] -0.708210074 #>  [4,] -0.616528258 #>  [5,] -0.629992257 #>  [6,] -0.674377781 #>  [7,] -0.700732067 #>  [8,] -0.697215039 #>  [9,] -0.651967365 #> [10,] -0.537788304 #> [11,] -0.341686833 #> [12,] -0.177164903 #> [13,]  0.007505898 #> [14,]  0.290089474 #> [15,]  0.451293220 #> [16,]  0.400021292 #> [17,]  0.366344295 #> [18,]  0.483695287 #> [19,]  0.784027942 #> [20,]  1.123146019 #> [21,]  1.463578011 #> [22,]  1.656349917 #> [23,]  1.738770154 #> [24,]  1.806016019 #> attr(,\"blclass\") #> [1] \"Rcpp_BaselearnerPSpline\" #>  #> $Petal.Width_spline #>               [,1] #>  [1,] -0.368363815 #>  [2,] -0.230727829 #>  [3,] -0.097569154 #>  [4,] -0.016578666 #>  [5,] -0.028837656 #>  [6,] -0.057857155 #>  [7,] -0.057668639 #>  [8,] -0.028703497 #>  [9,] -0.002385605 #> [10,] -0.010779152 #> [11,] -0.054068445 #> [12,] -0.040466735 #> [13,]  0.064127383 #> [14,]  0.136240247 #> [15,]  0.133221263 #> [16,]  0.102792184 #> [17,]  0.067773564 #> [18,]  0.110728014 #> [19,]  0.161049540 #> [20,]  0.160193965 #> [21,]  0.130363799 #> [22,]  0.046188478 #> [23,] -0.067301221 #> [24,] -0.181037323 #> attr(,\"blclass\") #> [1] \"Rcpp_BaselearnerPSpline\" #>  #> $Sepal.Width_spline #>               [,1] #>  [1,] -0.391634041 #>  [2,] -0.234533599 #>  [3,] -0.086240363 #>  [4,]  0.009214706 #>  [5,]  0.007799400 #>  [6,] -0.065038926 #>  [7,] -0.104271259 #>  [8,] -0.096040595 #>  [9,] -0.054096040 #> [10,] -0.017548114 #> [11,] -0.012499394 #> [12,]  0.013963397 #> [13,] -0.016707110 #> [14,] -0.064197029 #> [15,] -0.059409731 #> [16,]  0.010539003 #> [17,]  0.075101623 #> [18,]  0.111945966 #> [19,]  0.157947620 #> [20,]  0.253521014 #> [21,]  0.357967518 #> [22,]  0.374894173 #> [23,]  0.371380710 #> [24,]  0.369954604 #> attr(,\"blclass\") #> [1] \"Rcpp_BaselearnerPSpline\" #>  #> $offset #> [1] 5.79619 #>  table(mod$getSelectedBaselearner()) #>  #> Petal.Length_spline  Petal.Width_spline  Sepal.Width_spline  #>                  45                  26                  29  mod$predict() #>            [,1] #>   [1,] 5.001037 #>   [2,] 4.971113 #>   [3,] 4.890050 #>   [4,] 5.003350 #>   [5,] 5.410319 #>   [6,] 4.975044 #>   [7,] 4.956201 #>   [8,] 4.891916 #>   [9,] 5.129464 #>  [10,] 5.001642 #>  [11,] 4.859679 #>  [12,] 4.715600 #>  [13,] 5.228497 #>  [14,] 5.077558 #>  [15,] 5.303721 #>  [16,] 5.262642 #>  [17,] 5.223762 #>  [18,] 5.012599 #>  [19,] 5.036910 #>  [20,] 4.939170 #>  [21,] 4.994930 #>  [22,] 5.029948 #>  [23,] 5.266600 #>  [24,] 5.342117 #>  [25,] 5.003350 #>  [26,] 4.842022 #>  [27,] 4.958629 #>  [28,] 4.942359 #>  [29,] 4.928705 #>  [30,] 4.975044 #>  [31,] 5.035150 #>  [32,] 4.890050 #>  [33,] 5.127204 #>  [34,] 5.317974 #>  [35,] 5.212719 #>  [36,] 4.932458 #>  [37,] 5.129464 #>  [38,] 4.910691 #>  [39,] 6.220816 #>  [40,] 6.336589 #>  [41,] 6.109429 #>  [42,] 6.280500 #>  [43,] 5.204318 #>  [44,] 6.182060 #>  [45,] 5.668333 #>  [46,] 5.142687 #>  [47,] 6.005284 #>  [48,] 5.688583 #>  [49,] 5.457387 #>  [50,] 6.140691 #>  [51,] 5.707504 #>  [52,] 6.247146 #>  [53,] 6.262478 #>  [54,] 5.694635 #>  [55,] 6.247862 #>  [56,] 6.144328 #>  [57,] 6.276506 #>  [58,] 6.296937 #>  [59,] 5.282725 #>  [60,] 5.495523 #>  [61,] 5.459322 #>  [62,] 5.537781 #>  [63,] 6.242001 #>  [64,] 6.223752 #>  [65,] 6.067748 #>  [66,] 5.802462 #>  [67,] 5.635231 #>  [68,] 5.917087 #>  [69,] 6.273111 #>  [70,] 5.819931 #>  [71,] 5.833788 #>  [72,] 5.873289 #>  [73,] 5.015609 #>  [74,] 5.769503 #>  [75,] 6.704795 #>  [76,] 6.870084 #>  [77,] 6.445517 #>  [78,] 6.740277 #>  [79,] 7.253793 #>  [80,] 6.581417 #>  [81,] 6.285423 #>  [82,] 6.232806 #>  [83,] 6.242707 #>  [84,] 6.179686 #>  [85,] 6.267134 #>  [86,] 7.795450 #>  [87,] 7.541891 #>  [88,] 6.312717 #>  [89,] 7.576167 #>  [90,] 6.578978 #>  [91,] 6.874760 #>  [92,] 6.719281 #>  [93,] 7.603128 #>  [94,] 6.493966 #>  [95,] 6.281089 #>  [96,] 6.372074 #>  [97,] 6.376361 #>  [98,] 6.374356 #>  [99,] 6.404666 #> [100,] 6.777900 #> [101,] 6.352684 #> [102,] 6.289097 #> [103,] 6.203257 #> [104,] 6.311141 #> [105,] 6.264772"},{"path":"https://schalkdaniel.github.io/compboost/reference/getCustomCppExample.html","id":null,"dir":"Reference","previous_headings":"","what":"Get C++ example script to define a custom cpp logger — getCustomCppExample","title":"Get C++ example script to define a custom cpp logger — getCustomCppExample","text":"function can used print trace parameters trained compboost object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/getCustomCppExample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get C++ example script to define a custom cpp logger — getCustomCppExample","text":"","code":"getCustomCppExample(example = \"blearner\", silent = FALSE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/getCustomCppExample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get C++ example script to define a custom cpp logger — getCustomCppExample","text":"example (character(1)) Character value indicating example base-learner loss returned. values one blearner loss. silent (logical(1)) Logical value indicating example code printed screen .","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/getCustomCppExample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get C++ example script to define a custom cpp logger — getCustomCppExample","text":"function returns character vector can compiled using Rcpp::sourceCpp(code = getCustomCppExample()) define new custom C++ logger.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":null,"dir":"Reference","previous_headings":"","what":"Component-wise boosting learner — mlr_learners.compboost","title":"Component-wise boosting learner — mlr_learners.compboost","text":"Learner component-wise boosting model implemented Compboost package compboost.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Component-wise boosting learner — mlr_learners.compboost","text":"mlr3::Learner -> LearnerCompboost","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Component-wise boosting learner — mlr_learners.compboost","text":"mlr3::Learner$base_learner() mlr3::Learner$format() mlr3::Learner$help() mlr3::Learner$predict() mlr3::Learner$predict_newdata() mlr3::Learner$print() mlr3::Learner$reset() mlr3::Learner$train()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Component-wise boosting learner — mlr_learners.compboost","text":"LearnerCompboost$new() LearnerCompboost$importance() LearnerCompboost$selected_features() LearnerCompboost$save_json() LearnerCompboost$clone()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Component-wise boosting learner — mlr_learners.compboost","text":"Creates new instance R6 class.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting learner — mlr_learners.compboost","text":"","code":"LearnerCompboost$new(mode)"},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"method-importance-","dir":"Reference","previous_headings":"","what":"Method importance()","title":"Component-wise boosting learner — mlr_learners.compboost","text":"importance scores extracted model slot $calculateFEatureImportance().","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting learner — mlr_learners.compboost","text":"","code":"LearnerCompboost$importance()"},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting learner — mlr_learners.compboost","text":"Named numeric().","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"method-selected-features-","dir":"Reference","previous_headings":"","what":"Method selected_features()","title":"Component-wise boosting learner — mlr_learners.compboost","text":"Selected features extracted model slot $getSelectedBaselearner().","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting learner — mlr_learners.compboost","text":"","code":"LearnerCompboost$selected_features()"},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting learner — mlr_learners.compboost","text":"character().","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"method-save-json-","dir":"Reference","previous_headings":"","what":"Method save_json()","title":"Component-wise boosting learner — mlr_learners.compboost","text":"Save model JSON file.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting learner — mlr_learners.compboost","text":"","code":"LearnerCompboost$save_json(file)"},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting learner — mlr_learners.compboost","text":"file (character(1) Name/path file.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Component-wise boosting learner — mlr_learners.compboost","text":"character(1)","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Component-wise boosting learner — mlr_learners.compboost","text":"objects class cloneable method.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Component-wise boosting learner — mlr_learners.compboost","text":"","code":"LearnerCompboost$clone(deep = FALSE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners.compboost.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component-wise boosting learner — mlr_learners.compboost","text":"deep Whether make deep clone.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_classif.compboost.html","id":null,"dir":"Reference","previous_headings":"","what":"Classification component-wise boosting learner — mlr_learners_classif.compboost","title":"Classification component-wise boosting learner — mlr_learners_classif.compboost","text":"Learner component-wise boosting model implemented Compboost package compboost.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_classif.compboost.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Classification component-wise boosting learner — mlr_learners_classif.compboost","text":"mlr3::Learner -> compboost::LearnerCompboost -> LearnerClassifCompboost","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_classif.compboost.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Classification component-wise boosting learner — mlr_learners_classif.compboost","text":"mlr3::Learner$base_learner() mlr3::Learner$format() mlr3::Learner$help() mlr3::Learner$predict() mlr3::Learner$predict_newdata() mlr3::Learner$print() mlr3::Learner$reset() mlr3::Learner$train() compboost::LearnerCompboost$importance() compboost::LearnerCompboost$save_json() compboost::LearnerCompboost$selected_features()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_classif.compboost.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Classification component-wise boosting learner — mlr_learners_classif.compboost","text":"LearnerClassifCompboost$new() LearnerClassifCompboost$clone()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_classif.compboost.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Classification component-wise boosting learner — mlr_learners_classif.compboost","text":"Creates new instance R6 class.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_classif.compboost.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classification component-wise boosting learner — mlr_learners_classif.compboost","text":"","code":"LearnerClassifCompboost$new()"},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_classif.compboost.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Classification component-wise boosting learner — mlr_learners_classif.compboost","text":"objects class cloneable method.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_classif.compboost.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Classification component-wise boosting learner — mlr_learners_classif.compboost","text":"","code":"LearnerClassifCompboost$clone(deep = FALSE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_classif.compboost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classification component-wise boosting learner — mlr_learners_classif.compboost","text":"deep Whether make deep clone.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_regr.compboost.html","id":null,"dir":"Reference","previous_headings":"","what":"Regression component-wise boosting learner — mlr_learners_regr.compboost","title":"Regression component-wise boosting learner — mlr_learners_regr.compboost","text":"Learner component-wise boosting model implemented Compboost package compboost.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_regr.compboost.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Regression component-wise boosting learner — mlr_learners_regr.compboost","text":"mlr3::Learner -> compboost::LearnerCompboost -> LearnerRegrCompboost","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_regr.compboost.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Regression component-wise boosting learner — mlr_learners_regr.compboost","text":"mlr3::Learner$base_learner() mlr3::Learner$format() mlr3::Learner$help() mlr3::Learner$predict() mlr3::Learner$predict_newdata() mlr3::Learner$print() mlr3::Learner$reset() mlr3::Learner$train() compboost::LearnerCompboost$importance() compboost::LearnerCompboost$save_json() compboost::LearnerCompboost$selected_features()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_regr.compboost.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Regression component-wise boosting learner — mlr_learners_regr.compboost","text":"LearnerRegrCompboost$new() LearnerRegrCompboost$clone()","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_regr.compboost.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Regression component-wise boosting learner — mlr_learners_regr.compboost","text":"Creates new instance R6 class.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_regr.compboost.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Regression component-wise boosting learner — mlr_learners_regr.compboost","text":"","code":"LearnerRegrCompboost$new()"},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_regr.compboost.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Regression component-wise boosting learner — mlr_learners_regr.compboost","text":"objects class cloneable method.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_regr.compboost.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Regression component-wise boosting learner — mlr_learners_regr.compboost","text":"","code":"LearnerRegrCompboost$clone(deep = FALSE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/mlr_learners_regr.compboost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Regression component-wise boosting learner — mlr_learners_regr.compboost","text":"deep Whether make deep clone.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearner.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize contribution of one base learner — plotBaselearner","title":"Visualize contribution of one base learner — plotBaselearner","text":"function visualizes contribution base learner overall prediction score. visualization partial effects see plotPEUni().","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize contribution of one base learner — plotBaselearner","text":"","code":"plotBaselearner(cboost, blname, npoints = 100L)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize contribution of one base learner — plotBaselearner","text":"cboost (Compboost) trained Compboost object. blname (character(1L)) Name base learner. Must one cboost$getBaselearnerNames(). npoints (integer(1L)) Number points predicted lines (applies numerical features).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize contribution of one base learner — plotBaselearner","text":"ggplot object containing graphic.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearner.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize contribution of one base learner — plotBaselearner","text":"","code":"cboost = Compboost$new(data = iris, target = \"Petal.Length\",   loss = LossQuadratic$new()) cboost$addComponents(\"Sepal.Width\") cboost$train(500L) #>   1/500   risk = 1.5   #>  12/500   risk = 1.3   #>  24/500   risk = 1.2   #>  36/500   risk = 1.2   #>  48/500   risk = 1.1   #>  60/500   risk = 1.1   #>  72/500   risk = 1.1   #>  84/500   risk = 1.1   #>  96/500   risk = 1.1   #> 108/500   risk = 1.1   #> 120/500   risk = 1.1   #> 132/500   risk = 1.1   #> 144/500   risk = 1.1   #> 156/500   risk = 1.1   #> 168/500   risk = 1.1   #> 180/500   risk = 1.1   #> 192/500   risk = 1   #> 204/500   risk = 1   #> 216/500   risk = 1   #> 228/500   risk = 1   #> 240/500   risk = 1   #> 252/500   risk = 1   #> 264/500   risk = 1   #> 276/500   risk = 1   #> 288/500   risk = 1   #> 300/500   risk = 1   #> 312/500   risk = 1   #> 324/500   risk = 1   #> 336/500   risk = 1   #> 348/500   risk = 1   #> 360/500   risk = 1   #> 372/500   risk = 1   #> 384/500   risk = 1   #> 396/500   risk = 1   #> 408/500   risk = 1   #> 420/500   risk = 1   #> 432/500   risk = 1   #> 444/500   risk = 1   #> 456/500   risk = 1   #> 468/500   risk = 1   #> 480/500   risk = 1   #> 492/500   risk = 1   #>  #>  #> Train 500 iterations in 0 Seconds. #> Final risk based on the train set: 1 #>  plotBaselearner(cboost, \"Sepal.Width_linear\")  plotBaselearner(cboost, \"Sepal.Width_Sepal.Width_spline_centered\")"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearnerTraces.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize base learner traces — plotBaselearnerTraces","title":"Visualize base learner traces — plotBaselearnerTraces","text":"function shows base learners evolves fitting process. default show frequency single base learner included model evolves. Additionally, value argument, vectors (e.g. risk) can used show base learner specific risk reduction evolves fitting process.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearnerTraces.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize base learner traces — plotBaselearnerTraces","text":"","code":"plotBaselearnerTraces(cboost, value = 1, n_legend = 5L)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearnerTraces.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize base learner traces — plotBaselearnerTraces","text":"cboost (Compboost) trained Compboost object. value (numeric(1L) | numeric(length(cboost$getSelectedBaselearner()))) Value used show base learner development w.r.t. value. n_legend (integer(1L)) Number colored base learners added legend.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearnerTraces.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize base learner traces — plotBaselearnerTraces","text":"ggplot object containing graphic.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotBaselearnerTraces.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize base learner traces — plotBaselearnerTraces","text":"","code":"cboost = Compboost$new(data = iris, target = \"Petal.Length\",  loss = LossQuadratic$new()) cboost$addComponents(\"Sepal.Width\") cboost$addBaselearner(\"Species\", \"ridge\", BaselearnerCategoricalRidge) cboost$train(500L) #>   1/500   risk = 1.4   #>  12/500   risk = 0.52   #>  24/500   risk = 0.21   #>  36/500   risk = 0.13   #>  48/500   risk = 0.1   #>  60/500   risk = 0.093   #>  72/500   risk = 0.088   #>  84/500   risk = 0.085   #>  96/500   risk = 0.083   #> 108/500   risk = 0.081   #> 120/500   risk = 0.079   #> 132/500   risk = 0.078   #> 144/500   risk = 0.077   #> 156/500   risk = 0.076   #> 168/500   risk = 0.075   #> 180/500   risk = 0.074   #> 192/500   risk = 0.074   #> 204/500   risk = 0.073   #> 216/500   risk = 0.073   #> 228/500   risk = 0.072   #> 240/500   risk = 0.072   #> 252/500   risk = 0.072   #> 264/500   risk = 0.071   #> 276/500   risk = 0.071   #> 288/500   risk = 0.071   #> 300/500   risk = 0.071   #> 312/500   risk = 0.071   #> 324/500   risk = 0.07   #> 336/500   risk = 0.07   #> 348/500   risk = 0.07   #> 360/500   risk = 0.07   #> 372/500   risk = 0.07   #> 384/500   risk = 0.07   #> 396/500   risk = 0.069   #> 408/500   risk = 0.069   #> 420/500   risk = 0.069   #> 432/500   risk = 0.069   #> 444/500   risk = 0.069   #> 456/500   risk = 0.069   #> 468/500   risk = 0.069   #> 480/500   risk = 0.069   #> 492/500   risk = 0.069   #>  #>  #> Train 500 iterations in 0 Seconds. #> Final risk based on the train set: 0.069 #>  plotBaselearnerTraces(cboost)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotFeatureImportance.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize the feature importance — plotFeatureImportance","title":"Visualize the feature importance — plotFeatureImportance","text":"function visualizes feature importance horizontal bar plot.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotFeatureImportance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize the feature importance — plotFeatureImportance","text":"","code":"plotFeatureImportance(cboost, num_feats = NULL, aggregate = TRUE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotFeatureImportance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize the feature importance — plotFeatureImportance","text":"cboost (Compboost) trained Compboost object. num_feats (integer(1L)) Number features visualized. features added set NULL. aggregate (logical(1L)) Flag whether feature importance aggregated feature. Otherwise visualized per base learner.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotFeatureImportance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize the feature importance — plotFeatureImportance","text":"ggplot object containing graphic.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotFeatureImportance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize the feature importance — plotFeatureImportance","text":"","code":"cboost = boostSplines(data = iris, target = \"Sepal.Length\", loss = LossQuadratic$new()) #>   1/100   risk = 0.31  time = 0    #>   2/100   risk = 0.29  time = 102    #>   4/100   risk = 0.25  time = 247    #>   6/100   risk = 0.21  time = 386    #>   8/100   risk = 0.18  time = 521    #>  10/100   risk = 0.16  time = 659    #>  12/100   risk = 0.14  time = 794    #>  14/100   risk = 0.13  time = 930    #>  16/100   risk = 0.11  time = 1074    #>  18/100   risk = 0.1  time = 1254    #>  20/100   risk = 0.095  time = 1390    #>  22/100   risk = 0.088  time = 1525    #>  24/100   risk = 0.083  time = 1658    #>  26/100   risk = 0.078  time = 1792    #>  28/100   risk = 0.074  time = 1927    #>  30/100   risk = 0.071  time = 2063    #>  32/100   risk = 0.069  time = 2197    #>  34/100   risk = 0.067  time = 2333    #>  36/100   risk = 0.065  time = 2467    #>  38/100   risk = 0.063  time = 2600    #>  40/100   risk = 0.062  time = 2737    #>  42/100   risk = 0.061  time = 2873    #>  44/100   risk = 0.06  time = 3008    #>  46/100   risk = 0.059  time = 3144    #>  48/100   risk = 0.058  time = 3276    #>  50/100   risk = 0.057  time = 3433    #>  52/100   risk = 0.056  time = 3591    #>  54/100   risk = 0.056  time = 3748    #>  56/100   risk = 0.055  time = 3921    #>  58/100   risk = 0.054  time = 4086    #>  60/100   risk = 0.054  time = 4245    #>  62/100   risk = 0.053  time = 4404    #>  64/100   risk = 0.052  time = 4585    #>  66/100   risk = 0.052  time = 4727    #>  68/100   risk = 0.051  time = 4865    #>  70/100   risk = 0.051  time = 5003    #>  72/100   risk = 0.051  time = 5142    #>  74/100   risk = 0.05  time = 5285    #>  76/100   risk = 0.05  time = 5434    #>  78/100   risk = 0.049  time = 5611    #>  80/100   risk = 0.049  time = 5773    #>  82/100   risk = 0.049  time = 5933    #>  84/100   risk = 0.048  time = 6094    #>  86/100   risk = 0.048  time = 6256    #>  88/100   risk = 0.048  time = 6419    #>  90/100   risk = 0.048  time = 6583    #>  92/100   risk = 0.047  time = 6764    #>  94/100   risk = 0.047  time = 6905    #>  96/100   risk = 0.047  time = 7045    #>  98/100   risk = 0.047  time = 7186    #> 100/100   risk = 0.046  time = 7327    #>  #>  #> Train 100 iterations in 0 Seconds. #> Final risk based on the train set: 0.046 #>  plotFeatureImportance(cboost)  plotFeatureImportance(cboost, num_feats = 2)  plotFeatureImportance(cboost, num_feats = 2, aggregate = FALSE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotIndividualContribution.html","id":null,"dir":"Reference","previous_headings":"","what":"Decompose the predicted value based on the given features — plotIndividualContribution","title":"Decompose the predicted value based on the given features — plotIndividualContribution","text":"function visualizes contribution feature regarding predicted value. default, multiple base learners defined one feature aggregated. want show contribution single base learner, set aggregate = FALSE.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotIndividualContribution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Decompose the predicted value based on the given features — plotIndividualContribution","text":"","code":"plotIndividualContribution(   cboost,   newdata,   aggregate = TRUE,   colbreaks = c(-Inf, 0, Inf),   collabels = c(\"negative\", \"positive\"),   nround = 2L,   offset = TRUE )"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotIndividualContribution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Decompose the predicted value based on the given features — plotIndividualContribution","text":"cboost (Compboost) trained Compboost object. newdata (data.frame()) Data frame containing exactly one row holding new observations. aggregate (logical(1L)) Number colored base learners added legend. colbreaks (numeric()) Breaks visualize/highlight different predicted values. Default creates different colors positive negative score values. set NULL coloring applied. collabels (character(length(colbreaks) - 1)) Labels color breaks. set NULL intervals used labels. nround (integer(1L)) Digit passed round labels (default nround = 2L). offset (logical(1L)) Flag indicate whether offset added figure .","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotIndividualContribution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Decompose the predicted value based on the given features — plotIndividualContribution","text":"ggplot object containing graphic.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotIndividualContribution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Decompose the predicted value based on the given features — plotIndividualContribution","text":"","code":"dat = mtcars fnum = c(\"cyl\", \"disp\", \"hp\", \"drat\", \"wt\", \"qsec\") fcat = c(\"vs\", \"am\", \"gear\", \"carb\") for (fn in fcat) dat[[fn]] = as.factor(dat[[fn]])  cboost = Compboost$new(data = dat, target = \"mpg\",   loss = LossQuadratic$new())  for (fn in fnum) cboost$addComponents(fn, df = 3) for (fn in fcat) cboost$addBaselearner(fn, \"ridge\", BaselearnerCategoricalRidge) cboost$train(500L) #>   1/500   risk = 16   #>  12/500   risk = 7.9   #>  24/500   risk = 4.6   #>  36/500   risk = 3.4   #>  48/500   risk = 3   #>  60/500   risk = 2.7   #>  72/500   risk = 2.6   #>  84/500   risk = 2.6   #>  96/500   risk = 2.5   #> 108/500   risk = 2.5   #> 120/500   risk = 2.4   #> 132/500   risk = 2.4   #> 144/500   risk = 2.4   #> 156/500   risk = 2.3   #> 168/500   risk = 2.3   #> 180/500   risk = 2.3   #> 192/500   risk = 2.3   #> 204/500   risk = 2.3   #> 216/500   risk = 2.2   #> 228/500   risk = 2.2   #> 240/500   risk = 2.2   #> 252/500   risk = 2.2   #> 264/500   risk = 2.2   #> 276/500   risk = 2.2   #> 288/500   risk = 2.1   #> 300/500   risk = 2.1   #> 312/500   risk = 2.1   #> 324/500   risk = 2.1   #> 336/500   risk = 2.1   #> 348/500   risk = 2.1   #> 360/500   risk = 2.1   #> 372/500   risk = 2.1   #> 384/500   risk = 2.1   #> 396/500   risk = 2   #> 408/500   risk = 2   #> 420/500   risk = 2   #> 432/500   risk = 2   #> 444/500   risk = 2   #> 456/500   risk = 2   #> 468/500   risk = 2   #> 480/500   risk = 2   #> 492/500   risk = 2   #>  #>  #> Train 500 iterations in 0 Seconds. #> Final risk based on the train set: 2 #>  cbreaks = c(-Inf, -0.1, 0.1, Inf) clabs   = c(\"bad\", \"middle\", \"good\") plotIndividualContribution(cboost, dat[10, ], colbreaks = cbreaks,   collabels = clabs)  plotIndividualContribution(cboost, dat[10, ], offset = FALSE,   colbreaks = cbreaks, collabels = clabs)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotPEUni.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize partial effect of a feature — plotPEUni","title":"Visualize partial effect of a feature — plotPEUni","text":"function visualizes contribution specific feature overall prediction score. multiple base learner features included, added graphic well aggregated contribution. difference plotBaselearner() potentially multiple base learners based feat aggregated visualized plotBaselearner() visualizes contribution one specific base learner. function also automatically decides whether given feature numeric categorical chooses appropriate technique (lines numeric horizontal lines categorical).","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotPEUni.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize partial effect of a feature — plotPEUni","text":"","code":"plotPEUni(cboost, feat, npoints = 100L, individual = TRUE)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotPEUni.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize partial effect of a feature — plotPEUni","text":"cboost (Compboost) trained Compboost object. feat (character(1L)) Name feature. npoints (integer(1L)) Number points predicted lines (applies numerical features). individual (logical(1L)) Flag whether individual base learners added graphic .","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotPEUni.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize partial effect of a feature — plotPEUni","text":"ggplot object containing graphic.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotPEUni.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize partial effect of a feature — plotPEUni","text":"","code":"cboost = Compboost$new(data = iris, target = \"Petal.Length\",   loss = LossQuadratic$new()) cboost$addComponents(\"Sepal.Width\") cboost$train(500L) #>   1/500   risk = 1.5   #>  12/500   risk = 1.3   #>  24/500   risk = 1.2   #>  36/500   risk = 1.2   #>  48/500   risk = 1.1   #>  60/500   risk = 1.1   #>  72/500   risk = 1.1   #>  84/500   risk = 1.1   #>  96/500   risk = 1.1   #> 108/500   risk = 1.1   #> 120/500   risk = 1.1   #> 132/500   risk = 1.1   #> 144/500   risk = 1.1   #> 156/500   risk = 1.1   #> 168/500   risk = 1.1   #> 180/500   risk = 1.1   #> 192/500   risk = 1   #> 204/500   risk = 1   #> 216/500   risk = 1   #> 228/500   risk = 1   #> 240/500   risk = 1   #> 252/500   risk = 1   #> 264/500   risk = 1   #> 276/500   risk = 1   #> 288/500   risk = 1   #> 300/500   risk = 1   #> 312/500   risk = 1   #> 324/500   risk = 1   #> 336/500   risk = 1   #> 348/500   risk = 1   #> 360/500   risk = 1   #> 372/500   risk = 1   #> 384/500   risk = 1   #> 396/500   risk = 1   #> 408/500   risk = 1   #> 420/500   risk = 1   #> 432/500   risk = 1   #> 444/500   risk = 1   #> 456/500   risk = 1   #> 468/500   risk = 1   #> 480/500   risk = 1   #> 492/500   risk = 1   #>  #>  #> Train 500 iterations in 0 Seconds. #> Final risk based on the train set: 1 #>  plotPEUni(cboost, \"Sepal.Width\")"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotRisk.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize the risk — plotRisk","title":"Visualize the risk — plotRisk","text":"function visualizes risk training. validation data given, train risk plotted validation risk.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotRisk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize the risk — plotRisk","text":"","code":"plotRisk(cboost)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotRisk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize the risk — plotRisk","text":"cboost (Compboost) trained Compboost object.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotRisk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize the risk — plotRisk","text":"ggplot object containing graphic.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotRisk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize the risk — plotRisk","text":"","code":"cboost_no_valdat = boostSplines(data = iris, target = \"Sepal.Length\",   loss = LossQuadratic$new()) #>   1/100   risk = 0.31  time = 0    #>   2/100   risk = 0.29  time = 124    #>   4/100   risk = 0.25  time = 300    #>   6/100   risk = 0.21  time = 443    #>   8/100   risk = 0.18  time = 580    #>  10/100   risk = 0.16  time = 730    #>  12/100   risk = 0.14  time = 862    #>  14/100   risk = 0.13  time = 992    #>  16/100   risk = 0.11  time = 1120    #>  18/100   risk = 0.1  time = 1251    #>  20/100   risk = 0.095  time = 1404    #>  22/100   risk = 0.088  time = 1541    #>  24/100   risk = 0.083  time = 1678    #>  26/100   risk = 0.078  time = 1823    #>  28/100   risk = 0.074  time = 1956    #>  30/100   risk = 0.071  time = 2092    #>  32/100   risk = 0.069  time = 2228    #>  34/100   risk = 0.067  time = 2365    #>  36/100   risk = 0.065  time = 2499    #>  38/100   risk = 0.063  time = 2634    #>  40/100   risk = 0.062  time = 2772    #>  42/100   risk = 0.061  time = 2908    #>  44/100   risk = 0.06  time = 3043    #>  46/100   risk = 0.059  time = 3183    #>  48/100   risk = 0.058  time = 3333    #>  50/100   risk = 0.057  time = 3471    #>  52/100   risk = 0.056  time = 3607    #>  54/100   risk = 0.056  time = 3749    #>  56/100   risk = 0.055  time = 3876    #>  58/100   risk = 0.054  time = 4005    #>  60/100   risk = 0.054  time = 4154    #>  62/100   risk = 0.053  time = 4293    #>  64/100   risk = 0.052  time = 4431    #>  66/100   risk = 0.052  time = 4571    #>  68/100   risk = 0.051  time = 4799    #>  70/100   risk = 0.051  time = 4959    #>  72/100   risk = 0.051  time = 5120    #>  74/100   risk = 0.05  time = 5282    #>  76/100   risk = 0.05  time = 5444    #>  78/100   risk = 0.049  time = 5606    #>  80/100   risk = 0.049  time = 5768    #>  82/100   risk = 0.049  time = 5940    #>  84/100   risk = 0.048  time = 6078    #>  86/100   risk = 0.048  time = 6216    #>  88/100   risk = 0.048  time = 6354    #>  90/100   risk = 0.048  time = 6493    #>  92/100   risk = 0.047  time = 6633    #>  94/100   risk = 0.047  time = 6771    #>  96/100   risk = 0.047  time = 6909    #>  98/100   risk = 0.047  time = 7048    #> 100/100   risk = 0.046  time = 7188    #>  #>  #> Train 100 iterations in 0 Seconds. #> Final risk based on the train set: 0.046 #>  plotRisk(cboost_no_valdat)   cboost_valdat = boostSplines(data = iris, target = \"Sepal.Length\",   loss = LossQuadratic$new(), oob_fraction = 0.3) #>   1/100   risk = 0.32  oob_risk = 0.28   time = 0    #>   2/100   risk = 0.3  oob_risk = 0.26   time = 99    #>   4/100   risk = 0.25  oob_risk = 0.23   time = 227    #>   6/100   risk = 0.21  oob_risk = 0.2   time = 373    #>   8/100   risk = 0.18  oob_risk = 0.18   time = 485    #>  10/100   risk = 0.16  oob_risk = 0.16   time = 584    #>  12/100   risk = 0.14  oob_risk = 0.15   time = 684    #>  14/100   risk = 0.12  oob_risk = 0.13   time = 782    #>  16/100   risk = 0.11  oob_risk = 0.12   time = 879    #>  18/100   risk = 0.1  oob_risk = 0.12   time = 996    #>  20/100   risk = 0.091  oob_risk = 0.11   time = 1103    #>  22/100   risk = 0.084  oob_risk = 0.11   time = 1210    #>  24/100   risk = 0.078  oob_risk = 0.1   time = 1316    #>  26/100   risk = 0.073  oob_risk = 0.099   time = 1422    #>  28/100   risk = 0.069  oob_risk = 0.096   time = 1536    #>  30/100   risk = 0.066  oob_risk = 0.094   time = 1661    #>  32/100   risk = 0.063  oob_risk = 0.093   time = 1791    #>  34/100   risk = 0.061  oob_risk = 0.091   time = 1904    #>  36/100   risk = 0.059  oob_risk = 0.09   time = 2014    #>  38/100   risk = 0.057  oob_risk = 0.09   time = 2151    #>  40/100   risk = 0.056  oob_risk = 0.089   time = 2263    #>  42/100   risk = 0.055  oob_risk = 0.088   time = 2373    #>  44/100   risk = 0.053  oob_risk = 0.088   time = 2482    #>  46/100   risk = 0.052  oob_risk = 0.087   time = 2591    #>  48/100   risk = 0.051  oob_risk = 0.087   time = 2715    #>  50/100   risk = 0.05  oob_risk = 0.086   time = 2825    #>  52/100   risk = 0.049  oob_risk = 0.086   time = 2934    #>  54/100   risk = 0.048  oob_risk = 0.086   time = 3042    #>  56/100   risk = 0.048  oob_risk = 0.085   time = 3153    #>  58/100   risk = 0.047  oob_risk = 0.085   time = 3264    #>  60/100   risk = 0.046  oob_risk = 0.085   time = 3375    #>  62/100   risk = 0.046  oob_risk = 0.085   time = 3486    #>  64/100   risk = 0.045  oob_risk = 0.084   time = 3597    #>  66/100   risk = 0.045  oob_risk = 0.084   time = 3712    #>  68/100   risk = 0.044  oob_risk = 0.084   time = 3823    #>  70/100   risk = 0.044  oob_risk = 0.084   time = 3934    #>  72/100   risk = 0.043  oob_risk = 0.084   time = 4046    #>  74/100   risk = 0.043  oob_risk = 0.084   time = 4156    #>  76/100   risk = 0.042  oob_risk = 0.083   time = 4268    #>  78/100   risk = 0.042  oob_risk = 0.083   time = 4380    #>  80/100   risk = 0.042  oob_risk = 0.083   time = 4494    #>  82/100   risk = 0.041  oob_risk = 0.083   time = 4606    #>  84/100   risk = 0.041  oob_risk = 0.083   time = 4717    #>  86/100   risk = 0.041  oob_risk = 0.083   time = 4827    #>  88/100   risk = 0.041  oob_risk = 0.083   time = 4939    #>  90/100   risk = 0.04  oob_risk = 0.083   time = 5051    #>  92/100   risk = 0.04  oob_risk = 0.083   time = 5162    #>  94/100   risk = 0.04  oob_risk = 0.083   time = 5274    #>  96/100   risk = 0.04  oob_risk = 0.083   time = 5386    #>  98/100   risk = 0.039  oob_risk = 0.082   time = 5499    #> 100/100   risk = 0.039  oob_risk = 0.082   time = 5611    #>  #>  #> Train 100 iterations in 0 Seconds. #> Final risk based on the train set: 0.039 #>  plotRisk(cboost_valdat)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotTensor.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize bivariate tensor products — plotTensor","title":"Visualize bivariate tensor products — plotTensor","text":"function visualizes contribution bivariate tensor product.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotTensor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize bivariate tensor products — plotTensor","text":"","code":"plotTensor(cboost, tname, npoints = 100L, nbins = 15L)"},{"path":"https://schalkdaniel.github.io/compboost/reference/plotTensor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize bivariate tensor products — plotTensor","text":"cboost (Compboost) trained Compboost object. tname (character(2L)) Name tensor base learner. npoints (integer(1L)) Number grid points per numerical feature. Note: two numerical features overall number grid points npoints^2. numerical categorical feature npoints * ncat ncat number categories. two categorical features ncat^2 grid points drawn. nbins (logical(1L)) Number bins surface. applies case two numerical features. smooth surface drawn nbins = NULL.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotTensor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize bivariate tensor products — plotTensor","text":"ggplot object containing graphic.","code":""},{"path":"https://schalkdaniel.github.io/compboost/reference/plotTensor.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize bivariate tensor products — plotTensor","text":"","code":"cboost = Compboost$new(data = iris, target = \"Petal.Length\",   loss = LossQuadratic$new())  cboost$addTensor(\"Sepal.Width\", \"Sepal.Length\", df1 = 4, df2 = 4) cboost$addTensor(\"Sepal.Width\", \"Species\", df1 = 4, df2 = 2)  cboost$train(150L) #>   1/150   risk = 1.4   #>   4/150   risk = 1.1   #>   8/150   risk = 0.78   #>  12/150   risk = 0.58   #>  16/150   risk = 0.45   #>  20/150   risk = 0.35   #>  24/150   risk = 0.29   #>  28/150   risk = 0.25   #>  32/150   risk = 0.23   #>  36/150   risk = 0.21   #>  40/150   risk = 0.19   #>  44/150   risk = 0.18   #>  48/150   risk = 0.18   #>  52/150   risk = 0.17   #>  56/150   risk = 0.17   #>  60/150   risk = 0.17   #>  64/150   risk = 0.16   #>  68/150   risk = 0.16   #>  72/150   risk = 0.16   #>  76/150   risk = 0.16   #>  80/150   risk = 0.16   #>  84/150   risk = 0.16   #>  88/150   risk = 0.16   #>  92/150   risk = 0.16   #>  96/150   risk = 0.16   #> 100/150   risk = 0.15   #> 104/150   risk = 0.15   #> 108/150   risk = 0.15   #> 112/150   risk = 0.15   #> 116/150   risk = 0.15   #> 120/150   risk = 0.15   #> 124/150   risk = 0.15   #> 128/150   risk = 0.15   #> 132/150   risk = 0.15   #> 136/150   risk = 0.15   #> 140/150   risk = 0.15   #> 144/150   risk = 0.15   #> 148/150   risk = 0.15   #>  #>  #> Train 150 iterations in 0 Seconds. #> Final risk based on the train set: 0.15 #>   plotTensor(cboost, \"Sepal.Width_Species_tensor\")  plotTensor(cboost, \"Sepal.Width_Sepal.Length_tensor\")  plotTensor(cboost, \"Sepal.Width_Sepal.Length_tensor\", nbins = NULL)"}]
