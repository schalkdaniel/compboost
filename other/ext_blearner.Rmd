---
title: "Extending compboost: Base-Learner"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Extending compboost with losses}
  %\VignetteEngine{knitr::rmarkdown}
  \VignetteEncoding{UTF-8}
---

```{r, echo=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
  # fig.path = "Readme_files/"
)

library(compboost)
```


## Before Starting

- Read the [use-case](https://danielschalk.com/compboost/articles/getting_started/use_case.html) site to get to know how to define a `Compboost` object using the `R6` interface


## What is Needed

`compboost` was designed to provide a component-wise boosting framework with maximal flexibility. This document gives an overview about how to define custom base-learner in `R` as well as in `C++` without recompiling the whole package. These custom base-learner then can be used for training the model.

In boosting the adequate modeling of an effect depends on the used base-learner. Two important ones are already pre-implemented, the polynomial and p-spline base-learner.

In component-wise boosting the real task of a base-learner is to handle the mapping of a feature to a higher dimensional space. For instance, a linear base-learner (polynomial with degree one), takes a vector $x = (x_1, \dots, x_n)$, puts it into a matrix and adds the intercept (if requested):
$$x \rightarrow \left(\begin{array}{cc}
1 & x_1 \\
\vdots & \vdots \\
1 & x_n
\end{array}\right)$$

For this topic we are rebuilding two different loss function s:

- The quadratic loss as easy example for `C++`
- The Poisson loss for counting data as more sophisticated loss example for `R`


## Define a New Loss With `R`

For this example we are using the `VonBort` dataset provided by the package `vcd`:

> *"Data from von Bortkiewicz (1898), given by Andrews \& Herzberg (1985), on number of deaths by horse or mule kicks in 14 corps of the Prussian army."*

```{r}
data(VonBort, package = "vcd")
```

We like to model the deaths using the poisson regression context in boosting. That means we have to define the proper loss function, the gradient, and the constant initializer.

The scheme for the loss and the gradient is to specify a function of the following form:

- loss: `function (truth, response)`
- gradient: `function (truth, response)`
- constant initializer: `function (truth)`

### The loss function

$$L(y,f) = -\log\left( \exp(f)^y \exp(\exp(f)) \right) - \log(y!)$$

```{r}
lossPoisson = function (truth, response) {
  return(-log(exp(response)^truth * exp(-exp(response))) - gamma(truth + 1))
}
```

### The gradient of the loss function

$$\frac{\partial}{\partial f} L(y,f) = \exp(f) - y$$

```{r}
gradPoisson = function (truth, response) {
  return(exp(response) - truth)
}
```

### The constant initialization

$$\mathsf{arg min}_{c\in\mathbb{R}} \sum_{i = 1}^n L\left(y^{(i)}, c\right) = \log(\bar{y})$$

```{r}
constInitPoisson = function (truth) {
  return(log(mean.default(truth)))
}
```

### Define the loss

Finally, having the three components defined, we can pass them to the `LossCustom` object which sets them for the specific task:

```{r}
# Define custom loss:
my.poisson.loss = LossCustom$new(lossPoisson, gradPoisson, constInitPoisson)
```

### Train a model

This loss object can now be used for any task that requires a loss object, such as defining a `compboost` model:

```{r}
cboost = Compboost$new(VonBort, "deaths", loss = my.poisson.loss)
cboost$addBaselearner("year", "spline", BaselearnerPSpline)
cboost$train(100, trace = 0)
```


## Define a New Loss With `C++`

For this example we are using the `iris` dataset with `Sepal.Length` as target to keep it simple. We now want to replicate the quadratic loss. This is done by exposing external pointer to `R` that holds the function definition which then passed to the `LossCustomCpp` class.

A general advise is to write a `.cpp` file that contains the whole definition. This file then just needs to be sourced by `Rcpp::sourceCpp()`.

To be able to expose that functions we have to declare the head first:

```cpp
// [[Rcpp::depends(RcppArmadillo)]]
#include <RcppArmadillo.h>

typedef arma::mat (*lossFunPtr) (const arma::mat& true_value, const arma::mat& prediction);
typedef arma::mat (*gradFunPtr) (const arma::mat& true_value, const arma::mat& prediction);
typedef double (*constInitFunPtr) (const arma::mat& true_value);
```

As the type definition already indicates, the `C++` function must have the following signatures:

- loss: `arma::mat lossFun (const arma::mat& truth, const arma::mat& response)`
- gradient: `arma::mat gradFun (const arma::mat& truth, const arma::mat& response)`
- constant initializer: `constInitFun (const arma::mat& true_value)`

### The loss function

$$L(y,f) = -0.5 \left(y - f\right)^2$$

```cpp
arma::mat lossFun (const arma::mat& true_value, const arma::mat& prediction)
{
return arma::pow(true_value - prediction, 2) / 2;
}
```

### The gradient of the loss function

$$\frac{\partial}{\partial f} L(y,f) = f - y$$

```cpp
arma::mat gradFun (const arma::mat& true_value, const arma::mat& prediction)
{
return prediction - true_value;
}
```

### The constant initialization

$$\mathsf{arg min}_{c\in\mathbb{R}} \sum_{i = 1}^n L\left(y^{(i)}, c\right) = \bar{y}$$

```cpp
double constInitFun (const arma::mat& true_value)
{
return arma::accu(true_value) / true_value.size();
}
```

### Exposing external pointer

Now wrap the function to an XPtr. This one stores the pointer to the function and can be used as parameter for the `LossCustomCpp`.

Note that it isn't necessary to export the upper functions since we are interested in exporting the pointer not the function.

```cpp
// [[Rcpp::export]]
Rcpp::XPtr<lossFunPtr> lossFunSetter ()
{
  return Rcpp::XPtr<lossFunPtr> (new lossFunPtr (&lossFun));
}

// [[Rcpp::export]]
Rcpp::XPtr<gradFunPtr> gradFunSetter ()
{
  return Rcpp::XPtr<gradFunPtr> (new gradFunPtr (&gradFun));
}

// [[Rcpp::export]]
Rcpp::XPtr<constInitFunPtr> constInitFunSetter ()
{
  return Rcpp::XPtr<constInitFunPtr> (new constInitFunPtr (&constInitFun));
}
```

### Define the loss

```{r, include=FALSE}
Rcpp::sourceCpp(code = getCustomCppExample("loss", TRUE))
```

After exposing the function we can get the pointer to the function by calling it:
```{r}
lossFunSetter()
gradFunSetter()
constInitFunSetter()
```

This can now passed to the `LossCustomCpp` class:
```{r}
my_cpp_loss = LossCustomCpp$new(lossFunSetter(), gradFunSetter(), constInitFunSetter())
```

### Train a model

Finally, we use the custom loss to train our model:
```{r}
cboost = boostSplines(data = iris, target = "Sepal.Length",
  loss = my_cpp_loss, trace = 25)
```
