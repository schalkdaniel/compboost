---
title: "Extending compboost with losses"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Extending compboost with losses}
  %\VignetteEngine{knitr::rmarkdown}
  \VignetteEncoding{UTF-8}
---

```{r, echo=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
  # fig.path = "Readme_files/"
)

library(compboost)
```


## Before starting

- Read the [use-case](https://danielschalk.com/compboost/articles/getting_started/use_case.html) site to get to know how to define a `Compboost` object using the `R6` interface.

## What is Needed

`compboost` was designed to provide a component-wise boosting framework with maximal flexibility. This vignette gives an overview how to define custom losses in `R` as well as in `C++` without recompiling the whole package. These custom losses can be used for training the model and/or logging mechanisms.

The loss function for training a model with boosting is required to be differentiable. Hence, we need to define the loss function and the gradient. Further, boosting is initialized as loss optimal constant. To capture this, we have to define the loss optimal constant as function of a response vector. Having these three components, it is quite easy to define custom losses.

As showcase, we are rebuilding two different loss functions:

- The quadratic loss as easy example for `C++`
- The Poisson loss for counting data as more sophisticated loss example in `R`


## Define a new loss in `R`

For this example we are using the `VonBort` dataset provided by the package `vcd`:

> *"Data from von Bortkiewicz (1898), given by Andrews \& Herzberg (1985), on number of deaths by horse or mule kicks in 14 corps of the Prussian army."*

```{r}
data(VonBort, package = "vcd")
```

We like to model the deaths using a Poisson regression in boosting. That means we have to define a proper loss function, the gradient, and the constant initialization.

The scheme for the loss, the gradient, and the constant initialization is to specify a function of the following form:

- loss: `function (truth, response)`
- gradient: `function (truth, response)`
- constant initializer: `function (truth)`

### The loss function

$$L(y,f) = -\log\left( \exp(f)^y \exp(\exp(f)) \right) - \log(y!)$$

```{r}
lossPoisson = function (truth, response) {
  return(-log(exp(response)^truth * exp(-exp(response))) - gamma(truth + 1))
}
```

### The gradient of the loss function

$$\frac{\partial}{\partial f} L(y,f) = \exp(f) - y$$

```{r}
gradPoisson = function (truth, response) {
  return(exp(response) - truth)
}
```

### The constant initialization

$$\mathsf{arg min}_{c\in\mathbb{R}} \sum_{i = 1}^n L\left(y^{(i)}, c\right) = \log(\bar{y})$$

```{r}
constInitPoisson = function (truth) {
  return(log(mean(truth)))
}
```

### Define the loss

Finally, having these three components allows to define a `LossCustom` object:

```{r}
# Define custom loss:
my_poisson_loss = LossCustom$new(lossPoisson, gradPoisson, constInitPoisson)
```

### Train a model

This loss object can be used for any task that requires a loss object:

```{r}
cboost = Compboost$new(VonBort, "deaths", loss = my_poisson_loss)
cboost$addBaselearner("year", "spline", BaselearnerPSpline)
cboost$train(500, trace = 0)
```



<!-- NOT APPLICABLE AS LONG THE CODE IS BUGGY -----------------------------------------------



## Define a New Loss With `C++`

For this example, to keep it simple, we are using the `iris` dataset with `Sepal.Length` as target. The aim is to replicate the quadratic loss. This is achieved by exposing external pointers to `R` which hold the function definition and is passed to the constructor of the `LossCustomCpp` class.

A general advise is to write a `.cpp` file that contains the whole definition. This file needs to be sourced by `Rcpp::sourceCpp()`.

We have to declare the head first to be able to expose functions:

```cpp
// [[Rcpp::depends(RcppArmadillo)]]
#include <RcppArmadillo.h>

typedef arma::mat (*lossFunPtr) (const arma::mat& true_value, const arma::mat& prediction);
typedef arma::mat (*gradFunPtr) (const arma::mat& true_value, const arma::mat& prediction);
typedef double (*constInitFunPtr) (const arma::mat& true_value);
```

As the type definition already indicates, the `C++` functions require the following signature:

- loss: `arma::mat lossFun (const arma::mat& truth, const arma::mat& response)`
- gradient: `arma::mat gradFun (const arma::mat& truth, const arma::mat& response)`
- constant initializer: `constInitFun (const arma::mat& true_value)`

### The loss function

$$L(y,f) = -0.5 \left(y - f\right)^2$$

```cpp
arma::mat lossFun (const arma::mat& true_value, const arma::mat& prediction)
{
  return arma::pow(true_value - prediction, 2) / 2;
}
```

### The gradient of the loss function

$$\frac{\partial}{\partial f} L(y,f) = f - y$$

```cpp
arma::mat gradFun (const arma::mat& true_value, const arma::mat& prediction)
{
  return prediction - true_value;
}
```

### The constant initialization

$$\mathsf{arg min}_{c\in\mathbb{R}} \sum_{i = 1}^n L\left(y^{(i)}, c\right) = \bar{y}$$

```cpp
double constInitFun (const arma::mat& true_value)
{
  return arma::accu(true_value) / true_value.size();
}
```

### Exposing external pointer

These functions are exposed to an XPtr. This stores the pointer to the function and can be used as parameter for the `LossCustomCpp`.

Note that it is not necessary to export the upper functions, exporting the pointer is the goal.

```cpp
// [[Rcpp::export]]
Rcpp::XPtr<lossFunPtr> lossFunSetter ()
{
  return Rcpp::XPtr<lossFunPtr> (new lossFunPtr (&lossFun));
}

// [[Rcpp::export]]
Rcpp::XPtr<gradFunPtr> gradFunSetter ()
{
  return Rcpp::XPtr<gradFunPtr> (new gradFunPtr (&gradFun));
}

// [[Rcpp::export]]
Rcpp::XPtr<constInitFunPtr> constInitFunSetter ()
{
  return Rcpp::XPtr<constInitFunPtr> (new constInitFunPtr (&constInitFun));
}
```

### Define the loss

```{r, include=FALSE}
Rcpp::sourceCpp(code = getCustomCppExample("loss", TRUE))
```

We can get the pointer to the function after exposing them:
```{r}
lossFunSetter()
gradFunSetter()
constInitFunSetter()
```

Now, we can pass the pointer to the `LossCustomCpp` class:
```{r}
my_cpp_loss = LossCustomCpp$new(lossFunSetter(), gradFunSetter(), constInitFunSetter())

src = "
// [[Rcpp::depends(RcppArmadillo)]]
#include <RcppArmadillo.h>

typedef arma::mat (*lossFunPtr) (const arma::mat& true_value, const arma::mat& prediction);
typedef double (*constInitFunPtr) (const arma::mat& true_value);
// [[Rcpp::export]]

arma::mat cpploss (const arma::mat& true_value, const arma::mat& prediction, const SEXP& lossFun)
{
  Rcpp::XPtr<lossFunPtr> myTempLoss (lossFun);
  lossFunPtr lf = *myTempLoss;
  return lf(true_value, prediction);
}

// [[Rcpp::export]]
double cppinit (const arma::mat& true_value, const SEXP& initFun)
{
  Rcpp::XPtr<constInitFunPtr> myConstInit (initFun);
  constInitFunPtr lf = *myConstInit;
  return lf(true_value);
}
"
Rcpp::sourceCpp(code = src)
cpploss(cbind(1:10), cbind(2:11), lossFunSetter())
cppinit(cbind(1:10), constInitFunSetter())
```

### Train a model

Finally, we use the custom loss to train our model:
```{r, eval=FALSE}
cboost = boostSplines(data = iris, target = "Sepal.Length",
  loss = my_cpp_loss, trace = 25)
```
------------------------------------------------------------------------------------------->

