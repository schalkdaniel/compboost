---
title: "First Use-Case"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{First use-case using compboost}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
  # fig.path = "Readme_files/"
)

library(compboost)
```

## Data: Titanic Passenger Survival Data Set

We use the [titanic dataset](https://www.kaggle.com/c/titanic/data) with binary
classification on `survived`. First of all, we store the train and test data
into two data frames and remove all rows that contains missing values (`NA`s):

```{r}
# Store train and test data:
df_train = na.omit(titanic::titanic_train)

str(df_train)
```

In the next step we transform the response to a factor having more intuitive levels:

```{r}
df_train$Survived = factor(df_train$Survived, labels = c("no", "yes"))
```

## Initializing Model

Due to the `R6` API it is necessary to create a new class object by calling the `$new()` constructor which gets the data, the target as character, and the used loss. Note that it is important to pass an initialized loss object which gives the opportunity to use, for example, a custom offset:
```{r}
cboost = Compboost$new(data = df_train, target = "Survived",
  loss = LossBinomial$new(), oob_fraction = 0.3)
```

## Adding Base-Learner

Adding new base-learners requires as first argument a character to indicate what feature we want to use for the new base-learner. As second argument it is important to define an identifier for the factory. This is necessary since it is possible to define multiple base-learners on the same source.

### Numerical Features

We can define a spline and a linear base-learner of the same feature:
```{r}
# Spline base-learner of age:
cboost$addBaselearner("Age", "spline", BaselearnerPSpline)

# Linear base-learner of age (degree = 1 with intercept is default):
cboost$addBaselearner("Age", "linear", BaselearnerPolynomial)
```

Additional arguments can be specified after the base-learner. For a complete list see the [functionality](https://compboost.org/functionality.html) at the project page:
```{r}
# Spline base-learner of fare:
cboost$addBaselearner("Fare", "spline", BaselearnerPSpline, degree = 2,
  n_knots = 14, penalty = 10, differences = 2)
```

### Categorical Features

When adding categorical features, each group is added as single base-learner. Do also note that we don't want an intercept here:
```{r}
cboost$addBaselearner("Sex", "categorical", BaselearnerPolynomial,
  intercept = FALSE)
```

Finally, we can get all registered factories:
```{r}
cboost$getBaselearnerNames()
```

## Define Logger

A logger is another class that is evaluated after each iteration to track the performance, elapsed runtime, or the iterations. For each `Compboost` object is by default one iterations logger defined with as many iterations as specified in the `$train()` function.

To be able to control the fitting behavior with logger, each logger can also be defined as stopper to stop the fitting process after a pre-defined stopping criteria.

### Time logger

This logger tracks the elapsed time. The time unit can be one of `microseconds`, `seconds` or `minutes`. The logger stops if `max_time` is reached. But we do not use that logger as stopper here:

```{r}
cboost$addLogger(logger = LoggerTime, use_as_stopper = FALSE, logger_id = "time",
  max_time = 0, time_unit = "microseconds")
```


## Train Model and Access Elements

```{r, warnings=FALSE}
cboost$train(2000, trace = 100)
cboost
```

Objects of the `Compboost` class do have member functions such as `$getEstimatedCoef()`, `$getInbagRisk()` or `$predict()` to access the results:
```{r}
str(cboost$getEstimatedCoef())

str(cboost$getInbagRisk())

str(cboost$predict())
```

To obtain a vector of the selected base-learners just call `$getSelectedBaselearner()`
```{r}
table(cboost$getSelectedBaselearner())
```

We can also access the predictions directly from the response object `cboost$response` and `cboost$response_oob`. Note that `$response_oob` was created automatically when defining an `oob_fraction` within the constructor:
```{r}
oob_label = cboost$response_oob$getResponse()
oob_pred = cboost$response_oob$getPredictionResponse()
table(true_label = oob_label, predicted = oob_pred)
```


## Visualizing Inbag vs. Out-Of-Bag Behavior

```{r}
cboost$plotInbagVsOobRisk()
```

## Retrain the Model

To set the whole model to another iteration one can again call `$train()`. The model is then set to an already seen iteration, if the new iteration is smaller than the already trained once or it trains additional base-learner until the new number is reached:
```{r, warnings=FALSE}
cboost$train(3000)

str(cboost$getEstimatedCoef())

str(cboost$getInbagRisk())

table(cboost$getSelectedBaselearner())
```

## Visualizing Base-Learner

To visualize a base-learner it is important to exactly use a name from `$getBaselearnerNames()`:
```{r, eval=FALSE}
gg1 = cboost$plot("Age_spline")
gg2 = cboost$plot("Age_spline", iters = c(50, 100, 500, 1000, 1500))
```
```{r,echo=FALSE, warning=FALSE, fig.align="center", fig.width=7, fig.height=4.6, out.width="600px",out.height="400px"}
gg1 = cboost$plot("Age_spline")
gg2 = cboost$plot("Age_spline", iters = c(200, 500, 1000, 1500))

gridExtra::grid.arrange(gg1, gg2, ncol = 2)
```

```{r, eval=FALSE}
gg1 = cboost$plot("Age_spline")
gg2 = cboost$plot("Age_spline", iters = c(50, 100, 500, 1000, 1500))
```
```{r,echo=FALSE, warning=FALSE, fig.align="center", fig.width=7, fig.height=4.6, out.width="600px",out.height="400px"}
gg1 = cboost$plot("Fare_spline")
gg2 = cboost$plot("Fare_spline", iters = c(200, 500, 1000, 1500))

gridExtra::grid.arrange(gg1, gg2, ncol = 2)
```