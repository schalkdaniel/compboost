# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' In memory data class to store data in RAM
#'
#' \code{InMemoryData} creates an data object which can be used as source or
#' target object within the base-learner factories of \code{compboost}. The
#' convention to initialize target data is to call the constructor without
#' any arguments.
#'
#' @format \code{\link{S4}} object.
#' @name InMemoryData
#'
#' @section Usage:
#' \preformatted{
#' InMemoryData$new()
#' InMemoryData$new(data_mat, data_identifier)
#' }
#'
#' @section Arguments:
#' \describe{
#' \item{\code{data_mat} [\code{matrix}]}{
#'   Matrix containing the source data. This source data is later transformed
#'   to obtain the design matrix a base-learner uses for training.
#' }
#' \item{\code{data_identifier} [\code{character(1)}]}{
#'   The name for the data specified in \code{data_mat}. Note that it is
#'   important to have the same data names for train and evaluation data.
#' }
#' }
#'
#'
#' @section Details:
#'   The \code{data_mat} needs to suits the base-learner. For instance, the
#'   spline base-learner does just take a one column matrix since there are
#'   just one dimensional splines at the moment.
#'
#'   The \code{data_mat} and \code{data_identifier} of a target data object
#'   is set automatically by passing the source and target object to a
#'   factory. \code{getData()} can then be used to access the
#'   transformed data of the target object.
#'
#' @section Fields:
#'   This class doesn't contain public fields.
#'
#' @section Methods:
#' \describe{
#' \item{\code{getData()}}{}
#' \item{\code{getIdentifier()}}{}
#' }
#' @examples
#' # Sample data:
#' data_mat = cbind(1:10)
#'
#' # Create new data object:
#' data_obj = InMemoryData$new(data_mat, "my_data_name")
#'
#' # Get data and identifier:
#' data_obj$getData()
#' data_obj$getIdentifier()
#'
#' @export InMemoryData
NULL

#' Base-learner factory for polynomial regression
#'
#' \code{BaselearnerPolynomial} creates a polynomial base-learner factory
#'  object which can be registered within a base-learner list and then used
#'  for training.
#'
#' @format \code{\link{S4}} object.
#' @name BaselearnerPolynomial
#'
#' @section Usage:
#' \preformatted{
#' BaselearnerPolynomial$new(data_source, data_target, list(degree, intercept))
#' BaselearnerPolynomial$new(data_source, data_target, blearner_type, list(degree, intercept))
#' }
#'
#' @section Arguments:
#' \describe{
#' \item{\code{data_source} [\code{Data} Object]}{
#'   Data object which contains the source data.
#' }
#' \item{\code{data_target} [\code{Data} Object]}{
#'   Data object which gets the transformed source data.
#' }
#' \item{\code{degree} [\code{integer(1)}]}{
#'   This argument is used for transforming the source data. Each element is
#'   taken to the power of the \code{degree} argument.
#' }
#' \item{\code{intercept} [\code{logical(1)}]}{
#'   Indicating whether an intercept should be added or not. Default is set to TRUE.
#' }
#' }
#'
#'
#' @section Details:
#'   The polynomial base-learner factory takes any matrix which the user wants
#'   to pass the number of columns indicates how much parameter are estimated.
#'
#' @section Fields:
#'   This class doesn't contain public fields.
#'
#' @section Methods:
#' \describe{
#' \item{\code{getData()}}{Get the data matrix of the target data which is used
#'   for modeling.}
#' \item{\code{transformData(X)}}{Transform a data matrix as defined within the
#'   factory. The argument has to be a matrix with one column.}
#' \item{\code{summarizeFactory()}}{Summarize the base-learner factory object.}
#' }
#' @examples
#' # Sample data:
#' data_mat = cbind(1:10)
#'
#' # Create new data object:
#' data_source = InMemoryData$new(data_mat, "my_data_name")
#' data_target1 = InMemoryData$new()
#' data_target2 = InMemoryData$new()
#'
#' # Create new linear base-learner factory:
#' lin_factory = BaselearnerPolynomial$new(data_source, data_target1,
#'   list(degree = 2, intercept = FALSE))
#' lin_factory_int = BaselearnerPolynomial$new(data_source, data_target2,
#'   list(degree = 2, intercept = TRUE))
#'
#' # Get the transformed data:
#' lin_factory$getData()
#' lin_factory_int$getData()
#'
#' # Summarize factory:
#' lin_factory$summarizeFactory()
#'
#' # Transform data manually:
#' lin_factory$transformData(data_mat)
#' lin_factory_int$transformData(data_mat)
#'
#' @export BaselearnerPolynomial
NULL

#' Base-learner factory to do non-parametric B or P-spline regression
#'
#' \code{BaselearnerPSpline} creates a spline base-learner factory
#'  object which can be registered within a base-learner list and then used
#'  for training.
#'
#' @format \code{\link{S4}} object.
#' @name BaselearnerPSpline
#'
#' @section Usage:
#' \preformatted{
#' BaselearnerPSpline$new(data_source, data_target, list(degree, n_knots, penalty,
#'   differences))
#' }
#'
#' @section arguments:
#' \describe{
#' \item{\code{data_source} [\code{data} object]}{
#'   data object which contains the source data.
#' }
#' \item{\code{data_target} [\code{data} object]}{
#'   data object which gets the transformed source data.
#' }
#' \item{\code{degree} [\code{integer(1)}]}{
#'   degree of the spline functions to interpolate the knots.
#' }
#' \item{\code{n_knots} [\code{integer(1)}]}{
#'   number of \strong{inner knots}. to prevent weird behavior on the edges
#'   the inner knots are expanded by \eqn{\mathrm{degree} - 1} additional knots.
#' }
#' \item{\code{penalty} [\code{numeric(1)}]}{
#'   positive numeric value to specify the penalty parameter. setting the
#'   penalty to 0 ordinary b-splines are used for the fitting.
#' }
#' \item{\code{differences} [\code{integer(1)}]}{
#'   the number of differences which are penalized. a higher value leads to
#'   smoother curves.
#' }
#' \item{\code{bin_root} [\code{integer(1)}]}{
#'   If set to a value greater than zero, binning is applied and reduces the number of used
#'   x values to n^(1/bin_root) equidistant points. If you want to use binning we suggest
#'   to set \code{bin_root = 2}.
#' }
#' }
#'
#' @section Details:
#'   If \code{bin_root > 0} the original feature is discretized to on an equidistant grid on
#'   \eqn{[\min(x),\max(x)]} with \eqn{\sqrt{n}} points. The fitting is then done by
#'   using weights per new data point.
#'
#' @section Fields:
#'   This class doesn't contain public fields.
#'
#' @section Methods:
#' \describe{
#' \item{\code{getData()}}{Get the data matrix of the target data which is used
#'   for modeling.}
#' \item{\code{transformData(X)}}{Transform a data matrix as defined within the
#'   factory. The argument has to be a matrix with one column.}
#' \item{\code{summarizeFactory()}}{Summarize the base-learner factory object.}
#' }
#' @examples
#' # Sample data:
#' data_mat = cbind(1:10)
#' y = sin(1:10)
#'
#' # Create new data object:
#' data_source = InMemoryData$new(data_mat, "my_data_name")
#' data_target = InMemoryData$new()
#'
#' # Create new linear base-learner:
#' spline_factory = BaselearnerPSpline$new(data_source, data_target,
#'   list(degree = 3, n_knots = 4, penalty = 2, differences = 2))
#'
#' # Get the transformed data:
#' spline_factory$getData()
#'
#' # Summarize factory:
#' spline_factory$summarizeFactory()
#'
#' # Transform data manually:
#' spline_factory$transformData(data_mat)
#'
#' @export BaselearnerPSpline
NULL

#' Base-learner factory for categorical feature on a binary base-learner basis
#'
#' \code{BaselearnerCategoricalBinary} can be used to estimate effects of one category of a categorical
#' feature. The base-learner gets the data as index vector of the observations assigned to the group.
#' For example, if the vector is (a, a, b, b, a, b), then the index vector is (1, 2, 5) for group a.
#' This reduces memory load and fasten the fitting process.
#'
#' @format \code{\link{S4}} object.
#' @name BaselearnerCategoricalBinary
#'
#' @section Usage:
#' \preformatted{
#' BaselearnerCategoricalBinary$new(data_source, data_target, list(n_obs))
#' }
#'
#' @section arguments:
#' \describe{
#' \item{\code{data_source} [\code{data} object]}{
#'   data object which contains the source data.
#' }
#' \item{\code{data_target} [\code{data} object]}{
#'   data object which gets the transformed source data.
#' }
#' }
#'
#' @section Fields:
#'   This class doesn't contain public fields.
#'
#' @section Methods:
#' \describe{
#' \item{\code{getData()}}{Get the data matrix of the target data which is used
#'   for modeling.}
#' \item{\code{transformData(X)}}{Transform a data matrix as defined within the
#'   factory. The argument has to be a matrix with one column. In case of the categorical
#'   binary base-learner this is the index of non-zero elements concatinated with the
#'   number of observations. This helps to fully reconstruct the original feature by using less memory. This also speed up computation time.}
#' \item{\code{summarizeFactory()}}{Summarize the base-learner factory object.}
#' }
#' @examples
#' # Sample data:
#' x = sample(c(0,1), 20, TRUE)
#' data_mat = cbind(x)
#'
#' # Create new data object:
#' data_source = InMemoryData$new(data_mat, "my_data_name")
#'
#' # Create new linear base-learner:
#' cat_factory = BaselearnerCategoricalBinary$new(data_source)
#'
#' # Get the transformed data as stored for internal use:
#' cat_factory$getData()
#'
#' # Summarize factory:
#' cat_factory$summarizeFactory()
#'
#' # Transform data manually:
#' cat_factory$transformData(x)
#'
#' @export BaselearnerCategoricalBinary
NULL

#' Create custom base-learner factory by using R functions.
#'
#' \code{BaselearnerCustom} creates a custom base-learner factory by
#'   setting custom \code{R} functions. This factory object can be registered
#'   within a base-learner list and then used for training.
#'
#' @format \code{\link{S4}} object.
#' @name BaselearnerCustom
#'
#' @section Usage:
#' \preformatted{
#' BaselearnerCustom$new(data_source, data_target, list(instantiate_fun,
#'   train_fun, predict_fun, param_fun))
#' }
#'
#' @section Arguments:
#' \describe{
#' \item{\code{data_source} [\code{Data} Object]}{
#'   Data object which contains the source data.
#' }
#' \item{\code{data_target} [\code{Data} Object]}{
#'   Data object which gets the transformed source data.
#' }
#' \item{\code{instantiate_fun} [\code{function}]}{
#'   \code{R} function to transform the source data. For details see the
#'   \code{Details}.
#' }
#' \item{\code{train_fun} [\code{function}]}{
#'   \code{R} function to train the base-learner on the target data. For
#'   details see the \code{Details}.
#' }
#' \item{\code{predict_fun} [\code{function}]}{
#'   \code{R} function to predict on the object returned by \code{train}.
#'   For details see the \code{Details}.
#' }
#' \item{\code{param_fun} [\code{function}]}{
#'   \code{R} function to extract the parameter of the object returned by
#'   \code{train}. For details see the \code{Details}.
#' }
#' }
#'
#' @section Details:
#'   The function must have the following structure:
#'
#'   \code{instantiateData(X) { ... return (X_trafo) }} With a matrix argument
#'   \code{X} and a matrix as return object.
#'
#'   \code{train(y, X) { ... return (SEXP) }} With a vector argument \code{y}
#'   and a matrix argument \code{X}. The target data is used in \code{X} while
#'   \code{y} contains the response. The function can return any \code{R}
#'   object which is stored within a \code{SEXP}.
#'
#'   \code{predict(model, newdata) { ... return (prediction) }} The returned
#'   object of the \code{train} function is passed to the \code{model}
#'   argument while \code{newdata} contains a new matrix used for predicting.
#'
#'   \code{extractParameter() { ... return (parameters) }} Again, \code{model}
#'   contains the object returned by \code{train}. The returned object must be
#'   a matrix containing the estimated parameter. If no parameter should be
#'   estimated one can return \code{NA}.
#'
#'   For an example see the \code{Examples}.
#'
#' @section Fields:
#'   This class doesn't contain public fields.
#'
#' @section Methods:
#' \describe{
#' \item{\code{getData()}}{Get the data matrix of the target data which is used
#'   for modeling.}
#' \item{\code{transformData(X)}}{Transform a data matrix as defined within the
#'   factory. The argument has to be a matrix with one column.}
#' \item{\code{summarizeFactory()}}{Summarize the base-learner factory object.}
#' }
#' @examples
#' # Sample data:
#' data_mat = cbind(1, 1:10)
#' y = 2 + 3 * 1:10
#'
#' # Create new data object:
#' data_source = InMemoryData$new(data_mat, "my_data_name")
#' data_target = InMemoryData$new()
#'
#' instantiateDataFun = function (X) {
#'   return(X)
#' }
#' # Ordinary least squares estimator:
#' trainFun = function (y, X) {
#'   return(solve(t(X) %*% X) %*% t(X) %*% y)
#' }
#' predictFun = function (model, newdata) {
#'   return(as.matrix(newdata %*% model))
#' }
#' extractParameter = function (model) {
#'   return(as.matrix(model))
#' }
#'
#' # Create new custom linear base-learner factory:
#' custom_lin_factory = BaselearnerCustom$new(data_source, data_target,
#'   list(instantiate_fun = instantiateDataFun, train_fun = trainFun,
#'     predict_fun = predictFun, param_fun = extractParameter))
#'
#' # Get the transformed data:
#' custom_lin_factory$getData()
#'
#' # Summarize factory:
#' custom_lin_factory$summarizeFactory()
#'
#' # Transform data manually:
#' custom_lin_factory$transformData(data_mat)
#'
#' @export BaselearnerCustom
NULL

#' Create custom cpp base-learner factory by using cpp functions and external
#' pointer.
#'
#' \code{BaselearnerCustomCpp} creates a custom base-learner factory by
#'   setting custom \code{C++} functions. This factory object can be registered
#'   within a base-learner list and then used for training.
#'
#' @format \code{\link{S4}} object.
#' @name BaselearnerCustomCpp
#'
#' @section Usage:
#' \preformatted{
#' BaselearnerCustomCpp$new(data_source, data_target, list(instantiate_ptr,
#'   train_ptr, predict_ptr))
#' }
#'
#' @section Arguments:
#' \describe{
#' \item{\code{data_source} [\code{Data} Object]}{
#'   Data object which contains the source data.
#' }
#' \item{\code{data_target} [\code{Data} Object]}{
#'   Data object which gets the transformed source data.
#' }
#' \item{\code{instantiate_ptr} [\code{externalptr}]}{
#'   External pointer to the \code{C++} instantiate data function.
#' }
#' \item{\code{train_ptr} [\code{externalptr}]}{
#'   External pointer to the \code{C++} train function.
#' }
#' \item{\code{predict_ptr} [\code{externalptr}]}{
#'   External pointer to the \code{C++} predict function.
#' }
#' }
#'
#' @section Details:
#'   For an example see the extending compboost vignette or the function
#'   \code{getCustomCppExample}.
#'
#' @section Fields:
#'   This class doesn't contain public fields.
#'
#' @section Methods:
#' \describe{
#' \item{\code{getData()}}{Get the data matrix of the target data which is used
#'   for modeling.}
#' \item{\code{transformData(X)}}{Transform a data matrix as defined within the
#'   factory. The argument has to be a matrix with one column.}
#' \item{\code{summarizeFactory()}}{Summarize the base-learner factory object.}
#' }
#' @examples
#' \donttest{
#' # Sample data:
#' data_mat = cbind(1, 1:10)
#' y = 2 + 3 * 1:10
#'
#' # Create new data object:
#' data_source = InMemoryData$new(data_mat, "my_data_name")
#' data_target = InMemoryData$new()
#'
#' # Source the external pointer exposed by using XPtr:
#' Rcpp::sourceCpp(code = getCustomCppExample(silent = TRUE))
#'
#' # Create new linear base-learner:
#' custom_cpp_factory = BaselearnerCustomCpp$new(data_source, data_target,
#'   list(instantiate_ptr = dataFunSetter(), train_ptr = trainFunSetter(),
#'     predict_ptr = predictFunSetter()))
#'
#' # Get the transformed data:
#' custom_cpp_factory$getData()
#'
#' # Summarize factory:
#' custom_cpp_factory$summarizeFactory()
#'
#' # Transform data manually:
#' custom_cpp_factory$transformData(data_mat)
#' }
#' @export BaselearnerCustomCpp
NULL

#' Base-learner factory list to define the set of base-learners
#'
#' \code{BlearnerFactoryList} creates an object in which base-learner factories
#' can be registered. This object can then be passed to compboost as set of
#' base-learner which is used by the optimizer to get the new best
#' base-learner.
#'
#' @format \code{\link{S4}} object.
#' @name BlearnerFactoryList
#'
#' @section Usage:
#' \preformatted{
#' BlearnerFactoryList$new()
#' }
#'
#' @section Fields:
#'   This class doesn't contain public fields.
#'
#' @section Methods:
#' \describe{
#' \item{\code{registerFactory(BaselearnerFactory)}}{Takes a object of the
#'   class \code{BaseLearnerFactory} and adds this factory to the set of
#'   base-learner.}
#' \item{\code{printRegisteredFactories()}}{Get all registered factories.}
#' \item{\code{clearRegisteredFactories()}}{Remove all registered factories.
#'   Note that the factories are not deleted, just removed from the map.}
#' \item{\code{getModelFrame()}}{Get each target data matrix parsed to one
#'   big matrix.}
#' \item{\code{getNumberOfRegisteredFactories()}}{Get the number of registered
#'   factories.}
#' }
#' @examples
#' # Sample data:
#' data_mat = cbind(1:10)
#'
#' # Create new data object:
#' data_source = InMemoryData$new(data_mat, "my_data_name")
#' data_target1 = InMemoryData$new()
#' data_target2 = InMemoryData$new()
#'
#' lin_factory = BaselearnerPolynomial$new(data_source, data_target1,
#'   list(degree = 1, intercept = TRUE))
#' poly_factory = BaselearnerPolynomial$new(data_source, data_target2,
#'   list(degree = 2, intercept = TRUE))
#'
#' # Create new base-learner list:
#' my_bl_list = BlearnerFactoryList$new()
#'
#' # Register factories:
#' my_bl_list$registerFactory(lin_factory)
#' my_bl_list$registerFactory(poly_factory)
#'
#' # Get registered factories:
#' my_bl_list$printRegisteredFactories()
#'
#' # Get all target data matrices in one big matrix:
#' my_bl_list$getModelFrame()
#'
#' # Clear list:
#' my_bl_list$clearRegisteredFactories()
#'
#' # Get number of registered factories:
#' my_bl_list$getNumberOfRegisteredFactories()
#'
#' @export BlearnerFactoryList
NULL

#' Quadratic loss for regression tasks.
#'
#' This loss can be used for regression with \eqn{y \in \mathrm{R}}.
#'
#' \strong{Loss Function:}
#' \deqn{
#'   L(y, f(x)) = \frac{1}{2}( y - f(x))^2
#' }
#' \strong{Gradient:}
#' \deqn{
#'   \frac{\delta}{\delta f(x)}\ L(y, f(x)) = f(x) - y
#' }
#' \strong{Initialization:}
#' \deqn{
#'   \hat{f}^{[0]}(x) = \mathrm{arg~min}{c\in\mathrm{R}}{\mathrm{arg~min}}\ \frac{1}{n}\sum\limits_{i=1}^n
#'   L\left(y^{(i)}, c\right) = \bar{y}
#' }
#'
#' @format \code{\link{S4}} object.
#' @name LossQuadratic
#'
#' @section Usage:
#' \preformatted{
#' LossQuadratic$new()
#' LossQuadratic$new(offset)
#' }
#'
#' @section Arguments:
#' \describe{
#' \item{\code{offset} [\code{numeric(1)}]}{
#'   Numerical value which can be used to set a custom offset. If so, this
#'   value is returned instead of the loss optimal initialization.
#' }
#' }
#'
#' @examples
#'
#' # Create new loss object:
#' quadratic_loss = LossQuadratic$new()
#' quadratic_loss
#'
#' @export LossQuadratic
NULL

#' Absolute loss for regression tasks.
#'
#' This loss can be used for regression with \eqn{y \in \mathrm{R}}.
#'
#' \strong{Loss Function:}
#' \deqn{
#'   L(y, f(x)) = | y - f(x)|
#' }
#' \strong{Gradient:}
#' \deqn{
#'   \frac{\delta}{\delta f(x)}\ L(y, f(x)) = -\mathrm{sign}(y - f(x))
#' }
#' \strong{Initialization:}
#' \deqn{
#'   \hat{f}^{[0]}(x) = \mathrm{arg~min}_{c\in R}\ \frac{1}{n}\sum\limits_{i=1}^n
#'   L(y^{(i)}, c) = \mathrm{median}(y)
#' }
#'
#' @format \code{\link{S4}} object.
#' @name LossAbsolute
#'
#' @section Usage:
#' \preformatted{
#' LossAbsolute$new()
#' LossAbsolute$new(offset)
#' }
#'
#' @section Arguments:
#' \describe{
#' \item{\code{offset} [\code{numeric(1)}]}{
#'   Numerical value which can be used to set a custom offset. If so, this
#'   value is returned instead of the loss optimal initialization.
#' }
#' }
#'
#' @examples
#'
#' # Create new loss object:
#' absolute_loss = LossAbsolute$new()
#' absolute_loss
#'
#' @export LossAbsolute
NULL

#' Quantile loss for regression tasks.
#'
#' This loss can be used for regression with \eqn{y \in \mathrm{R}}.
#'
#' \strong{Loss Function:}
#' \deqn{
#'   L(y, f(x)) = h| y - f(x)|
#' }
#' \strong{Gradient:}
#' \deqn{
#'   \frac{\delta}{\delta f(x)}\ L(y, f(x)) = -h\mathrm{sign}( y - f(x))
#' }
#' \strong{Initialization:}
#' \deqn{
#'   \hat{f}^{[0]}(x) = \mathrm{arg~min}_{c\in R}\ \frac{1}{n}\sum\limits_{i=1}^n
#'   L(y^{(i)}, c) = \mathrm{quantile}(y, q)
#' }
#'
#' @format \code{\link{S4}} object.
#' @name LossQuantile
#'
#' @section Usage:
#' \preformatted{
#' LossAbsolute$new()
#' LossAbsolute$new(quantile)
#' LossAbsolute$new(offset, quantile)
#' }
#'
#' @section Arguments:
#' \describe{
#' \item{\code{offset} [\code{numeric(1)}]}{
#'   Numerical value which can be used to set a custom offset. If so, this
#'   value is returned instead of the loss optimal initialization.
#' }
#' \item{\code{quantile} [\code{numeric(1)}]}{
#'   Numerical value between 0 and 1 indicating the quantile used for boosting.
#' }
#' }
#'
#' @examples
#'
#' # Create new loss object:
#' quadratic_loss = LossQuadratic$new()
#' quadratic_loss
#'
#' @export LossQuantile
NULL

#' Huber loss for regression tasks.
#'
#' This loss can be used for regression with \eqn{y \in \mathrm{R}}.
#'
#' \strong{Loss Function:}
#' \deqn{
#'   L(y, f(x)) = 0.5(y - f(x))^2 \ \ \mathrm{if} \ \ |y - f(x)| < d
#' }
#' \deqn{
#'   L(y, f(x)) = d|y - f(x)| - 0.5d^2 \ \ \mathrm{otherwise}
#' }
#' \strong{Gradient:}
#' \deqn{
#'   \frac{\delta}{\delta f(x)}\ L(y, f(x)) = f(x) - y \ \ \mathrm{if} \ \ |y - f(x)| < d
#' }
#' \deqn{
#'   \frac{\delta}{\delta f(x)}\ L(y, f(x)) = -d\mathrm{sign}(y - f(x)) \ \ \mathrm{otherwise}
#' }
#'
#' @format \code{\link{S4}} object.
#' @name LossHuber
#'
#' @section Usage:
#' \preformatted{
#' LossHuber$new()
#' LossHuber$new(delta)
#' LossHuber$new(offset, delta)
#' }
#'
#' @section Arguments:
#' \describe{
#' \item{\code{offset} [\code{numeric(1)}]}{
#'   Numerical value which can be used to set a custom offset. If so, this
#'   value is returned instead of the loss optimal initialization.
#' }
#' \item{\code{delta} [\code{numeric(1)}]}{
#'   Numerical value greater than 0 to specify the interval around 0 for the quadratic error measuring.
#'   Default is 1.
#' }
#' }
#'
#' @examples
#'
#' # Create new loss object:
#' huber_loss = LossHuber$new()
#' huber_loss
#'
#' @export LossHuber
NULL

#' 0-1 Loss for binary classification derived of the binomial distribution
#'
#' This loss can be used for binary classification. The coding we have chosen
#' here acts on
#' \eqn{y \in \{-1, 1\}}.
#'
#' \strong{Loss Function:}
#' \deqn{
#'   L(y, f(x)) = \log(1 + \mathrm{exp}(-2yf(x)))
#' }
#' \strong{Gradient:}
#' \deqn{
#'   \frac{\delta}{\delta f(x)}\ L(y, f(x)) = - \frac{y}{1 + \mathrm{exp}(2yf)}
#' }
#' \strong{Initialization:}
#' \deqn{
#'   \hat{f}^{[0]}(x) = \frac{1}{2}\mathrm{log}(p / (1 - p))
#' }
#' with
#' \deqn{
#'   p = \frac{1}{n}\sum\limits_{i=1}^n\mathrm{1}_{\{y^{(i)} = 1\}}
#' }
#'
#' @format \code{\link{S4}} object.
#' @name LossBinomial
#'
#' @section Usage:
#' \preformatted{
#' LossBinomial$new()
#' LossBinomial$new(offset)
#' }
#'
#' @section Arguments:
#' \describe{
#' \item{\code{offset} [\code{numeric(1)}]}{
#'   Numerical value which can be used to set a custom offset. If so, this
#'   value is returned instead of the loss optimal initialization.
#' }
#' }
#'
#' @examples
#'
#' # Create new loss object:
#' bin_loss = LossBinomial$new()
#' bin_loss
#'
#' @export LossBinomial
NULL

#' Create LossCustom by using R functions.
#'
#' \code{LossCustom} creates a custom loss by using
#' \code{Rcpp::Function} to set \code{R} functions.
#'
#' @format \code{\link{S4}} object.
#' @name LossCustom
#'
#' @section Usage:
#' \preformatted{
#' LossCustom$new(lossFun, gradientFun, initFun)
#' }
#'
#' @section Arguments:
#' \describe{
#' \item{\code{lossFun} [\code{function}]}{
#'   \code{R} function to calculate the loss. For details see the
#'   \code{Details}.
#' }
#' \item{\code{gradientFun} [\code{function}]}{
#'   \code{R} function to calculate the gradient. For details see the
#'   \code{Details}.
#' }
#' \item{\code{initFun} [\code{function}]}{
#'   \code{R} function to calculate the constant initialization. For
#'   details see the \code{Details}.
#' }
#' }
#'
#' @section Details:
#'   The functions must have the following structure:
#'
#'   \code{lossFun(truth, prediction) { ... return (loss) }} With a vector
#'   argument \code{truth} containing the real values and a vector of
#'   predictions \code{prediction}. The function must return a vector
#'   containing the loss for each component.
#'
#'   \code{gradientFun(truth, prediction) { ... return (grad) }} With a vector
#'   argument \code{truth} containing the real values and a vector of
#'   predictions \code{prediction}. The function must return a vector
#'   containing the gradient of the loss for each component.
#'
#'   \code{initFun(truth) { ... return (init) }} With a vector
#'   argument \code{truth} containing the real values. The function must
#'   return a numeric value containing the offset for the constant
#'   initialization.
#'
#' @examples
#'
#' # Loss function:
#' myLoss = function (true_values, prediction) {
#'   return (0.5 * (true_values - prediction)^2)
#' }
#' # Gradient of loss function:
#' myGradient = function (true_values, prediction) {
#'   return (prediction - true_values)
#' }
#' # Constant initialization:
#' myConstInit = function (true_values) {
#'   return (mean(true_values))
#' }
#'
#' # Create new custom quadratic loss:
#' my_loss = LossCustom$new(myLoss, myGradient, myConstInit)
#'
#' @export LossCustom
NULL

#' Create custom cpp losses by using cpp functions and external pointer.
#'
#' \code{LossCustomCpp} creates a custom loss by using
#' \code{Rcpp::XPtr} to set \code{C++} functions.
#'
#' @format \code{\link{S4}} object.
#' @name LossCustomCpp
#'
#' @section Usage:
#' \preformatted{
#' LossCustomCpp$new(loss_ptr, grad_ptr, const_init_ptr)
#' }
#'
#' @section Arguments:
#' \describe{
#' \item{\code{loss_ptr} [\code{externalptr}]}{
#'   External pointer to the \code{C++} loss function.
#' }
#' \item{\code{grad_ptr} [\code{externalptr}]}{
#'   External pointer to the \code{C++} gradient function.
#' }
#' \item{\code{const_init_ptr} [\code{externalptr}]}{
#'   External pointer to the \code{C++} constant initialization function.
#' }
#' }
#'
#' @examples
#' \donttest{
#' # Load loss functions:
#' Rcpp::sourceCpp(code = getCustomCppExample(example = "loss", silent = TRUE))
#'
#' # Create new custom quadratic loss:
#' my_cpp_loss = LossCustomCpp$new(lossFunSetter(), gradFunSetter(), constInitFunSetter())
#' }
#' @export LossCustomCpp
NULL

#' Create response object for regression.
#'
#' \code{ResponseRegr} creates a response object that are used as target during the
#' fitting process.
#'
#' @format \code{\link{S4}} object.
#' @name ResponseRegr
#'
#' @section Usage:
#' \preformatted{
#' ResponseRegr$new(target_name, response)
#' ResponseRegr$new(target_name, response, weights)
#' }
#'
#' @example
#'
#' response_regr = ResponseRegr$new("target", cbind(rnorm(10)))
#' response_regr$getResponse()
#' response_regr$getTargetName()
#'
#' @export ResponseRegr
NULL

#' Create response object for binary classification.
#'
#' \code{ResponseBinaryClassif} creates a response object that are used as target during the
#' fitting process.
#'
#' @format \code{\link{S4}} object.
#' @name ResponseBinaryClassif
#'
#' @section Usage:
#' \preformatted{
#' ResponseBinaryClassif$new(target_name, pos_class, response)
#' ResponseBinaryClassif$new(target_name, pos_class, response, weights)
#' }
#'
#' @example
#'
#' response_binary = ResponseBinaryClassif$new("target", "A", sample(c("A", "B"), 10, TRUE))
#' response_binary$getResponse()
#' response_binary$getPrediction()
#' response_binary$getPredictionTransform() # Applies sigmoid to prediction scores
#' response_binary$getPredictionResponse()  # Categorizes depending on the transformed predictions
#' response_binary$getTargetName()
#' response_binary$setThreshold(0.7)
#' response_binary$getThreshold()
#' response_binary$getPositiveClass()
#'
#' @export ResponseBinaryClassif
NULL

#' Logger class to log the current iteration
#'
#' @format \code{\link{S4}} object.
#' @name LoggerIteration
#'
#' @section Usage:
#' \preformatted{
#' LoggerIterationWrapper$new(logger_id, use_as_stopper, max_iterations)
#' }
#'
#' @section Arguments:
#' \describe{
#' \item{\code{logger_id} [\code{character(1)}]}{
#'   Unique identifier of the logger.
#' }
#' \item{\code{use_as_stopper} [\code{logical(1)}]}{
#'   Boolean to indicate if the logger should also be used as stopper.
#' }
#' \item{\code{max_iterations} [\code{integer(1)}]}{
#'   If the logger is used as stopper this argument defines the maximal
#'   iterations.
#' }
#' }
#'
#' @section Fields:
#'   This class doesn't contain public fields.
#'
#' @section Methods:
#' \describe{
#' \item{\code{summarizeLogger()}}{Summarize the logger object.}
#' }
#' @examples
#' # Define logger:
#' log_iters = LoggerIteration$new("iterations", FALSE, 100)
#'
#' # Summarize logger:
#' log_iters$summarizeLogger()
#'
#' @export LoggerIteration
NULL

#' Logger class to log the inbag risk
#'
#' This class logs the inbag risk for a specific loss function. It is also
#' possible to use custom losses to log performance measures. For details
#' see the use case or extending compboost vignette.
#'
#' @format \code{\link{S4}} object.
#' @name LoggerInbagRisk
#'
#' @section Usage:
#' \preformatted{
#' LoggerInbagRisk$new(logger_id, use_as_stopper, used_loss, eps_for_break, patience)
#' }
#'
#' @section Arguments:
#' \describe{
#' \item{\code{logger_id} [\code{character(1)}]}{
#'   Unique identifier of the logger.
#' }
#' \item{\code{use_as_stopper} [\code{logical(1)}]}{
#'   Boolean to indicate if the logger should also be used as stopper.
#' }
#' \item{\code{used_loss} [\code{Loss} object]}{
#'   The loss used to calculate the empirical risk by taking the mean of the
#'   returned defined loss within the loss object.
#' }
#' \item{\code{eps_for_break} [\code{numeric(1)}]}{
#'   This argument is used if the loss is also used as stopper. If the relative
#'   improvement of the logged inbag risk falls above this boundary the stopper
#'   returns \code{TRUE}.
#' }
#' }
#'
#' @section Details:
#'
#' This logger computes the risk for the given training data
#' \eqn{\mathcal{D} = \{(x^{(i)},\ y^{(i)})\ |\ i \in \{1, \dots, n\}\}}
#' and stores it into a vector. The empirical risk \eqn{\mathcal{R}} for
#' iteration \eqn{m} is calculated by:
#' \deqn{
#'   \mathcal{R}_\mathrm{emp}^{[m]} = \frac{1}{n}\sum\limits_{i = 1}^n L(y^{(i)}, \hat{f}^{[m]}(x^{(i)}))
#' }
#'
#' \strong{Note:}
#' \itemize{
#'   \item
#'     If \eqn{m=0} than \eqn{\hat{f}} is just the offset.
#'
#'   \item
#'     The implementation to calculate \eqn{\mathcal{R}_\mathrm{emp}^{[m]}} is
#'     done in two steps:
#'       \enumerate{
#'        \item
#'          Calculate vector \code{risk_temp} of losses for every observation for
#'          given response \eqn{y^{(i)}} and prediction \eqn{\hat{f}^{[m]}(x^{(i)})}.
#'
#'        \item
#'          Average over \code{risk_temp}.
#'      }
#'    }
#'    This procedure ensures, that it is possible to e.g. use the AUC or any
#'    arbitrary performance measure for risk logging. This gives just one
#'    value for \code{risk_temp} and therefore the average equals the loss
#'    function. If this is just a value (like for the AUC) then the value is
#'    returned.
#'
#' @section Fields:
#'   This class doesn't contain public fields.
#'
#' @section Methods:
#' \describe{
#'   \item{\code{summarizeLogger()}}{Summarize the logger object.}
#' }
#' @examples
#' # Used loss:
#' log_bin = LossBinomial$new()
#'
#' # Define logger:
#' log_inbag_risk = LoggerInbagRisk$new("inbag", FALSE, log_bin, 0.05, 5)
#'
#' # Summarize logger:
#' log_inbag_risk$summarizeLogger()
#'
#' @export LoggerInbagRisk
NULL

#' Logger class to log the out of bag risk
#'
#' This class logs the out of bag risk for a specific loss function. It is
#' also possible to use custom losses to log performance measures. For details
#' see the use case or extending compboost vignette.
#'
#' @format \code{\link{S4}} object.
#' @name LoggerOobRisk
#'
#' @section Usage:
#' \preformatted{
#' LoggerOobRisk$new(logger_id, use_as_stopper, used_loss, eps_for_break,
#'   patience, oob_data, oob_response)
#' }
#'
#' @section Arguments:
#' \describe{
#' \item{\code{logger_id} [\code{character(1)}]}{
#'   Unique identifier of the logger.
#' }
#' \item{\code{use_as_stopper} [\code{logical(1)}]}{
#'   Boolean to indicate if the logger should also be used as stopper.
#' }
#' \item{\code{used_loss} [\code{Loss} object]}{
#'   The loss used to calculate the empirical risk by taking the mean of the
#'   returned defined loss within the loss object.
#' }
#' \item{\code{eps_for_break} [\code{numeric(1)}]}{
#'   This argument is used if the loss is also used as stopper. If the relative
#'   improvement of the logged inbag risk falls above this boundary the stopper
#'   returns \code{TRUE}.
#' }
#' \item{\code{oob_data} [\code{list}]}{
#'   A list which contains data source objects which corresponds to the
#'   source data of each registered factory. The source data objects should
#'   contain the out of bag data. This data is then used to calculate the
#'   prediction in each step.
#' }
#' \item{\code{oob_response} [\code{numeric}]}{
#'   Vector which contains the response for the out of bag data given within
#'   the \code{list}.
#' }
#' }
#'
#' @section Details:
#'
#' This logger computes the risk for a given new dataset
#' \eqn{\mathcal{D}_\mathrm{oob} = \{(x^{(i)},\ y^{(i)})\ |\ i \in I_\mathrm{oob}\}}
#' and stores it into a vector. The OOB risk \eqn{\mathcal{R}_\mathrm{oob}} for
#' iteration \eqn{m} is calculated by:
#' \deqn{
#'   \mathcal{R}_\mathrm{oob}^{[m]} = \frac{1}{|\mathcal{D}_\mathrm{oob}|}\sum\limits_{(x,y) \in \mathcal{D}_\mathrm{oob}}
#'   L(y, \hat{f}^{[m]}(x))
#' }
#'
#' \strong{Note:}
#'   \itemize{
#'
#'   \item
#'     If \eqn{m=0} than \eqn{\hat{f}} is just the offset.
#'
#'   \item
#'     The implementation to calculate \eqn{\mathcal{R}_\mathrm{emp}^{[m]}} is
#'     done in two steps:
#'       \enumerate{
#'
#'       \item
#'         Calculate vector \code{risk_temp} of losses for every observation for
#'         given response \eqn{y^{(i)}} and prediction \eqn{\hat{f}^{[m]}(x^{(i)})}.
#'
#'       \item
#'         Average over \code{risk_temp}.
#'      }
#'    }
#'
#'    This procedure ensures, that it is possible to e.g. use the AUC or any
#'    arbitrary performance measure for risk logging. This gives just one
#'    value for \eqn{risk_temp} and therefore the average equals the loss
#'    function. If this is just a value (like for the AUC) then the value is
#'    returned.
#'
#' @section Fields:
#'   This class doesn't contain public fields.
#'
#' @section Methods:
#' \describe{
#' \item{\code{summarizeLogger()}}{Summarize the logger object.}
#' }
#' @examples
#' # Define data:
#' X1 = cbind(1:10)
#' X2 = cbind(10:1)
#' data_source1 = InMemoryData$new(X1, "x1")
#' data_source2 = InMemoryData$new(X2, "x2")
#'
#' oob_list = list(data_source1, data_source2)
#'
#' set.seed(123)
#' y_oob = rnorm(10)
#'
#' # Used loss:
#' log_bin = LossBinomial$new()
#'
#' # Define response object of oob data:
#' oob_response = ResponseRegr$new("oob_response", as.matrix(y_oob))
#'
#' # Define logger:
#' log_oob_risk = LoggerOobRisk$new("oob", FALSE, log_bin, 0.05, 5, oob_list, oob_response)
#'
#' # Summarize logger:
#' log_oob_risk$summarizeLogger()
#'
#' @export LoggerOobRisk
NULL

#' Logger class to log the elapsed time
#'
#' This class just logs the elapsed time. This should be very handy if one
#' wants to run the algorithm for just 2 hours and see how far he comes within
#' that time. There are three time units available for logging:
#' \itemize{
#'   \item minutes
#'   \item seconds
#'   \item microseconds
#' }
#'
#' @format \code{\link{S4}} object.
#' @name LoggerTime
#'
#' @section Usage:
#' \preformatted{
#' LoggerTime$new(logger_id, use_as_stopper, max_time, time_unit)
#' }
#'
#' @section Arguments:
#' \describe{
#' \item{\code{logger_id} [\code{character(1)}]}{
#'   Unique identifier of the logger.
#' }
#' \item{\code{use_as_stopper} [\code{logical(1)}]}{
#'   Boolean to indicate if the logger should also be used as stopper.
#' }
#' \item{\code{max_time} [\code{integer(1)}]}{
#'   If the logger is used as stopper this argument contains the maximal time
#'   which are available to train the model.
#' }
#' \item{\code{time_unit} [\code{character(1)}]}{
#'   Character to specify the time unit. Possible choices are \code{minutes},
#'   \code{seconds} or \code{microseconds}
#' }
#' }
#'
#' @section Fields:
#'   This class doesn't contain public fields.
#'
#' @section Methods:
#' \describe{
#' \item{\code{summarizeLogger()}}{Summarize the logger object.}
#' }
#' @examples
#' # Define logger:
#' log_time = LoggerTime$new("time_minutes", FALSE, 20, "minutes")
#'
#' # Summarize logger:
#' log_time$summarizeLogger()
#'
#' @export LoggerTime
NULL

#' Logger list class to collect all loggers
#'
#' This class is meant to define all logger which should be used to track the
#' progress of the algorithm.
#'
#' @format \code{\link{S4}} object.
#' @name LoggerList
#'
#' @section Usage:
#' \preformatted{
#' LoggerList$new()
#' }
#'
#' @section Fields:
#'   This class doesn't contain public fields.
#'
#' @section Methods:
#' \describe{
#' \item{\code{clearRegisteredLogger()}}{Removes all registered logger
#'   from the list. The used logger are not deleted, just removed from the
#'   map.}
#' \item{\code{getNamesOfRegisteredLogger()}}{Returns the registered logger
#'   names as character vector.}
#' \item{\code{getNumberOfRegisteredLogger()}}{Returns the number of registered
#'   logger as integer.}
#' \item{\code{printRegisteredLogger()}}{Prints all registered logger.}
#' \item{\code{registerLogger(logger)}}{Includes a new \code{logger}
#'   into the logger list with the \code{logger_id} as key.}
#' }
#' @examples
#' # Define logger:
#' log_iters = LoggerIteration$new("iteration", TRUE, 100)
#' log_time = LoggerTime$new("time", FALSE, 20, "minutes")
#'
#' # Create logger list:
#' logger_list = LoggerList$new()
#'
#' # Register new loggeR:
#' logger_list$registerLogger(log_iters)
#' logger_list$registerLogger(log_time)
#'
#' # Print registered logger:
#' logger_list$printRegisteredLogger()
#'
#' # Remove all logger:
#' logger_list$clearRegisteredLogger()
#'
#' # Get number of registered logger:
#' logger_list$getNumberOfRegisteredLogger()
#'
#' @export LoggerList
NULL

#' Coordinate Descent
#'
#' This class defines a new object for the greedy optimizer. The optimizer
#' just calculates for each base-learner the sum of squared errors and returns
#' the base-learner with the smallest SSE.
#'
#' @format \code{\link{S4}} object.
#' @name OptimizerCoordinateDescent
#'
#' @section Usage:
#' \preformatted{
#' OptimizerCoordinateDescent$new()
#' }
#'
#' @examples
#'
#' # Define optimizer:
#' optimizer = OptimizerCoordinateDescent$new()
#'
#' @export OptimizerCoordinateDescent
NULL

#' Coordinate Descent with line search
#'
#' This class defines a new object which is used to conduct Coordinate Descent with line search.
#' The optimizer just calculates for each base-learner the sum of squared error and returns
#' the base-learner with the smallest SSE. In addition, this optimizer computes
#' a line search to find the optimal step size in each iteration.
#'
#' @format \code{\link{S4}} object.
#' @name OptimizerCoordinateDescentLineSearch
#'
#' @section Usage:
#' \preformatted{
#' OptimizerCoordinateDescentLineSearch$new()
#' }
#'
#' @examples
#'
#' # Define optimizer:
#' optimizer = OptimizerCoordinateDescentLineSearch$new()
#'
#' @export OptimizerCoordinateDescentLineSearch
NULL

#' Main Compboost Class
#'
#' This class collects all parts such as the factory list or the used logger
#' and passes them to \code{C++}. On the \code{C++} side is then the main
#' algorithm.
#'
#' @format \code{\link{S4}} object.
#' @name Compboost_internal
#'
#' @section Usage:
#' \preformatted{
#' Compboost$new(response, learning_rate, stop_if_all_stopper_fulfilled,
#'   factory_list, loss, logger_list, optimizer)
#' }
#'
#' @section Arguments:
#' \describe{
#' \item{\code{response} [\code{numeric}]}{
#'   Vector of the true values which should be modeled.
#' }
#' \item{\code{learning_rate} [\code{numeric(1)}]}{
#'   The learning rate which is used to shrink the parameter in each iteration.
#' }
#' \item{\code{stop_if_all_stopper_fulfilled} [\code{logical(1)}]}{
#'   Boolean to indicate which stopping strategy is used. If \code{TRUE} then
#'   the algorithm stops if all registered logger stopper are fulfilled.
#' }
#' \item{\code{factory_list} [\code{BlearnerFactoryList} object]}{
#'   List of base-learner factories from which one base-learner is selected
#'   in each iteration by using the
#' }
#' \item{\code{loss} [\code{Loss} object]}{
#'   The loss which should be used to calculate the pseudo residuals in each
#'   iteration.
#' }
#' \item{\code{logger_list} [\code{LoggerList} object]}{
#'   The list with all registered logger which are used to track the algorithm.
#' }
#' \item{\code{optimizer} [\code{Optimizer} object]}{
#'   The optimizer which is used to select in each iteration one good
#'   base-learner.
#' }
#' }
#'
#' @section Fields:
#'   This class doesn't contain public fields.
#'
#' @section Methods:
#' \describe{
#' \item{\code{train(trace)}}{Initial training of the model. The integer
#'   argument \code{trace} indicates if the logger progress should be printed
#'   or not and if so trace indicates which iterations should be printed.}
#' \item{\code{continueTraining(trace, logger_list)}}{Continue the training
#'   by using an additional \code{logger_list}. The retraining is stopped if
#'   the first logger says that the algorithm should be stopped.}
#' \item{\code{getPrediction()}}{Get the inbag prediction which is done during
#'   the fitting process.}
#' \item{\code{getSelectedBaselearner()}}{Returns a character vector of how
#'   the base-learner are selected.}
#' \item{\code{getLoggerData()}}{Returns a list of all logged data. If the
#'   algorithm is retrained, then the list contains for each training one
#'   element.}
#' \item{\code{getEstimatedParameter()}}{Returns a list with the estimated
#'   parameter for base-learner which was selected at least once.}
#' \item{\code{getParameterAtIteration(k)}}{Calculates the prediction at the
#'   iteration \code{k}.}
#' \item{\code{getParameterMatrix()}}{Calculates a matrix where row \code{i}
#'   includes the parameter at iteration \code{i}. There are as many rows
#'   as done iterations.}
#' \item{\code{isTrained()}}{This function returns just a boolean value which
#'   indicates if the initial training was already done.}
#' \item{\code{predict(newdata)}}{Prediction on new data organized within a
#'   list of source data objects. It is important that the names of the source
#'   data objects matches those one that were used to define the factories.}
#' \item{\code{predictAtIteration(newdata, k)}}{Prediction on new data by using
#'   another iteration \code{k}.}
#' \item{\code{setToIteration(k)}}{Set the whole model to another iteration
#'   \code{k}. After calling this function all other elements such as the
#'   parameters or the prediction are calculated corresponding to \code{k}.}
#' \item{\code{summarizeCompboost()}}{Summarize the \code{Compboost} object.}
#' }
#' @examples
#'
#' # Some data:
#' df = mtcars
#' df$mpg_cat = ifelse(df$mpg > 20, "high", "low")
#'
#' # # Create new variable to check the polynomial base-learner with degree 2:
#' # df$hp2 = df[["hp"]]^2
#'
#' # Data for the baselearner are matrices:
#' X_hp = as.matrix(df[["hp"]])
#' X_wt = as.matrix(df[["wt"]])
#'
#' # Target variable:
#' response = ResponseBinaryClassif$new("mpg_cat", "high", df[["mpg_cat"]])
#'
#' data_source_hp = InMemoryData$new(X_hp, "hp")
#' data_source_wt = InMemoryData$new(X_wt, "wt")
#'
#' data_target_hp1 = InMemoryData$new()
#' data_target_hp2 = InMemoryData$new()
#' data_target_wt1 = InMemoryData$new()
#' data_target_wt2 = InMemoryData$new()
#'
#' # List for oob logging:
#' oob_data = list(data_source_hp, data_source_wt)
#'
#' # List to test prediction on newdata:
#' test_data = oob_data
#'
#' # Factories:
#' linear_factory_hp = BaselearnerPolynomial$new(data_source_hp, data_target_hp1,
#'   list(degree = 1, intercept = TRUE))
#' linear_factory_wt = BaselearnerPolynomial$new(data_source_wt, data_target_wt1,
#'   list(degree = 1, intercept = TRUE))
#' quadratic_factory_hp = BaselearnerPolynomial$new(data_source_hp, data_target_hp2,
#'   list(degree = 2, intercept = TRUE))
#' spline_factory_wt = BaselearnerPSpline$new(data_source_wt, data_target_wt2,
#'   list(degree = 3, n_knots = 10, penalty = 2, differences = 2))
#'
#' # Create new factory list:
#' factory_list = BlearnerFactoryList$new()
#'
#' # Register factories:
#' factory_list$registerFactory(linear_factory_hp)
#' factory_list$registerFactory(linear_factory_wt)
#' factory_list$registerFactory(quadratic_factory_hp)
#' factory_list$registerFactory(spline_factory_wt)
#'
#' # Define loss:
#' loss_bin = LossBinomial$new()
#'
#' # Define optimizer:
#' optimizer = OptimizerCoordinateDescent$new()
#'
#' ## Logger
#'
#' # Define logger. We want just the iterations as stopper but also track the
#' # time, inbag risk and oob risk:
#' log_iterations  = LoggerIteration$new(" iteration_logger", TRUE, 500)
#' log_time        = LoggerTime$new("time_logger", FALSE, 500, "microseconds")
#'
#' # Define new logger list:
#' logger_list = LoggerList$new()
#'
#' # Register the logger:
#' logger_list$registerLogger(log_iterations)
#' logger_list$registerLogger(log_time)
#'
#' # Run compboost:
#' # --------------
#'
#' # Initialize object:
#' cboost = Compboost_internal$new(
#'   response      = response,
#'   learning_rate = 0.05,
#'   stop_if_all_stopper_fulfilled = FALSE,
#'   factory_list = factory_list,
#'   loss         = loss_bin,
#'   logger_list  = logger_list,
#'   optimizer    = optimizer
#' )
#'
#' # Train the model (we want to print the trace):
#' cboost$train(trace = 50)
#' cboost
#'
#' # Get estimated parameter:
#' cboost$getEstimatedParameter()
#'
#' # Get trace of selected base-learner:
#' cboost$getSelectedBaselearner()
#'
#' # Set to iteration 200:
#' cboost$setToIteration(200, 30)
#'
#' # Get new parameter values:
#' cboost$getEstimatedParameter()
#'
#' @export Compboost_internal
NULL

